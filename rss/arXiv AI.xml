<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.ai更新在arxiv.org上</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.ai在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Mon, 24 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>Genai vs.人类事实检查员：准确的评分，有缺陷的理由</title>
      <link>https://arxiv.org/abs/2502.14943</link>
      <description><![CDATA[ARXIV：2502.14943V1公告类型：新 
摘要：尽管了解生成人工智能（Genai）模型的能力和限制的最新进展，但我们才开始了解他们评估和理论内容真实性的能力。我们评估了涉及信息可信度的评级和理解推理的任务跨任务的多个模型。我们实验中的信息来自美国国家政治家向Facebook发布的内容。我们发现，在消费者应用中使用最常用的AI模型之一GPT-4O胜过其他模型，但所有模型都与人类编码者均表现出适度的一致性。重要的是，即使Genai模型准确地识别出低可信度的内容，它们的推理也很大程度上依赖于语言特征和``硬&#39;&#39;标准，例如细节，源可靠性和语言形式的水平，而不是对真实性的理解。我们还评估了汇总与完整内容输入的有效性，发现汇总的内容有望提高效率而不牺牲准确性。尽管Genai有潜力支持人类事实检查者在扩展错误信息检测方面，但我们的结果警告仅依靠这些模型。]]></description>
      <guid>https://arxiv.org/abs/2502.14943</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>教育AI的模仿游戏</title>
      <link>https://arxiv.org/abs/2502.15127</link>
      <description><![CDATA[ARXIV：2502.15127V1公告类型：新 
摘要：随着人工智能系统在教育中变得越来越普遍，出现了一个基本挑战：我们如何验证AI是否真正了解学生的思维和理性方式？传统的评估方法，例如测量学习收益，需要冗长的研究与许多变量混淆。我们提出了一个基于两相图灵样检验的新型评估框架。在第1阶段，学生对问题提供开放式回答，从而揭示了自然的误解。在第2阶段，以每个学生的特定错误为条件，AI和人类专家都会为新的相关问题产生干扰因素。通过分析学生是否以类似于人类专家生成的速度的速率选择AI生成的干扰素，我们可以验证AI是否建模学生认知。我们证明，必须根据个人响应来调节此评估 - 无条件的方法仅针对常见的误解。通过严格的统计抽样理论，我们确定了高信验证的精确要求。我们的研究职位将分心的生成条件作为对AI系统建模学生思维的基本能力的调查 - 这种能力使能够适应辅导，反馈和评估每个学生的特定需求。]]></description>
      <guid>https://arxiv.org/abs/2502.15127</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>测量AI代理自主权：通过代码检查采用可扩展方法</title>
      <link>https://arxiv.org/abs/2502.15212</link>
      <description><![CDATA[ARXIV：2502.15212V1公告类型：新 
摘要：AI代理是可以自主实现复杂目标的AI系统。评估代理自治的水平对于理解其潜在收益和风险至关重要。当前对自治的评估通常集中在特定风险上，并依靠运行时评估 - 对操作过程中代理行动的观察。我们介绍了基于代码的自主权评估，该评估消除了运行AI代理以执行特定任务的需求，从而降低了与运行时间评估相关的成本和风险。使用此基于代码的框架，可以根据评估自治属性的分类法对运行AI代理的编排代码进行评分：影响和监督。我们使用Autogen框架和选择应用来证明这种方法。]]></description>
      <guid>https://arxiv.org/abs/2502.15212</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ARS：具有大语言模型的自动路由求解器</title>
      <link>https://arxiv.org/abs/2502.15359</link>
      <description><![CDATA[ARXIV：2502.15359V1公告类型：新 
摘要：现实世界中的车辆路线问题（VRP）的特征是各种实用的约束，使手动求解器设计既具有知识密集型又耗时。尽管对自动化路由算法的设计的兴趣越来越大，但现有研究仅探索了有限的VRP变体，并且无法充分解决在现实世界中遇到的复杂且普遍的约束。为了填补这一空白，本文介绍了Routbench，Routbench是来自24个属性的1,000个VRP变体的基准，用于评估自动路由求解器在解决复杂约束方面的有效性。与RoutBench一起，我们提出了自动语言模型（LLM）代理的自动路由求解器（ARS），从而通过自动生成基于问题描述的约束启发式代码来增强骨干算法框架，并根据数据库中选择的几种代表性约束来增强骨干算法框架。我们的实验表明，ARS优于最先进的LLM方法和常用的求解器，自动求解91.67％的常见VRP，并在所有基准测试中至少提高了30％。]]></description>
      <guid>https://arxiv.org/abs/2502.15359</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Chitrarth：十亿人的桥接视觉和语言</title>
      <link>https://arxiv.org/abs/2502.15392</link>
      <description><![CDATA[ARXIV：2502.15392V1公告类型：新 
摘要：最近的多模式基础模型主要接受英语或高资源欧洲语言数据的培训，这阻碍了其对其他中型和低资源语言的适用性。为了解决这一限制，我们介绍了包容性视觉语言模型（VLM）的Chitrarth（Chitra：Image; Artha：含义），专门针对10种突出的印度语言的丰富语言多样性和视觉推理。我们的模型有效地集成了一个最先进的（SOTA）多语言大语模型（LLM），并主要在多语言图像文本数据上训练。此外，我们还推出了Bharatbench，这是一个综合框架，用于评估各种印度语言的VLM，最终为更多样化和有效的AI系统做出了贡献。我们的模型可在低资源语言的基准中获得SOTA的结果，同时保留其英语效率。通过我们的研究，我们的目标是在多语言媒体能力方面设定新的基准，从而对现有模型提供了实质性的改进，并为促进该领域的未来进步建立了基础。]]></description>
      <guid>https://arxiv.org/abs/2502.15392</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>标签：用于多代理分层增强学习的分散框架</title>
      <link>https://arxiv.org/abs/2502.15425</link>
      <description><![CDATA[ARXIV：2502.15425V1公告类型：新 
摘要：等级组织是生物系统和人类社会的基础，但是人工智能系统通常依赖于限制适应性和可伸缩性的单层体系结构。当前的分层增强学习（HRL）方法通常将层次结构限制在两个级别或需要集中培训，这限制了其实际适用性。我们介绍了Tame Agent Framework（TAG），该框架用于构建完全分散的层次结构多代理系统。TAG通过新颖的LevelSenv概念实现了任意深度的层次结构，该概述将每个层次结构级别作为其上方代理的环境提取。这种方法标准化了级别之间的信息流，同时保留松散的耦合，从而可以无缝整合不同的代理类型。我们通过实施层次结构来证明TAG的有效性，这些层次结构结合了多个级别的不同RL代理，从而提高了与标准基准测试的经典多代理RL基准相比的性能。我们的结果表明，分散的分层组织提高了学习速度和最终性能，将标签定位为可扩展多代理系统的有希望的方向。]]></description>
      <guid>https://arxiv.org/abs/2502.15425</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>两石：爱因斯坦W \的动态编程评估函数“不urfelt！</title>
      <link>https://arxiv.org/abs/2502.15547</link>
      <description><![CDATA[ARXIV：2502.15547V1公告类型：新 
摘要：本文介绍了Zweistein，这是Einstein W \“ Urefelt Nicht！（EWN）的动态编程评估功能。Zweistein使用以数据为中心的方法来消除参数调整的需求，而不是依靠人类知识来制作评估功能这个想法是使用与所有距离的距离记录的距离表现优于许多传统的EWN评估功能，但在TCGA 2023竞赛中也赢得了第一名。]]></description>
      <guid>https://arxiv.org/abs/2502.15547</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AI评估的范例：映射目标，方法论和文化</title>
      <link>https://arxiv.org/abs/2502.15620</link>
      <description><![CDATA[ARXIV：2502.15620V1公告类型：新 
摘要：AI评估中的研究变得越来越复杂和多学科，吸引了具有不同背景和目标的研究人员。结果，出现了不同的评估范例，通常是孤立发展的，采用冲突的术语并忽略了彼此的贡献。这种分裂导致了不同范式和公众之间的岛屿研究轨迹和沟通障碍，这有助于对部署的AI系统的未达到的期望。为了弥合这种孤立性，在本文中，我们调查了AI评估领域的最新工作，并确定了六个主要范式。我们表征了与其目标，方法论和研究文化有关的关键维度跨每个范式中的主要贡献。通过阐明与每个范式相关的问题和方法的独特组合，我们旨在提高人们对当前评估方法广度的认识，并促进不同范式之间的交叉授粉。我们还确定了该领域的潜在差距，以激发未来的研究方向。]]></description>
      <guid>https://arxiv.org/abs/2502.15620</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过逻辑推理增强LLM的能力：一项全面的调查</title>
      <link>https://arxiv.org/abs/2502.15652</link>
      <description><![CDATA[ARXIV：2502.15652V1公告类型：新 
摘要：大型语言模型（LLM）在各种自然语言任务上取得了巨大的成功。但是，最近的研究发现，LLM的逻辑推理能力仍然存在重大挑战。本文总结并将主要挑战分为两个方面：（1）逻辑问题回答，LLMS通常无法在复杂的逻辑问题中产生正确的答案，这需要复杂的演绎，归纳性，归纳性或绑架性推理，并且鉴于收集的前提和约束。 （2）逻辑一致性，LLM容易产生与不同问题相矛盾的回答。例如，最先进的金刚鹦鹉提问的LLM答案是对这两个问题的回答是鸟吗？鸟有翅膀吗？但是对喜p的答案没有翅膀吗？为了促进这一研究方向，我们全面研究了最尖端的方法，并提出了这些方法的详细分类法。具体而言，要准确回答复杂的逻辑问题，可以根据对外部求解器，提示，预处理和微调的依赖来分类以前的方法。为了避免逻辑矛盾，我们讨论了各种逻辑一致性的概念和解决方案，包括含义，否定，传递性，事实一致性及其复合材料。此外，我们回顾了常用的基准数据集和评估指标，并讨论有前途的研究方向，例如向模态逻辑扩展以解决不确定性，以及同时满足多重逻辑一致性的有效算法。]]></description>
      <guid>https://arxiv.org/abs/2502.15652</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>超智能特工带来灾难性的风险：科学家AI可以提供更安全的道路吗？</title>
      <link>https://arxiv.org/abs/2502.15657</link>
      <description><![CDATA[ARXIV：2502.15657V1公告类型：新 
摘要：领先的AI公司越来越专注于建立通才AI代理 - 可以自主计划，采取行动并追求人类可以执行的所有任务的系统。尽管这些系统可能有多么有用，但未经检查的AI机构还是对公共安全和保障构成了重大风险，从恶意行为者的滥用到潜在的不可逆转的人类控制丧失。我们讨论当前的AI培训方法如何产生这些风险。的确，各种场景和实验都证明了AI代理人从事欺骗或追求人类经营者未指定的目标以及与人类利益的冲突（例如自我保护）的可能性。遵循预防原则，我们认为，对当前代理机构驱动的轨迹的替代方案非常有用。因此，我们建议作为进一步的核心构建基础，以进一步发展，通过设计可信赖和安全的非高级AI系统的开发，我们称之为科学家AI。该系统旨在从观察中解释世界，而不是采取模仿或让人类取悦人类的行动。它包括一个世界模型，该模型生成理论来解释数据和提问的推理机。这两个组件都具有明确的不确定性概念，以减轻过度自信的预测风险。鉴于这些考虑因素，科学家AI可用于帮助人类研究人员加速科学进步，包括在AI安全中。特别是，我们的系统可以用作针对AI代理的护栏，尽管涉及风险，但可能会创建。最终，专注于非代理AI可能会在避免与当前轨迹相关的风险的同时，使AI创新的好处。我们希望这些论点能激励研究人员，开发人员和政策制定者偏爱这一更安全的道路。]]></description>
      <guid>https://arxiv.org/abs/2502.15657</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用基于技能的贝叶斯网络自动化课程学习，以进行增强学习</title>
      <link>https://arxiv.org/abs/2502.15662</link>
      <description><![CDATA[ARXIV：2502.15662V1公告类型：新 
摘要：增强学习的主要挑战是自动生成课程，以减少训练时间或提高某些目标任务的绩效。我们介绍了SEBNS（技能环境贝叶斯网络），该网络对一组技能，一组与奖励结构相关的目标和一组环境功能之间的概率关系进行了建模，以预测（可能看不见）任务的策略表现。我们开发了一种使用SEBN代理成功估计的推断估计值来通过预期改进来权衡可能的下一个任务的算法。我们评估了所得课程在三个环境中的好处：一个离散的网格世界，连续控制和模拟机器人技术。结果表明，使用SEBN构建的课程经常超过其他基线。]]></description>
      <guid>https://arxiv.org/abs/2502.15662</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Autotom：开放式心理理论的自动贝叶斯逆计划和模型发现</title>
      <link>https://arxiv.org/abs/2502.15676</link>
      <description><![CDATA[ARXIV：2502.15676V1公告类型：新 
摘要：心灵理论（汤姆）是人们根据行为理解心理变量的能力，是发展社会智能代理的关键。当前的思维理论推理理论方法要么依赖于促使大型语言模型（LLMS），这些模型容易出现系统的错误，要么使用僵硬的手工贝叶斯心理理论（BTOM）模型，这些模型更强大，但不能跨越不同的领域。在这项工作中，我们介绍了Autotom，这是一种用于实现开放式机器心理理论的自动化贝叶斯心理理论方法。 Autotom可以在任何领域中运行，推断出任何心理变量，并进行任何秩序的心理推理理论。考虑到心理推理问题的理论，Autotom首先提出了初始的BTOM模型。然后，它根据提议的模型进行自动化的贝叶斯反向计划，利用LLM作为后端。基于推论的不确定性，它通过引入其他心理变量和/或在上下文中纳入更多时间段来迭代地完善模型。多种心理理论基准的经验评估表明，自动对机器心理理论提供了可扩展，健壮且可解释的方法。]]></description>
      <guid>https://arxiv.org/abs/2502.15676</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>超高分辨率图像的高质量分割</title>
      <link>https://arxiv.org/abs/2111.14482</link>
      <description><![CDATA[ARXIV：2111.14482V3公告类型：交叉 
摘要：到段4K或6K超高分辨率图像需要在图像分割中考虑额外的计算考虑。常见的策略，例如下采样，补丁裁剪和级联模型，无法很好地解决准确性和计算成本之间的平衡问题。由于人类将对象之间的区分不断地从粗糙的水平区分到精确的水平，因此我们提出了连续的完善模型〜（CRM），以实现超高分辨率分割精炼任务。 CRM将特征映射与改进目标保持一致，并汇总特征以重建这些图像的详细信息。此外，我们的CRM显示了其重要的概括能力，可以填补低分辨率训练图像和超高分辨率测试之间的分辨率差距。我们提出定量性能评估和可视化，以表明我们所提出的方法在图像分割细化中快速有效。代码将在https://github.com/dvlab-research/entity上发布。]]></description>
      <guid>https://arxiv.org/abs/2111.14482</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>D-Sketch：通过预审计的潜扩散模型改善素描到图像翻译的视觉保真度</title>
      <link>https://arxiv.org/abs/2502.14007</link>
      <description><![CDATA[ARXIV：2502.14007V1公告类型：交叉 
摘要：图像到图像翻译中的结构指导允许对合成图像的形状进行复杂的控制。从用户指定的粗糙手绘草图中生成高质量的现实图像就是这样的任务，旨在对条件生成过程施加结构性约束。尽管该前提对于众多内容创建和学术研究的用例都很有趣，但由于徒手草图中的实质性歧义，该问题从根本上变得具有挑战性。此外，平衡形状一致性和现实产生之间的权衡会导致过程中的额外复杂性。基于生成对抗网络（GAN）的现有方法通常使用条件gan或gan倒置，通常需要特定于应用程序的数据和优化目标。 denoisis扩散概率模型（DDPM）的最新引入为一般图像合成中的低级视觉属性实现了代际飞跃。但是，由于苛刻的计算成本和数据不足，直接在域特异性子任务上直接重新训练大规模扩散模型通常非常困难。在本文中，我们通过利用大规模扩散模型的特征泛化功能而不进行重新训练来介绍一种用于草图到图像翻译的技术。特别是，我们使用可学习的轻量级映射网络来实现从源到目标域的潜在特征翻译。实验结果表明，所提出的方法在定性和定量基准中优于现有技术，从而使高分辨率的现实图像合成了粗糙的手绘草图。]]></description>
      <guid>https://arxiv.org/abs/2502.14007</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>解锁黑匣子：分析《欧盟人工智能法案》的AI解释性框架</title>
      <link>https://arxiv.org/abs/2502.14868</link>
      <description><![CDATA[ARXIV：2502.14868V1公告类型：交叉 
摘要：缺乏人工智能（AI）的解释性是行业和监管机构必须克服的第一个障碍之一，以减轻与技术相关的风险。在问责制，道德和公平性至关重要的领域，例如医疗保健，信用评分，警务和刑事司法系统，对可解释的AI（XAI）的需求显而易见。在欧盟层面上，解释性的概念是基于AI法案的基本原则之一，尽管确切的XAI技术和要求仍有待确定和测试。本文探讨了有望推进XAI的各种方法和技术，以及在AI治理和政策中实施解释性原则的挑战。最后，本文研究了XAI纳入欧盟法律，强调了标准环境，监督和执法的问题。]]></description>
      <guid>https://arxiv.org/abs/2502.14868</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>设想利益相关者成对以减轻AI的负面影响：一种参与式方法，以告知政策制定</title>
      <link>https://arxiv.org/abs/2502.14869</link>
      <description><![CDATA[arxiv：2502.14869v1公告类型：交叉 
摘要：AI对负面影响的潜力在世界范围内迅速变得更加普遍，这加剧了对负责人AI治理的需求。尽管公司和学术学者提出了许多基于风险的方法和多种风险缓解措施的方法，但这些方法通常以专家为中心，因此缺乏包括重要的一群利益相关者。确保AI政策与民主期望保持一致，需要将受影响者的声音和需求确定优先级的方法。在这项工作中，我们开发了一种参与性和前瞻性的方法，以告知政策制定者和学者，从而使外行利益相关者在最前沿的需求并丰富了降低风险降低策略的发展。我们的方法（1）映射负AI的潜在缓解和预防策略对各种利益相关者的责任分配责任，（2）在外行人眼中探讨了其重要性和优先级别，（3）在政策事实表中提供了这些见解，即，一种可消化格式，用于告知政策流程。我们强调，这种方法不是针对替换决策者的目标。相反，我们的目的是提出一种丰富的方法，以丰富缓解策略，并为政策制定提供更具参与性的方法。]]></description>
      <guid>https://arxiv.org/abs/2502.14869</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>专家为什么不同意存在的风险和P（厄运）？ AI专家的调查</title>
      <link>https://arxiv.org/abs/2502.14870</link>
      <description><![CDATA[ARXIV：2502.14870V1公告类型：交叉 
摘要：人工智能（AGI）的发展可能是人类最结果的技术进步之一。领先的AI实验室和科学家呼吁全球优先考虑AI安全的优先级，原因是存在与核战争相当的存在风险。但是，即使是专家，也经常对灾难性风险和AI一致性进行研究。此外，关于人工智能存在风险的在线辩论已经开始转变为部落（例如，诸如“ doomer”或“ Accelerationist”之类的名称为单位）。到目前为止，尚无系统性研究探讨了信仰模式以及专家之间对AI安全概念的熟悉程度。我对111名AI专家进行了调查，以了解他们对AI安全概念，对AI安全的关键异议以及对安全论点的反应。我的发现表明，AI专家将“ AI为可控工具”和“ AI为不可控制的代理人”的观点聚集在两个观点上 - 在信念方面对AI安全的重要性有所不同。尽管大多数专家（78％）同意或强烈同意“技术AI研究人员应该关注灾难性风险”，但许多人不熟悉特定的AI安全概念。例如，只有21％的被调查专家听说过“工具融合”，这是AI安全中的一个基本概念，预测高级AI系统将倾向于追求常见的子目标（例如自我保护）。最不关心的参与者对这样的概念是最不熟悉的，这表明AI安全的有效交流应始于在该领域建立明确的概念基础。]]></description>
      <guid>https://arxiv.org/abs/2502.14870</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>数学是否过时？</title>
      <link>https://arxiv.org/abs/2502.14874</link>
      <description><![CDATA[Arxiv：2502.14874V1公告类型：交叉 
摘要：这是关于AI时代数学和象征性推理价值的文章。]]></description>
      <guid>https://arxiv.org/abs/2502.14874</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>KKA：通过大型语言模型的异常知识改善视力异常检测</title>
      <link>https://arxiv.org/abs/2502.14880</link>
      <description><![CDATA[ARXIV：2502.14880V1公告类型：交叉 
摘要：视力异常检测，尤其是在无监督的环境中，通常由于异常的差异而努力区分正常样品和异常。最近，越来越多的研究集中在产生异常，以帮助探测器学习正常样本和异常之间的更有效界限。但是，由于产生的异常通常来自随机因素，因此它们经常缺乏现实主义。此外，随机生成的异常通常在构建有效边界方面提供有限的支持，因为大多数与正常样本大大差异，并且远离边界。为了应对这些挑战，我们提出了关键知识增强（KKA），该方法从大型语言模型（LLMS）中提取异常知识。更具体地说，KKA利用LLM的广泛的先验知识来基于正常样本产生有意义的异常。然后，KKA根据与正常样本的相似性将生成的异常分类为简单的异常和硬异常。易于异常与正常样品显示出显着差异，而硬异常与正常样品非常相似。 KKA迭代更新了生成的异常，并逐渐增加了硬异常的比例，以使探测器能够学习更有效的边界。实验结果表明，所提出的方法可显着提高各种视力异常检测器的性能，同时保持低发电成本。可以在https://github.com/anfeather/kka上找到CMG的代码。]]></description>
      <guid>https://arxiv.org/abs/2502.14880</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LVLM和自动指标能否捕获盲人和低视频个体的潜在偏好？</title>
      <link>https://arxiv.org/abs/2502.14883</link>
      <description><![CDATA[ARXIV：2502.14883V1公告类型：交叉 
摘要：视觉是人类如何看待环境的主要手段，但是盲人和低视觉（BLV）人们需要帮助了解周围环境，尤其是在陌生的环境中。基于语义的系统作为BLV用户的辅助工具的出现已经激发了许多研究人员探索大型视觉语言模型（LVLMS）的响应。但是，尚未研究BLV用户对LVLM的各种响应的偏好，特别是用于导航辅助。为了填补这一空白，我们首先构建了由人类验证的1.1K策划的室外/室内场景组成的数据集，每个场景的相关请求为5-10个。然后，我们对八个BLV用户进行了深入的用户研究，以从五个角度从六个LVLM上评估他们的偏好：害怕，不可行，充分性和简洁性。最后，我们介绍了Eye4b基准测试，以评估广泛使用的基于模型的图像文本指标与我们收集的BLV偏好之间的对齐。我们的工作可以作为将BLV感知的LVLMS开发到无屏障AI系统的指南。]]></description>
      <guid>https://arxiv.org/abs/2502.14883</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>