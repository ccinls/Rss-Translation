<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.ai更新在arxiv.org上</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.ai在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Wed, 26 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>用户意图将Deekseep用于医疗保健目的及其对大语言模型的信任：跨国调查研究</title>
      <link>https://arxiv.org/abs/2502.17487</link>
      <description><![CDATA[ARXIV：2502.17487V1公告类型：新 
摘要：大型语言模型（LLMS）越来越多地充当交互式医疗资源，但用户接受仍然没有被忽视。这项研究探讨了用于医疗保健目的的易用性，可感知的有用性，信任和风险感知相互作用，以塑造一种采用DeepSeek的意图，这是一个新兴LLM的平台。对来自印度，英国和美国的556名参与者进行了横断面调查，以衡量观念和使用模式。结构方程模型评估了直接和间接效应，包括潜在的二次关系。结果表明，信任起着关键的中介作用：易用性通过信任对使用意图产生了显着的间接影响，而感知的有用性既有助于信任发展和直接采用。相比之下，风险感知会对使用意图产生负面影响，强调了强大的数据治理和透明度的重要性。值得注意的是，观察到明显的非线性路径，以易于使用和风险，表明阈值或高原影响。测量模型表现出强大的可靠性和有效性，并由高复合可靠性，提取的平均方差和判别有效性度量得到支持。这些发现通过阐明敏感领域中用户采用的多方面性质来扩展技术的接受和健康信息学研究。利益相关者应投资于建立信任的策略，以用户为中心的设计以及减轻风险措施，以鼓励医疗保健中LLM的持续和安全吸收。未来的工作可以采用纵向设计或检查特定文化的变量，以进一步阐明用户的看法如何随着时间和不同的监管环境而发展。这种见解对于利用AI提高结果至关重要。]]></description>
      <guid>https://arxiv.org/abs/2502.17487</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>数据集特征：通过无监督的数据重建揭示自然语言特征</title>
      <link>https://arxiv.org/abs/2502.17541</link>
      <description><![CDATA[arxiv：2502.17541v1公告类型：新 
摘要：解释数据对于现代研究至关重要。大型语言模型（LLMS）在提供数据的自然语言解释方面表现出了希望，但简单的特征提取方法（例如提示）通常无法为各种数据集产生准确且通用的描述，并且缺乏对粒度和规模的控制。为了解决这些局限性，我们提出了一种用于数据集特征的域 - 不可吻合的方法，该方法提供了对提取的功能数量的精确控制，同时保持与人类专家标签相当的紧凑和描述性表示。我们的方法通过评估LLM使用这些功能重建原始数据的能力来优化信息丰富的二进制功能。我们通过两个案例研究证明了其在数据集建模任务中的有效性：（1）构建越狱策略的特征表示，该策略既遵守了一组较大的人类攻击的有效性和多样性； （2）自动化发现与人类偏好相符的功能，从而实现与专家精选功能相当的精确性和鲁棒性。此外，我们表明管道尺度有效，随着采样其他功能的改进，使其适用于大型和多样化的数据集。]]></description>
      <guid>https://arxiv.org/abs/2502.17541</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言猴子如何获得力量（法律）？</title>
      <link>https://arxiv.org/abs/2502.17578</link>
      <description><![CDATA[ARXIV：2502.17578V1公告类型：新 
摘要：跨数学问题解决，证明助手编程和多模式越狱文件的最新研究引人注目的发现：当（多模式）语言模型应对每项任务多次尝试的任务套件 - 如果任何尝试是正确的，那么负面的日志在平均成功率中，在尝试数量中扩展了权力定律。在这项工作中，我们确定了一个明显的难题：一个简单的数学计算预测，在每个问题上，故障率应随着尝试的数量而成倍地下降。我们从经验上证实了这一预测，提出了一个问题：聚集多项式缩放从何处出现？然后，我们通过证明单个指数缩放的每个问题来回答这个问题，如果单击成功概率的分布重量较重，则可以使总多项式缩放一致，以使总体成功概率的一小部分任务集体地扭曲了总体成功趋势趋势进入权力定律 - 即使每个问题都自行呈指数级扩展。我们进一步证明，这种分布观点解释了先前观察到的与权力定律缩放的偏差，并提供了一种简单的方法，用于预测功率法指数的相对错误较低或等效地，$ {\ sim} 2-4 $幅度较少推理计算。总体而言，我们的工作有助于更好地理解神经语言模型的表现如何通过缩放推理计算以及对（多模式）语言模型的可预测评估的开发发展。]]></description>
      <guid>https://arxiv.org/abs/2502.17578</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>实时互动导航图中的意图识别</title>
      <link>https://arxiv.org/abs/2502.17581</link>
      <description><![CDATA[ARXIV：2502.17581V1公告类型：新 
摘要：在此演示中，我们开发了IntentRec4maps，这是一个识别用户在交互式图中的意图的系统。 IntentRec4maps使用Google Maps平台作为现实世界的交互式地图，是一种非常有效的方法，用于实时识别用户的意图。我们使用两个不同的路径计划和一个大语言模型（LLM）展示了IntentRec4maps的识别过程。
  github：https：//github.com/peijiez/intentrec4maps]]></description>
      <guid>https://arxiv.org/abs/2502.17581</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型的代表工程：调查和研究挑战</title>
      <link>https://arxiv.org/abs/2502.17601</link>
      <description><![CDATA[ARXIV：2502.17601V1公告类型：新 
摘要：大型语言模型能够完成各种任务，但仍然不可预测且棘手。代表工程试图通过采用对比鲜明的输入样本来检测和编辑诚实，有害或寻求权力等概念的高级表示，以通过一种新的方法来解决此问题。我们将代表工程的目标和方法形式化，以呈现这一新兴学科中工作的凝聚力图片。我们将其与替代方法进行比较，例如机械性解释性，及时工程和微调。我们概述了诸如绩效降低，计算时间增加和可置换问题之类的风险。我们为将来的研究提出了一个明确的议程，以建立可预测的，动态的，安全和个性化的LLM。]]></description>
      <guid>https://arxiv.org/abs/2502.17601</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>苏格拉底式：通过支持AI的教练增强人类团队合作</title>
      <link>https://arxiv.org/abs/2502.17643</link>
      <description><![CDATA[arxiv：2502.17643v1公告类型：新 
摘要：教练对于有效的协作至关重要，但是成本和资源限制通常会限制其在现实任务中的可用性。这种限制在依靠有效的团队合作（例如医疗保健和灾难反应）的生命领域构成了严重的挑战。为了解决这一差距，我们提出并实现了AI：任务时间团队指导的创新应用。具体来说，我们介绍了一种新颖的AI系统Socratic，通过在执行任务执行期间提供实时指导来补充人类教练。苏格拉底式监视团队的行为，发现团队成员共同理解的未对准，并提供自动干预措施以提高团队绩效。我们通过涉及二元合作的两个人类主题实验验证了苏格拉底式。结果表明，该系统通过最少的干预措施可显着提高团队绩效。参与者还认为苏格拉底人是有益和值得信赖的，支持其收养潜力。我们的发现还暗示了AI研究及其实用应用的有希望的方向，以增强人类团队合作。]]></description>
      <guid>https://arxiv.org/abs/2502.17643</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从看法到决策：野火疏散决策预测具有行为理论的LLMS</title>
      <link>https://arxiv.org/abs/2502.17701</link>
      <description><![CDATA[ARXIV：2502.17701V1公告类型：新 
摘要：疏散决策预测对于有效有效的野火反应至关重要，通过帮助紧急管理预测交通拥堵和瓶颈，分配资源并最大程度地减少负面影响。疏散决策预测的传统统计方法无法捕获不同个体的复杂和多样化的行为逻辑。在这项工作中，我们首次引入耀斑，促进LLM的缩写，用于野火疏散决策预测的高级推理，这是一种基于大的语言模型（LLM）的框架，该框架集成了行为理论和模型，以简化思想链。 （COT）推理并随后与基于内存的增强学习（RL）模块集成，以提供准确的疏散决策预测和理解。我们提出的方法解决了使用现有LLM进行疏散行为预测的局限性，例如有限的调查数据，与行为理论不匹配，个人偏好相抵触，隐式和复杂的精神状态以及棘手的精神状态表现映射。三个后火灾调查数据集的实验表明，与传统理论知识的行为模型相比，绩效平均提高了20.47％，具有强烈的跨事概要。我们的完整代码可在https://github.com/susuxu-s-lab/flare上公开获取]]></description>
      <guid>https://arxiv.org/abs/2502.17701</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>注意手势：评估AI对文化冒犯性非语言手势的敏感性</title>
      <link>https://arxiv.org/abs/2502.17710</link>
      <description><![CDATA[ARXIV：2502.17710V1公告类型：新 
摘要：手势是非语言交流不可或缺的一部分，其含义在各种文化之间各不相同，并且误解可能会带来严重的社会和外交后果。随着AI系统更加集成到全球应用中，确保它们不会无意间永久存在文化犯罪至关重要。为此，我们介绍了多个文化的不适当手势和非语言标志（MC-Signs），这是一个288个手势国家配对的数据集，以注释了25个手势和85个国家 /地区的25个手势，文化意义和上下文因素。通过使用MC-Signs进行系统的评估，我们发现了临界局限性：文本对图像（T2i）系统表现出强烈的以美国为中心的偏见，在美国环境中检测进攻性手势的表现要比非US的偏见更好；大型语言模型（LLMS）倾向于过度限制手势。和视觉模型（VLMS）默认为基于美国的解释时，在响应普遍的概念之类的诸如希望某人运气之类的普遍概念时，经常暗示文化上不适当的手势。这些发现凸显了对文化意识的AI安全机制的迫切需求，以确保AI技术的公平全球部署。]]></description>
      <guid>https://arxiv.org/abs/2502.17710</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用编码样式的功能检测LLM-ParaphrApprApprApprAPHAPRAPHAPRAPRAPRAPRAPRAPRAPRAPRAPRAPRAM-PORAPHRAPRAPRAPRATY代码和负责LLM的识别</title>
      <link>https://arxiv.org/abs/2502.17749</link>
      <description><![CDATA[ARXIV：2502.17749V1公告类型：新 
摘要：代码生成的大型语言模型（LLMS）的最新进展引起了人们对知识产权保护的严重关注。恶意用户可以利用LLMS生成非常类似于原件的专有代码的释义版本。尽管LLM辅助代码释义的潜力不断增长，但检测其检测仍然有限的研究，强调了迫切需要检测系统。我们通过提出两个任务来应对这种需求。第一个任务是检测LLM生成的代码是否是原始人工编写代码的释义版本。第二个任务是确定哪个LLM用于解释原始代码。对于这些任务，我们构建了一个数据集LPCode，该数据集由使用各种LLM的成对人工编写的代码和LLM-Paraphratraphrater代码组成。
  我们从统计上证实了人工写入和LLM参数代码的编码方式的显着差异，尤其是在命名一致性，代码结构和可读性方面。基于这些发现，我们开发了LPCODEDEC，这是一种检测方法，它可以确定人写的和LLM生成的代码之间的释义关系，并发现哪些LLM用于释义。 LPCODEDEC在两项任务中的表现优于最佳基线，而F1分别提高了2.64％和15.17％，而达到1,343倍和213倍的加速度。]]></description>
      <guid>https://arxiv.org/abs/2502.17749</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Docpuzzle：一种用于评估现实长篇下说推理功能的过程感知基准</title>
      <link>https://arxiv.org/abs/2502.17807</link>
      <description><![CDATA[ARXIV：2502.17807V1公告类型：新 
摘要：我们提出了Docpuzzle，这是一种严格构建的基准，用于评估大语言模型（LLMS）中的长篇文章推理能力。该基准包括100个专家级别的质量检查问题，需要在长期实际文档上进行多个步骤推理。为了确保任务质量和复杂性，我们实施了人类协作注释 - 验证管道。 Docpuzzle引入了一个创新的评估框架，该框架通过清单引导的过程分析来减轻猜测偏见，从而建立了评估LLMS推理能力的新标准。我们的评估结果表明：1）先进的慢速推理模型，例如O1-Preiview（69.7％）和DeepSeek-R1（66.3％）明显胜过Claude 3.5 SONNET（57.7％）（57.7％）的最佳最佳通用指导模型； 2）像DeepSeek-R1-Distill-Qwen-32b（41.3％）这样的蒸馏推理模型远远落后于教师模型，这表明挑战以维持仅依赖蒸馏的推理能力的概括。]]></description>
      <guid>https://arxiv.org/abs/2502.17807</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过自动定理生成证明定理的组合身份基准</title>
      <link>https://arxiv.org/abs/2502.17840</link>
      <description><![CDATA[ARXIV：2502.17840V1公告类型：新 
摘要：大型语言模型（LLMS）具有明显的高级正式定理证明，但是高质量培训数据的稀缺性限制了它们在复杂的数学领域中的能力。数学的基石组合提供了组合，为分析离散结构和解决优化问题提供了必不可少的工具。但是，它固有的复杂性使其对于组合身份的自动定理（ATP）特别具有挑战性。为了解决这个问题，我们在Lean中手动构建LeanComb，组合身份基准，据我们所知，这是第一个正式定理证明为组合身份建立的基准。我们为组合身份的自动定理生成器ATG4CI开发了自动化的定理生成器，该生成器结合了由自我提出的大语言模型与强化学习树搜索方法进行战术预测所建议的候选策略。通过利用ATG4CI，我们生成一个由260K组合身份定理组成的精益增强的数据集，每个数据集都有完整的正式证明，并且实验评估表明，在该数据集上训练的模型可以产生更有效的策略，从而提高自动化理论的成功率用于组合身份。]]></description>
      <guid>https://arxiv.org/abs/2502.17840</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>跨语言的科学：评估LLM科学论文的多语言翻译</title>
      <link>https://arxiv.org/abs/2502.17882</link>
      <description><![CDATA[ARXIV：2502.17882V1公告类型：新 
摘要：科学研究本质上是全球性的。但是，绝大多数学术期刊都是用英语出版的，为非母语 - 英语研究人员造成了障碍。在这项研究中，我们利用大型语言模型（LLM）来翻译已发表的科学文章，同时保留其本地JATS XML格式，从而开发了一种实用的，自动化的方法来通过学术期刊实施。使用我们的方法，我们将跨多个科学学科的文章转化为28种语言。为了评估翻译精度，我们介绍了一种新颖的问答（QA）基准测试方法，其中LLM从原始文本中生成基于理解的问题，然后根据翻译文本回答它们。我们的基准结果表明，平均表现为95.9％，表明关键的科学细节是准确传达的。在一项用户研究中，我们将15位研究人员的科学论文转化为他们的母语，发现作者始终发现翻译以准确捕获其文章中的原始信息。有趣的是，三分之一的作者发现了许多技术术语“过度翻译”，表达了使术语在英语未翻译中更加熟悉的偏爱。最后，我们演示了如何使用文化学习技术与域特异性偏好相结合，例如减轻过度翻译，突出了LLM驱动的科学翻译的适应性和实用性。代码和翻译文章可在https://hankleid.github.io/projectmundo上找到。]]></description>
      <guid>https://arxiv.org/abs/2502.17882</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>迈向可持续的网络代理：透明度和奉献精力指标的认罪</title>
      <link>https://arxiv.org/abs/2502.17903</link>
      <description><![CDATA[ARXIV：2502.17903V1公告类型：新 
摘要：大语言模型领域的改进已转向构建能够使用外部工具和解释其输出的模型。这些所谓的Web代理具有与Internet自主互动的能力。这使他们能够成为强大的日常助手，以处理耗时，重复性的任务，同时支持用户日常活动。尽管Web代理研究蓬勃发展，但该研究方向的可持续性方面仍未得到探索。我们对与Web代理相关的能源和二氧化碳成本进行初步探索。我们的结果表明，网络代理创建中不同的哲学如何严重影响相关的消耗能量。我们强调，在估计能源消耗时，对于某些Web代理的披露，缺乏透明度，作为某些Web代理作为限制因素。因此，我们的工作提倡在评估网络代理时进行思维的改变，保证专门的指标用于能源消耗和可持续性。]]></description>
      <guid>https://arxiv.org/abs/2502.17903</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在建议系统中揭示性别偏见并增强类别意识公平</title>
      <link>https://arxiv.org/abs/2502.17921</link>
      <description><![CDATA[Arxiv：2502.17921V1公告类型：新 
摘要：推荐系统现在是我们日常生活中不可或缺的一部分。我们依靠它们来完成诸如发现新电影，在社交媒体上找到朋友以及将求职者与相关机会联系起来的任务。鉴于它们的重要作用，我们必须确保这些建议没有社会刻板印象。因此，在推荐系统中评估和解决此类偏见至关重要。评估推荐物品公平性的先前工作未能捕捉到某些细微差别，因为它们主要集中于比较不同敏感群体的性能指标。在本文中，我们介绍了一组综合指标，以量化建议中的性别偏见。具体来说，我们表明了在更详细的水平上评估公平性的重要性，可以使用我们的指标使用推荐项目（例如电影类型）来捕获性别偏见来实现这一目标。此外，我们表明，使用类别意识的公平度量作为正规化术语以及训练期间的主要建议损失可以有效地最大程度地减少模型输出中的偏见。我们使用五个基线模型与两个流行的公平感知模型一起在三个现实世界数据集上进行了实验，以显示我们指标在评估性别偏见方面的有效性。与以前的指标相比，我们的指标有助于增强对推荐项目的偏见的洞察力。此外，我们的结果表明，合并我们的正则化项如何显着提高不同类别的建议的公平性，而不会在总体建议性能中实质性降低。]]></description>
      <guid>https://arxiv.org/abs/2502.17921</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>精益：指导搜索神经定理通过证明进度预测证明</title>
      <link>https://arxiv.org/abs/2502.17925</link>
      <description><![CDATA[ARXIV：2502.17925V1公告类型：新 
摘要：由于幻觉，数学推理对于大语言模型（LLM）仍然是一个重大挑战。当与精益等正式的证明助手结合使用时，可以通过严格的验证来消除这些幻觉，从而使定理可靠。但是，即使经过正式验证，LLM仍然在长期证明和复杂的数学形式上挣扎。虽然LLM的Lean在检索引理，产生策略甚至完整的证据方面提供了宝贵的帮助，但它缺乏至关重要的能力：提供证据的感觉。这种限制特别影响大型正式项目的整体发展效率。我们介绍LeanProgress，这是一种预测证明进度的方法。培训和评估我们的模型对精益工作簿Plus和Mathlib4的大量精益证明的模型，以及要完成的步骤，我们采用数据预处理和平衡技术来处理证明长度的偏斜分布。我们的实验表明，LeanProgress在预测进度量以及剩余的步骤数量时达到了75.1 \％的总体预测准确性。当使用Reprover集成到最佳优先搜索框架中时，我们的方法显示了Mathlib4的3.8 \％改进，而基线性能为41.2 \％，尤其是对于更长的证明。这些结果表明了证明进度预测如何增强自动化定理和交互式定理，从而使用户能够就证明策略做出更明智的决定。]]></description>
      <guid>https://arxiv.org/abs/2502.17925</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GNN-XAR：用于智能家居中可解释活动识别的图形神经网络</title>
      <link>https://arxiv.org/abs/2502.17999</link>
      <description><![CDATA[ARXIV：2502.17999V1公告类型：新 
摘要：智能家庭环境中基于传感器的人类活动识别（HAR）对于多种应用，尤其是在医疗领域中至关重要。大多数现有方法都利用深度学习模型。尽管这些方法是有效的，但其产出背后的理由是不透明的。最近，出现了可解释的人工智能（XAI）方法，为HAR模型的输出提供了直观的解释。据我们所知，这些方法利用CNN或RNN等经典的深层模型。最近，事实证明，图形神经网络（GNN）对基于传感器的HAR有效。但是，现有的方法并未考虑解释性。在这项工作中，我们提出了第一个明确为Smart Home Har设计的可解释的图形神经网络。我们在两个公共数据集上的结果表明，这种方法比最先进的方法提供了更好的解释，同时也略微提高了识别率。]]></description>
      <guid>https://arxiv.org/abs/2502.17999</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在AI系统中定义偏见：偏见的模型是公平的模型</title>
      <link>https://arxiv.org/abs/2502.18060</link>
      <description><![CDATA[ARXIV：2502.18060V1公告类型：新 
摘要：关于AI系统中偏见的辩论对于讨论算法公平的讨论至关重要。但是，尽管经常与公平形成鲜明对比，但“偏见”一词通常缺乏明确的定义，这意味着无偏模型本质上是公平的。在本文中，我们挑战了这一假设，并认为偏见的精确概念化是有效解决公平关注的必要条件。我们没有将偏见视为固有的负面或不公平，而是强调了区分偏见和歧视的重要性。我们进一步探讨了这种重点转变如何在AI系统公平性的学术辩论中促进更具建设性的论述。]]></description>
      <guid>https://arxiv.org/abs/2502.18060</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>小屋：人类运动分析的多模式多模式</title>
      <link>https://arxiv.org/abs/2502.18180</link>
      <description><![CDATA[ARXIV：2502.18180V1公告类型：新 
摘要：多模式大语言模型（MLLM）的进步已经提高了人类运动的理解。但是，这些模型仍然受其“仅教学”性质的限制，缺乏互动性和适应性的分析观点。为了应对这些挑战，我们引入了ChatMotion，这是一个多式模式的人类运动分析框架。 ChatMotion动态解释用户意图，将复杂的任务分解为元任务，并激活专门的功能模块以进行运动理解。它整合了多个专业模块，例如运动核，以从各个角度分析人类运动。广泛的实验证明了Chatmotion的精度，适应性和用户参与人的运动理解。]]></description>
      <guid>https://arxiv.org/abs/2502.18180</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>柑橘：利用医学语言模型中的专家认知途径进行高级医学决策支持</title>
      <link>https://arxiv.org/abs/2502.18274</link>
      <description><![CDATA[ARXIV：2502.18274V1公告类型：新 
摘要：大型语言模型（LLM），尤其是具有推理能力的人，近年来已经迅速提高，在广泛的应用中表现出巨大的潜力。但是，他们在医疗保健中的部署，尤其是在疾病推理任务中的部署，受到获取专家级认知数据的挑战。在本文中，我们介绍了一种医学语言模型，该模型通过模拟医学专家的认知过程来弥合临床专业知识与AI推理之间的差距。该模型经过大量模拟专家疾病推理数据的培训，该数据使用一种新型方法合成，可准确捕获临床医生的决策途径。这种方法使柑橘能够更好地模拟诊断和治疗医疗状况所涉及的复杂推理过程。为了进一步解决缺乏医疗推理任务的公开可用数据集，我们发布了最后阶段的培训数据，包括定制的医学诊断对话。数据集。这种开源贡献旨在支持该领域的进一步研究和发展。使用权威的基准（例如MEDQA）进行的评估，涵盖医学推理和语言理解中的任务，表明柑橘与其他相似大小的模型相比，柑橘的性能优越。这些结果突出了柑橘的潜力，可以显着增强医疗决策支持系统，从而为临床决策提供了更准确，更有效的工具。]]></description>
      <guid>https://arxiv.org/abs/2502.18274</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GraphRank Pro+：通过知识图和情感增强技能分析来推进人才分析</title>
      <link>https://arxiv.org/abs/2502.18315</link>
      <description><![CDATA[ARXIV：2502.18315V1公告类型：新 
摘要：由于各种格式样式和主观内容组织，从半结构化文本中提取信息（例如简历）一直是一个挑战。常规解决方案依赖于针对特定用例量身定制的专门逻辑。但是，我们提出了一种革命性的方法，利用结构化图，自然语言处理（NLP）和深度学习。通过将复杂的逻辑抽象成图形结构，我们将原始数据转换为综合知识图。这个创新的框架可以进行精确的信息提取和复杂的查询。我们系统地构建了分配技能权重的字典，为细微的人才分析铺平了道路。我们的系统不仅使招聘人员和课程设计师受益，而且使求职者能够具有针对性的基于查询的过滤和排名能力。]]></description>
      <guid>https://arxiv.org/abs/2502.18315</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>