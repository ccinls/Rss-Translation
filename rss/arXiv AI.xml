<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.AI 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.AI 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Wed, 19 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>AI 应该优化你的代码吗？当前大型语言模型与传统优化编译器的比较研究</title>
      <link>https://arxiv.org/abs/2406.12146</link>
      <description><![CDATA[arXiv:2406.12146v1 公告类型：新
摘要：在当代计算机架构领域，对高效并行编程的需求依然存在，需要强大的优化技术。传统的优化编译器在历史上一直是这一努力的关键，适应现代软件系统不断变化的复杂性。大型语言模型 (LLM) 的出现引发了一个有趣的问题，即人工智能驱动的方法是否有可能彻底改变代码优化方法。
本文对两种最先进的大型语言模型 GPT-4.0 和 CodeLlama-70B 与传统优化编译器进行了比较分析，评估了它们在优化代码以实现最高效率方面的各自能力和局限性。此外，我们介绍了一套具有挑战性的优化模式基准测试套件和一种自动机制，用于评估此类工具生成的代码的性能和正确性。我们使用了两种不同的提示方法来评估 LLM 的性能——思路链 (CoT) 和指令提示 (IP)。然后，我们在一系列实际用例中将这些结果与三种传统的优化编译器 CETUS、PLUTO 和 ROSE 进行了比较。
一个关键的发现是，虽然 LLM 有可能胜过当前的优化编译器，但它们通常会在大代码量上生成不正确的代码，需要自动验证方法。我们对 3 种不同的基准测试套件进行了广泛的评估，结果表明 CodeLlama-70B 是两种 LLM 中性能更优越的优化器，能够实现高达 2.1 倍的加速。此外，CETUS 是优化编译器中最好的，最大加速为 1.9 倍。我们还发现两种提示方法之间没有显著差异：思路链 (Cot) 和指导提示 (IP)。]]></description>
      <guid>https://arxiv.org/abs/2406.12146</guid>
      <pubDate>Wed, 19 Jun 2024 06:27:29 GMT</pubDate>
    </item>
    <item>
      <title>元认知人工智能：神经符号方法的框架和案例</title>
      <link>https://arxiv.org/abs/2406.12147</link>
      <description><![CDATA[arXiv:2406.12147v1 公告类型：新
摘要：元认知是关于代理自身内部过程的推理概念，最初是在发展心理学领域引入的。在这篇立场文件中，我们研究了将元认知应用于人工智能的概念。我们引入了一个理解元认知人工智能 (AI) 的框架，我们称之为 TRAP：透明度、推理、适应和感知。我们依次讨论这些方面，并探索如何利用神经符号 AI (NSAI) 来应对元认知的挑战。]]></description>
      <guid>https://arxiv.org/abs/2406.12147</guid>
      <pubDate>Wed, 19 Jun 2024 06:27:29 GMT</pubDate>
    </item>
    <item>
      <title>人工智能系统的 ID</title>
      <link>https://arxiv.org/abs/2406.12137</link>
      <description><![CDATA[arXiv:2406.12137v1 公告类型：新
摘要：人工智能系统越来越普及，但决定是否以及如何与它们互动所需的信息可能不存在或无法访问。用户可能无法验证系统是否满足某些安全标准。当系统导致事故时，调查人员可能不知道要调查谁。平台可能很难惩罚与同一系统的重复负面互动。在许多领域，ID 通过识别 \textit{特定} 实体（例如，特定的波音 747）并提供有关同一类别的其他实体（例如，部分或全部波音 747）的信息来解决类似的问题。我们提出了一个框架，其中 ID 被归因于 AI 系统的 \textbf{实例}（例如，与 Claude 3 的特定聊天会话），并且相关信息可供寻求与该系统交互的各方访问。我们描述了人工智能系统的 ID，认为主要参与者对 ID 的需求可能很大，分析了这些参与者如何激励 ID 的采用，探索了我们框架的潜在实现，并强调了局限性和风险。ID 似乎在高风险环境中最有必要，在这种环境中，某些参与者（例如，那些使人工智能系统能够进行金融交易的参与者）可以尝试激励 ID 的使用。人工智能系统的部署者可以尝试开发 ID 实现。随着进一步研究，ID 可以帮助管理人工智能系统遍布社会的世界。]]></description>
      <guid>https://arxiv.org/abs/2406.12137</guid>
      <pubDate>Wed, 19 Jun 2024 06:27:28 GMT</pubDate>
    </item>
    <item>
      <title>文本嵌入模型中的偏见</title>
      <link>https://arxiv.org/abs/2406.12138</link>
      <description><![CDATA[arXiv:2406.12138v1 公告类型：新
摘要：文本嵌入正成为一种越来越流行的人工智能方法，尤其是在企业中，但文本嵌入模型存在偏见的可能性尚不清楚。本文研究了一些流行的文本嵌入模型的偏见程度，特别是在性别维度上。更具体地说，本文研究了这些模型将给定职业列表与性别术语相关联的程度。分析表明，文本嵌入模型容易受到性别偏见的影响，但方式各不相同。虽然存在某些模型间共性，例如，护士、家庭主妇和社交名媛等职业与女性标识符的关联性更强，而首席执行官、经理和老板等职业与男性标识符的关联性更强，但并非所有模型都对每种职业做出相同的性别关联。此外，偏见的程度和方向性也可能因模型而异，并取决于提示模型的特定词语。本文指出性别偏见困扰着文本嵌入模型，并建议使用这项技术的企业需要注意这个问题的具体方面。]]></description>
      <guid>https://arxiv.org/abs/2406.12138</guid>
      <pubDate>Wed, 19 Jun 2024 06:27:28 GMT</pubDate>
    </item>
    <item>
      <title>DTGB：动态文本属性图的综合基准</title>
      <link>https://arxiv.org/abs/2406.12072</link>
      <description><![CDATA[arXiv:2406.12072v1 公告类型：新 
摘要：动态文本属性图 (DyTAG) 在各种现实场景中普遍存在，其中每个节点和边都与文本描述相关联，并且图结构和文本描述都会随着时间的推移而发展。尽管它们具有广泛的适用性，但针对 DyTAG 定制的基准数据集却明显稀缺，这阻碍了许多研究领域的潜在进步。为了解决这一差距，我们引入了动态文本属性图基准 (DTGB)，它是来自不同领域的大规模、随时间演变的图的集合，其中的节点和边通过动态变化的文本属性和类别得到丰富。为了方便使用 DTGB，我们根据四个实际用例设计了标准化的评估程序：未来链接预测、目标节点检索、边分类和文本关系生成。这些任务要求模型同时理解动态图结构和自然语言，这凸显了 DyTAG 带来的独特挑战。此外，我们对 DTGB 进行了广泛的基准测试实验，评估了 7 种流行的动态图学习算法及其使用 LLM 嵌入适应文本属性的变体，以及 6 种强大的大型语言模型 (LLM)。我们的结果表明了现有模型在处理 DyTAG 方面的局限性。我们的分析还证明了 DTGB 在研究结构和文本动态结合方面的实用性。提出的 DTGB 促进了对 DyTAG 及其广泛应用的研究。它为评估和改进模型以处理动态图结构和自然语言之间的相互作用提供了全面的基准。数据集和源代码可在 https://github.com/zjs123/DTGB 上找到。]]></description>
      <guid>https://arxiv.org/abs/2406.12072</guid>
      <pubDate>Wed, 19 Jun 2024 06:27:27 GMT</pubDate>
    </item>
    <item>
      <title>模糊日志与声明性时间规范的一致性检查</title>
      <link>https://arxiv.org/abs/2406.12078</link>
      <description><![CDATA[arXiv:2406.12078v1 公告类型：新
摘要：传统的一致性检查任务假设事件数据提供了对实际流程执行的忠实而完整的表示。这一假设最近受到了质疑：越来越多的事件没有被明确追踪，而是作为事件识别管道的结果间接获得，因此本质上带有不确定性。在这项工作中，与不确定性的典型概率解释不同，我们考虑了在模糊语义下不确定性指的是实际进行的活动的相关情况。在这个新颖的环境中，我们考虑检查模糊事件数据是否符合声明模式或更一般地作为有限迹上的线性时间逻辑 (LTLf) 公式指定的声明性时间规则的问题。这需要放宽在每个时刻只执行一个活动的假设，并相应地重新定义具有模糊语义的逻辑布尔运算符。具体来说，我们提供了三重贡献。首先，我们定义了一个适合我们目的的 LTLf 的模糊对应物。其次，我们将模糊日志的一致性检查视为此逻辑中的验证问题。第三，我们基于 PyTorch Python 库提供概念验证的高效实现，适合一次检查多个模糊跟踪的一致性。]]></description>
      <guid>https://arxiv.org/abs/2406.12078</guid>
      <pubDate>Wed, 19 Jun 2024 06:27:27 GMT</pubDate>
    </item>
    <item>
      <title>$\tau$-bench：真实世界领域中工具-代理-用户交互的基准</title>
      <link>https://arxiv.org/abs/2406.12045</link>
      <description><![CDATA[arXiv:2406.12045v1 公告类型：新
摘要：现有基准测试并未测试语言代理与人类用户的交互或遵循特定领域规则的能力，而这两者对于在实际应用中部署它们都至关重要。我们提出了 $\tau$-bench，这是一个模拟用户（由语言模型模拟）与语言代理之间的动态对话的基准测试，该语言代理配备了特定领域的 API 工具和策略指南。我们采用高效而忠实的评估流程，将对话结束时的数据库状态与带注释的目标状态进行比较。我们还提出了一个新的指标（pass^k）来评估代理行为在多次试验中的可靠性。我们的实验表明，即使是最先进的函数调用代理（如 gpt-4o）也只能在不到 50% 的任务中取得成功，而且非常不一致（零售业中的 pass^8 &lt;25%）。我们的研究结果表明，需要能够提高代理一致行动和可靠遵循规则的能力的方法。]]></description>
      <guid>https://arxiv.org/abs/2406.12045</guid>
      <pubDate>Wed, 19 Jun 2024 06:27:26 GMT</pubDate>
    </item>
    <item>
      <title>WellDunn：论语言模型和大型语言模型在识别健康维度方面的稳健性和可解释性</title>
      <link>https://arxiv.org/abs/2406.12058</link>
      <description><![CDATA[arXiv:2406.12058v1 公告类型：新
摘要：语言模型 (LM) 被提议用于心理健康应用，其中不良后果的风险增加意味着预测性能可能不是模型在临床实践中效用的充分试金石。一个可以信赖的实践模型应该在解释和临床确定之间有对应关系，但之前没有研究检查过这些模型的注意力保真度及其对基本事实解释的影响。我们引入了一种评估设计，重点关注 LM 在识别健康维度 (WD) 方面的稳健性和可解释性。我们专注于两个心理健康和幸福感数据集：(a) 基于多标签分类的 MultiWD，以及 (b) WellXplain，用于根据专家标记的解释评估注意力机制的真实性。这些标签基于 Halbert Dunn 的健康理论，为我们的评估奠定了基础。我们揭示了有关 LM/LLM 的四个令人惊讶的结果：（1）尽管 GPT-3.5/4 具有与人类类似的能力，但落后于 RoBERTa 和 MedAlpaca，经过微调的 LLM 未能在性能或解释方面带来任何显着的改进。 （2）基于面向置信度的损失函数重新检查 LM 的预测，发现性能显着下降。 （3）在所有 LM/LLM 中，注意力和解释之间的一致性仍然很低，LLM 得分惨淡，为 0.0。 （4）大多数心理健康专用的 LM/LLM 忽视了特定领域的知识并低估了解释，从而导致了这些差异。这项研究强调需要进一步研究它们在心理健康和幸福感方面的一致性和解释。]]></description>
      <guid>https://arxiv.org/abs/2406.12058</guid>
      <pubDate>Wed, 19 Jun 2024 06:27:26 GMT</pubDate>
    </item>
    <item>
      <title>展望未来：测试 GPT-4 在路径规划中的极限</title>
      <link>https://arxiv.org/abs/2406.12000</link>
      <description><![CDATA[arXiv:2406.12000v1 公告类型：新 
摘要：大型语言模型 (LLM) 在各种任务中都表现出令人印象深刻的能力。然而，它们在长期规划方面仍然面临挑战。为了研究这一点，我们提出路径规划任务作为平台来评估 LLM 在几何约束下导航长轨迹的能力。我们提出的基准系统地测试了复杂环境中的路径规划技能。利用这一点，我们使用各种任务表示和提示方法检查了 GPT-4 的规划能力。我们发现将提示构建为 Python 代码并分解长轨迹任务可以提高 GPT-4 的路径规划效率。然而，虽然这些方法显示出提高模型规划能力的希望，但它们并没有获得最优路径，也无法在扩展视野范围内进行推广。]]></description>
      <guid>https://arxiv.org/abs/2406.12000</guid>
      <pubDate>Wed, 19 Jun 2024 06:27:25 GMT</pubDate>
    </item>
    <item>
      <title>成绩分数：量化 LLM 在选项选择方面的表现</title>
      <link>https://arxiv.org/abs/2406.12043</link>
      <description><![CDATA[arXiv:2406.12043v1 公告类型：新
摘要：本研究引入了“成绩分数”，这是一种新颖的指标，旨在评估大型语言模型 (LLM) 在用作多项选择评判者时在顺序偏差和选择一致性方面的一致性和公平性。成绩分数结合了熵（用于测量顺序偏差）和模式频率（用于评估选择稳定性），为 LLM 的可靠性和公正性提供了见解。该研究探索了诸如提示工程和选项抽样策略等技术来优化成绩分数，证明了它们在提高 LLM 性能方面的有效性。结果展示了 LLM 在提示方面的表现各不相同，并强调了包含不相关选项的积极影响。该研究还确定了指令遵循模型中的一种新兴行为，它们适应针对特定偏差的指令，展示了它们的适应性。成绩分数有助于 LLM 之间的比较，并鼓励持续研究优化其决策过程，这可能有助于提高其在各种应用中的可靠性和公平性。所有代码均可在 GitHub 上找到 https://github.com/IoDmitri/GradeLab]]></description>
      <guid>https://arxiv.org/abs/2406.12043</guid>
      <pubDate>Wed, 19 Jun 2024 06:27:25 GMT</pubDate>
    </item>
    <item>
      <title>提示设计对计算社会科学任务很重要，但方式却难以预测</title>
      <link>https://arxiv.org/abs/2406.11980</link>
      <description><![CDATA[arXiv:2406.11980v1 公告类型：新
摘要：手动注释计算社会科学任务的数据可能成本高昂、耗时且耗费情感。虽然最近的研究表明 LLM 可以在零样本设置中执行此类注释任务，但人们对提示设计如何影响 LLM 的合规性和准确性知之甚少。我们进行了一项大规模多提示实验，以测试模型选择（ChatGPT、PaLM2 和 Falcon7b）和提示设计特征（定义包含、输出类型、解释和提示长度）如何影响 LLM 生成的注释在四个 CSS 任务（毒性、情绪、谣言立场和新闻框架）上的合规性和准确性。我们的结果表明，LLM 的合规性和准确性高度依赖于提示。例如，提示数字分数而不是标签会降低所有 LLM 的合规性和准确性。整体最佳提示设置取决于任务，微小的提示更改可能会导致生成的标签分布发生很大变化。通过表明提示设计显著影响 LLM 生成的注释的质量和分布，这项工作为研究人员和从业人员提供了警告和实用指南。]]></description>
      <guid>https://arxiv.org/abs/2406.11980</guid>
      <pubDate>Wed, 19 Jun 2024 06:27:24 GMT</pubDate>
    </item>
    <item>
      <title>通过离散世界模型了解心智理论的复杂性</title>
      <link>https://arxiv.org/abs/2406.11911</link>
      <description><![CDATA[arXiv:2406.11911v1 公告类型：新
摘要：心智理论 (ToM) 可用于评估大型语言模型 (LLM) 在需要社会推理的复杂场景中的能力。虽然研究界已经提出了许多 ToM 基准，但它们的难度差异很大，而且复杂性定义不明确。这项工作提出了一个框架来衡量 ToM 任务的复杂性。我们将问题的复杂性量化为正确解决问题所需的状态数。我们的复杂性度量还考虑了旨在使其看起来更难的 ToM 问题的虚假状态。我们使用我们的方法来评估五个广泛采用的 ToM 基准的复杂性。在这个框架的基础上，我们设计了一种提示技术，它通过描述环境如何随着代理的交互而变化来增强模型可用的信息。我们将这种技术命名为离散世界模型 (DWM)，并展示它如何在 ToM 任务上获得卓越的表现。]]></description>
      <guid>https://arxiv.org/abs/2406.11911</guid>
      <pubDate>Wed, 19 Jun 2024 06:27:23 GMT</pubDate>
    </item>
    <item>
      <title>跟踪交互语言模型的视角</title>
      <link>https://arxiv.org/abs/2406.11938</link>
      <description><![CDATA[arXiv:2406.11938v1 公告类型：新
摘要：大型语言模型 (LLM) 能够以前所未有的速度生成高质量信息。随着这些模型继续在社会中扎根，它们生成的内容将在数据库中变得越来越普遍，而这些数据库又被纳入其他语言模型的预训练数据、微调数据、检索数据等。在本文中，我们形式化了 LLM 通信网络的概念，并介绍了一种在 LLM 集合中表示单个模型视角的方法。利用这些工具，我们系统地研究了各种模拟环境中 LLM 通信网络中的信息传播。]]></description>
      <guid>https://arxiv.org/abs/2406.11938</guid>
      <pubDate>Wed, 19 Jun 2024 06:27:23 GMT</pubDate>
    </item>
    <item>
      <title>最大割点的基准：面向组合优化学习启发式方法评估的标准化</title>
      <link>https://arxiv.org/abs/2406.11897</link>
      <description><![CDATA[arXiv:2406.11897v1 公告类型：新
摘要：最近，通过结合图神经网络 (GNN) 来学习特定于分布的解决方案结构，人们在设计基于图的组合优化问题的通用启发式方法方面投入了大量精力。然而，在评估这些启发式方法时，在所选的基线和实例方面缺乏一致性，这使得评估算法的相对性能变得困难。在本文中，我们提出了一个开源基准套件 MaxCut-Bench，专门用于 NP-hard 最大割问题的加权和非加权变体，该套件基于从不同图数据集中精心挑选的实例。该套件为各种启发式方法（包括传统方法和基于机器学习的方法）提供了统一的接口。接下来，我们利用基准测试试图系统地证实或重现几种流行的基于学习的方法的结果，包括 S2V-DQN [31]、ECO-DQN [4] 等，从三个维度来看：目标值、泛化和可扩展性。我们的实证结果表明，几种学习到的启发式方法无法超越朴素贪婪算法，并且只有其中一种始终优于 Tabu Search（一种基于局部搜索的简单通用启发式方法）。此外，我们发现，如果将 GNN 替换为与 Tabu Search 相关的特征子集上的简单线性回归，ECO-DQN 的性能将保持不变或得到改善。代码、数据和预训练模型可在以下位置获得：\url{https://github.com/ankurnath/MaxCut-Bench}。]]></description>
      <guid>https://arxiv.org/abs/2406.11897</guid>
      <pubDate>Wed, 19 Jun 2024 06:27:22 GMT</pubDate>
    </item>
    <item>
      <title>为归纳知识图谱补全提供更好的基准数据集</title>
      <link>https://arxiv.org/abs/2406.11898</link>
      <description><![CDATA[arXiv:2406.11898v1 公告类型：新
摘要：知识图谱补全 (KGC) 试图预测知识图谱 (KG) 中缺失的事实。最近，人们越来越关注设计可以在归纳设置中表现出色的 KGC 方法，其中推理中看到的部分或全部实体和关系在训练期间未被观察到。已经为归纳 KGC 提出了许多基准数据集，所有这些数据集都是用于传导 KGC 的现有 KG 的子集。然而，我们发现当前构建归纳 KGC 数据集的过程无意中创建了一种捷径，即使忽略关系信息也可以利用这种捷径。具体而言，我们观察到个性化 PageRank (PPR) 分数可以在大多数归纳数据集上实现强大或接近 SOTA 的性能。在本文中，我们研究了这个问题的根本原因。利用这些见解，我们提出了一种构建归纳 KGC 数据集的替代策略，有助于缓解 PPR 捷径。然后，我们使用新构建的数据集对多种流行方法进行基准测试，并分析其性能。新的基准数据集通过消除任何影响性能的捷径，有助于更好地理解归纳式 KGC 的功能和挑战。]]></description>
      <guid>https://arxiv.org/abs/2406.11898</guid>
      <pubDate>Wed, 19 Jun 2024 06:27:22 GMT</pubDate>
    </item>
    <item>
      <title>可解释人工智能在地球系统科学中的应用</title>
      <link>https://arxiv.org/abs/2406.11882</link>
      <description><![CDATA[arXiv:2406.11882v1 公告类型：新 
摘要：近年来，人工智能（AI）的影响力迅速扩大，如果加以适当利用，有望促进地球系统科学（ESS）的发展。在将AI应用于ESS时，一个重大障碍在于可解释性难题，这是由于AI算法的复杂性而产生的黑箱性质的固有问题。为了解决这个问题，可解释的AI（XAI）提供了一套强大的工具，使模型更加透明。本综述的目的有两个：首先，为ESS学者，特别是新手提供对XAI的基础理解，作为激发未来研究进步的入门读物；其次，鼓励ESS专业人士接受AI的好处，摆脱由于缺乏可解释性而产生的先入为主的偏见。我们首先阐明XAI的概念以及典型方法。然后，我们深入研究了 ESS 文献中 XAI 应用的回顾，强调了 XAI 在促进与 AI 模型决策的沟通、改进模型诊断和揭示科学见解方面发挥的重要作用。我们确定了 XAI 在 ESS 中面临的四个重大挑战，并提出了解决方案。此外，我们还全面阐述了多方面的观点。鉴于 ESS 中的独特挑战，将 AI 与领域特定知识无缝集成的可解释混合方法似乎是增强 AI 在 ESS 中实用性的有前途的方法。ESS 的远见卓识设想了一种和谐的融合，其中基于过程的模型控制已知，AI 模型探索未知，而 XAI 通过提供解释来弥合差距。]]></description>
      <guid>https://arxiv.org/abs/2406.11882</guid>
      <pubDate>Wed, 19 Jun 2024 06:27:21 GMT</pubDate>
    </item>
    <item>
      <title>预测用户对国际象棋走法精彩程度的感知</title>
      <link>https://arxiv.org/abs/2406.11895</link>
      <description><![CDATA[arXiv:2406.11895v1 公告类型：新
摘要：国际象棋的人工智能研究主要集中在生产能够最大限度提高获胜概率的更强大的代理。然而，国际象棋的另一个方面基本上没有得到研究：它的美学吸引力。具体来说，存在一类被称为“精彩”动作的国际象棋动作。这些动作因其高度的智力美学而受到玩家的赞赏和钦佩。我们展示了第一个将国际象棋动作分类为精彩的系统。该系统使用神经网络，使用国际象棋引擎的输出以及描述游戏树形状的特征。该系统的准确率为 79%（基准率为 50%），PPV 为 83%，NPV 为 75%。我们证明，人类认为的“精彩”动作不仅仅是最好的动作。我们表明，如果较弱的引擎认为某步棋质量较低（对于较强的引擎给出的相同评级），则在其他条件相同的情况下，该步棋更有可能被预测为绝妙之举。我们的系统为计算机象棋引擎开辟了道路，使其（看起来）能够展现出人类般的才华，从而展现出创造力。]]></description>
      <guid>https://arxiv.org/abs/2406.11895</guid>
      <pubDate>Wed, 19 Jun 2024 06:27:21 GMT</pubDate>
    </item>
    <item>
      <title>基于逻辑的可解释性：过去、现在和未来</title>
      <link>https://arxiv.org/abs/2406.11873</link>
      <description><![CDATA[arXiv:2406.11873v1 公告类型：新
摘要：近年来，机器学习 (ML) 和人工智能 (AI) 对社会的影响非常显著。预计这种影响将在可预见的未来持续下去。然而，AI/ML 的采用也引起了严重担忧。最先进的 AI/ML 模型的运行通常超出了人类决策者的理解范围。因此，影响人类的决策可能无法理解，并且可能缺乏严格的验证。可解释的人工智能 (XAI) 致力于为人类决策者提供对 ML 模型所做预测的可理解解释。因此，XAI 是值得信赖的人工智能的基石。尽管具有战略重要性，但大多数关于 XAI 的工作都缺乏严谨性，因此它在高风险或安全关键领域的使用会助长不信任，而不是有助于建立急需的信任。基于逻辑的 XAI 最近已成为其他非严格 XAI 方法的严格替代方案。本文对基于逻辑的可解释人工智能进行了技术概述，包括其起源、当前的研究课题以及未来新兴的研究课题。本文还强调了可解释人工智能非严格方法中普遍存在的许多误解。]]></description>
      <guid>https://arxiv.org/abs/2406.11873</guid>
      <pubDate>Wed, 19 Jun 2024 06:27:20 GMT</pubDate>
    </item>
    <item>
      <title>ChatPCG：大型语言模型驱动的程序内容生成奖励设计</title>
      <link>https://arxiv.org/abs/2406.11875</link>
      <description><![CDATA[arXiv:2406.11875v1 公告类型：新
摘要：在机器学习快速发展的推动下，游戏人工智能 (AI) 的最新进展显著影响了各种游戏类型的生产力。奖励设计在训练游戏 AI 模型中起着关键作用，研究人员在其中实现特定奖励功能的概念。然而，尽管存在人工智能，奖励设计过程主要仍然属于人类专家的领域，因为它严重依赖于他们的创造力和工程技能。因此，本文提出了 ChatPCG，一个大型语言模型 (LLM) 驱动的奖励设计框架。它利用人类层面的洞察力，加上游戏专业知识，自动生成针对特定游戏功能的奖励。此外，ChatPCG 与深度强化学习相结合，展示了其在多人游戏内容生成任务中的潜力。结果表明，所提出的 LLM 表现出理解游戏机制和内容生成任务的能力，从而能够为特定游戏生成定制的内容。这项研究不仅强调了提高内容生成可访问性的潜力，而且旨在简化游戏 AI 开发流程。]]></description>
      <guid>https://arxiv.org/abs/2406.11875</guid>
      <pubDate>Wed, 19 Jun 2024 06:27:20 GMT</pubDate>
    </item>
    <item>
      <title>生成式 AI 投票：公平的集体选择能够抵御 LLM 偏见和不一致</title>
      <link>https://arxiv.org/abs/2406.11871</link>
      <description><![CDATA[arXiv:2406.11871v1 公告类型：新
摘要：扩大审议和投票参与是一项长期努力——直接民主和合法集体选择的基石。生成人工智能 (AI) 和大型语言模型 (LLM) 的最新突破为数字民主提供了前所未有的机遇，但也警示了风险。人工智能个人助理可以克服人类的认知带宽限制，提供决策支持能力，甚至可以大规模直接代表人类选民。然而，这种代表的质量以及将集体决策委托给 LLM 时表现出的潜在偏见是一个令人震惊且需要及时解决的挑战。通过在 81 场现实世界的投票选举中严格模拟超过 50K 个 LLM 投票角色，我们表明，与更简单、更一致的多数选举相比，不同的 LLM（GPT 3、GPT 3.5 和 Llama2）在复杂的优先选票格式中存在偏见和显著的不一致性。令人惊讶的是，公平投票汇总方法（例如平等份额）被证明是双赢的：人类的投票结果更公平，人工智能的代表也更公平。这种新颖的潜在关系对于进步主义场景中的民主韧性至关重要，在人工智能代表的支持下，选民投票率低，选民疲劳：通过恢复更公平的高度代表性投票结果，可以减轻弃权选民的负担。这些见解为科学、政策制定者和公民提供了重要基础，以解释和减轻民主创新中的人工智能风险。]]></description>
      <guid>https://arxiv.org/abs/2406.11871</guid>
      <pubDate>Wed, 19 Jun 2024 06:27:19 GMT</pubDate>
    </item>
    </channel>
</rss>