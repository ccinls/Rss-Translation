<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.AI 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.AI 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Tue, 02 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>具有安全约束的离线多智能体强化学习的扩散模型</title>
      <link>https://arxiv.org/abs/2407.00741</link>
      <description><![CDATA[arXiv:2407.00741v1 公告类型：新
摘要：多智能体强化学习 (MARL) 的最新进展使其应用扩展到各种安全关键场景。然而，大多数方法都侧重于在线学习，这在实际环境中部署时会带来很大的风险。为了应对这一挑战，我们引入了一个创新框架，将扩散模型集成到 MARL 范式中。这种方法通过降低风险和建模协调行动，显著提高了多个智能体采取的行动的安全性。我们的框架以集中训练和分散执行 (CTDE) 架构为基础，并通过扩散模型进行增强，以生成预测轨迹。此外，我们还采用了专门的算法来进一步确保操作安全。我们根据 DSRL 基准对我们的模型进行了评估。实验结果表明，我们的模型不仅遵守严格的安全约束，而且与现有方法相比，性能更佳。这强调了我们的方法在提高 MARL 在实际应用中的安全性和有效性方面的潜力。]]></description>
      <guid>https://arxiv.org/abs/2407.00741</guid>
      <pubDate>Wed, 03 Jul 2024 03:22:07 GMT</pubDate>
    </item>
    <item>
      <title>因果认知的解耦表征</title>
      <link>https://arxiv.org/abs/2407.00744</link>
      <description><![CDATA[arXiv:2407.00744v1 公告类型：新
摘要：复杂自适应代理通过解决似乎需要理解因果信息的问题来持续实现其目标，因果信息涉及代理-环境系统组合元素之间存在的因果关系。因果认知研究并描述人类和非人类动物因果学习和推理的主要特征，提供了一个概念框架来讨论基于对任务的明显因果理解水平的认知表现。尽管使用了正式的基于干预的因果关系模型，包括因果贝叶斯网络，但对因果认知的心理和行为研究尚未提供计算说明来操作代理如何获得对世界的因果理解。机器学习和强化学习对因果关系的研究，尤其是将解耦作为构建因果表示的候选过程，一方面代表了设计因果人工智能体的具体尝试，这些因果人工智能体可以阐明自然因果认知的内部运作。在这项工作中，我们将这两个研究领域联系起来，为因果认知构建一个统一的框架，该框架将为动物认知研究提供计算视角，并为开发人工智能中因果强化学习的新算法提供见解。]]></description>
      <guid>https://arxiv.org/abs/2407.00744</guid>
      <pubDate>Wed, 03 Jul 2024 03:22:07 GMT</pubDate>
    </item>
    <item>
      <title>BAPO：大型语言模型中个性化对齐的基锚偏好优化</title>
      <link>https://arxiv.org/abs/2407.00693</link>
      <description><![CDATA[arXiv:2407.00693v1 公告类型：新
摘要：虽然学习将大型语言模型 (LLM) 与人类偏好对齐已经取得了显著的成功，但对齐这些模型以满足不同的用户偏好在保留先前知识方面提出了进一步的挑战。本文研究了个性化偏好优化对 LLM 的影响，发现知识丢失的程度随偏好异质性而显着变化。虽然以前的方法利用了参考模型和策略模型之间的 KL 约束，但我们观察到它们在面对个性化偏好时无法保持一般知识和对齐。为此，我们引入了基础锚定偏好优化 (BAPO)，这是一种简单而有效的方法，它利用参考模型的初始响应来减轻遗忘，同时适应个性化对齐。BAPO 有效地适应不同的用户偏好，同时对全局知识或一般对齐的影响最小。我们的实验证明了 BAPO 在各种设置中的有效性。]]></description>
      <guid>https://arxiv.org/abs/2407.00693</guid>
      <pubDate>Wed, 03 Jul 2024 03:22:06 GMT</pubDate>
    </item>
    <item>
      <title>从内在动机学习形式数学</title>
      <link>https://arxiv.org/abs/2407.00695</link>
      <description><![CDATA[arXiv:2407.00695v1 公告类型：新
摘要：人类是如何从以太中诱导出数学的？我们探索了柏拉图的观点，即数学可以从其公理中发现——一个猜想和证明的游戏。我们描述了 Minimo（内在动机数学）：一个共同学习为自己提出具有挑战性的问题（猜想）并解决它们（定理证明）的代理。给定一个在依赖类型理论中公理化的数学领域，我们首先结合约束解码和类型导向合成的方法，从语言模型中抽取有效的猜想。即使我们从随机初始化的模型开始，我们的方法也能通过构造保证格式良好的猜想。我们使用相同的模型来表示用于指导证明搜索的策略和价值函数。我们的代理的目标是生成困难但可证明的猜想——这是一个移动的目标，因为它自己的定理证明能力也会随着训练而提高。我们提出了在证明搜索树上进行事后重新标记的新方法，以显著提高代理在这两项任务中的采样效率。在 3 个公理领域（命题逻辑、算术和群论）进行的实验表明，我们的代理可以仅从公理开始，在生成真实且具有挑战性的猜想和寻找证明方面自我改进。]]></description>
      <guid>https://arxiv.org/abs/2407.00695</guid>
      <pubDate>Wed, 03 Jul 2024 03:22:06 GMT</pubDate>
    </item>
    <item>
      <title>上下文组合赌博法在谈判中的应用</title>
      <link>https://arxiv.org/abs/2407.00567</link>
      <description><![CDATA[arXiv:2407.00567v1 公告类型：新
摘要：学习有效的谈判策略面临两个关键挑战：探索-利用困境和处理大型行动空间。然而，缺乏基于学习的方法可以有效地解决谈判中的这些挑战。本文介绍了一种全面的公式来解决各种谈判问题。我们的方法利用上下文组合多臂老虎机，老虎机解决了探索-利用困境，而组合性质可以处理大型行动空间。在此公式的基础上，我们引入了 NegUCB，这是一种新方法，它还可以处理谈判中的常见问题，例如部分观察和复杂的奖励函数。NegUCB 是上下文相关的，并且针对全老虎机反馈进行了定制，而对奖励函数没有限制。在温和的假设下，它确保了亚线性遗憾上限。在三个谈判任务上进行的实验证明了我们方法的优越性。]]></description>
      <guid>https://arxiv.org/abs/2407.00567</guid>
      <pubDate>Wed, 03 Jul 2024 03:22:05 GMT</pubDate>
    </item>
    <item>
      <title>利用文本子空间实现高效的个性化文本到图像生成</title>
      <link>https://arxiv.org/abs/2407.00608</link>
      <description><![CDATA[arXiv:2407.00608v1 公告类型：新
摘要：个性化文本到图像生成在最近几年引起了前所未有的关注，因为它具有通过使用输入概念数据集和新颖的文本提示生成高度个性化图像的独特能力。然而，以前的方法仅仅关注重建任务的性能，降低了其与不同文本提示相结合的能力。此外，在高维嵌入空间中进行优化通常会导致不必要的耗时训练过程和缓慢的收敛。为了解决这些问题，我们提出了一种有效的方法来探索文本子空间中的目标嵌入，从自我表达属性中汲取灵感。此外，我们提出了一种有效的选择策略来确定文本子空间的基向量。实验评估表明，学习到的嵌入不仅可以忠实地重建输入图像，还可以显着提高其与新输入文本提示的对齐程度。此外，我们观察到，在文本子空间中进行优化可以显著提高对初始词的鲁棒性，从而放宽要求用户输入最相关初始词的限制。我们的方法为个性化文本到图像生成的更高效的表示学习打开了大门。]]></description>
      <guid>https://arxiv.org/abs/2407.00608</guid>
      <pubDate>Wed, 03 Jul 2024 03:22:05 GMT</pubDate>
    </item>
    <item>
      <title>基于规则的自动驾驶行为规划器</title>
      <link>https://arxiv.org/abs/2407.00460</link>
      <description><![CDATA[arXiv:2407.00460v1 公告类型：新
摘要：自动驾驶汽车需要高度复杂的决策来确定其运动。本文介绍了如何使用从专家驾驶决策中学习到的实用规则引擎实现此类功能。我们提出了一种算法来创建和维护基于规则的行为规划器，使用两层规则理论。第一层根据感知到的环境状态确定一组可行的参数化行为。从这些行为中，解析函数选择最保守的高级操作。然后，第二层将参数协调为单一行为。为了证明我们的方法的实用性，我们报告了它在 3 级自动驾驶汽车中的实施结果以及在城市环境中的现场测试结果。]]></description>
      <guid>https://arxiv.org/abs/2407.00460</guid>
      <pubDate>Wed, 03 Jul 2024 03:22:04 GMT</pubDate>
    </item>
    <item>
      <title>ShapG：基于Shapley值的新型特征重要性方法</title>
      <link>https://arxiv.org/abs/2407.00506</link>
      <description><![CDATA[arXiv:2407.00506v1 公告类型：新 
摘要：随着人工智能 (AI) 的广泛应用，使 AI 系统的决策可解释且透明变得尤为重要。在本文中，我们提出了一种新的可解释人工智能 (XAI) 方法 ShapG（基于图的 Shapley 值的解释）来测量特征重要性。ShapG 是一种与模型无关的全局解​​释方法。在第一阶段，它基于数据集定义一个无向图，其中节点表示特征，并根据特征之间的相关系数计算添加边。在第二阶段，它通过考虑此图结构对数据进行采样来计算近似的 Shapley 值。ShapG 的采样方法可以有效地计算特征的重要性，即降低计算复杂度。将 ShapG 与其他现有的 XAI 方法进行比较，可以发现它为两个检查的数据集提供了更准确的解释。我们还对基于合作博弈论开发的其他XAI方法与ShapG在运行时间上进行了比较，结果表明ShapG在运行时间上具有明显优势，进一步证明了ShapG的高效性。此外，大量实验证明了ShapG方法对解释复杂模型的广泛适用性。我们认为ShapG是提高AI系统可解释性和透明度的重要工具，并相信它可以广泛应用于各个领域。]]></description>
      <guid>https://arxiv.org/abs/2407.00506</guid>
      <pubDate>Wed, 03 Jul 2024 03:22:04 GMT</pubDate>
    </item>
    <item>
      <title>利用本体来记录数据中的偏见</title>
      <link>https://arxiv.org/abs/2407.00509</link>
      <description><![CDATA[arXiv:2407.00509v1 公告类型：新
摘要：机器学习 (ML) 系统能够重现并经常放大不良偏见。这强调了在能够研究和理解 ML 管道内在特征的实践下操作的重要性，促使文档框架的出现，其理念是“任何偏见的补救措施都始于对其存在的认识”。然而，仍然缺少一种能够正式描述这些管道检测到的偏见的资源。为了填补这一空白，我们提出了 Doc-BiasO 本体，这是一种旨在创建一个集成词汇表的资源，其中包含 \textit{fair-ML} 文献中定义的偏见及其度量，以及纳入相关术语及其之间的关系。通过监督本体工程最佳实践，我们重新使用机器学习和人工智能方面的现有词汇，以促进与其研究、开发、监管等有关的参与者之间的知识共享和互操作性。总的来说，我们的主要目标是帮助澄清偏见研究的现有术语，因为它迅速扩展到人工智能的所有领域，并改善对数据偏见和下游影响的解释。]]></description>
      <guid>https://arxiv.org/abs/2407.00509</guid>
      <pubDate>Wed, 03 Jul 2024 03:22:04 GMT</pubDate>
    </item>
    <item>
      <title>测试用例特征作为归纳编程的超启发式方法</title>
      <link>https://arxiv.org/abs/2407.00519</link>
      <description><![CDATA[arXiv:2407.00519v1 公告类型：新
摘要：指令子集是一种启发式方法，可以将归纳编程搜索空间的大小减少数十个数量级。它们由许多不同大小的重叠子集组成，可作为对编码任何问题解决方案所需指令的预测。目前，这种方法采用单一、大型子集系列，这意味着某些问题在找到解决方案之前可以搜索数千个子集。在本文中，我们介绍了使用测试用例类型签名作为超启发式方法来选择许多较小的指令子集系列之一。任何一组测试用例的类型签名都直接映射到单个系列，较小的系列意味着大多数问题需要考虑的子集较少。拥有许多系列还允许对子集进行重新排序，以更好地反映它们在人类代码中的相对出现 - 再次减少许多问题的搜索空间大小。总体而言，新方法可以进一步将归纳编程搜索空间的大小减少 1 到 3 个数量级，具体取决于类型签名。通过使用更复杂的类型系统，可以实现更大、更一致的减少。还简要讨论了将其他测试用例功能用作超启发式方法的潜在用途以及其他一些可能的未来工作。]]></description>
      <guid>https://arxiv.org/abs/2407.00519</guid>
      <pubDate>Wed, 03 Jul 2024 03:22:04 GMT</pubDate>
    </item>
    <item>
      <title>外部模型激励代理：增强环境采样的强化学习</title>
      <link>https://arxiv.org/abs/2407.00264</link>
      <description><![CDATA[arXiv:2407.00264v1 公告类型：新
摘要：与强化学习 (RL) 代理不同，人类在不断变化的环境中仍然能够胜任多任务处理。尽管人们只通过自己的观察和互动来体验世界，但他们知道如何在专注于任务和了解变化如何影响他们对世界的理解之间取得平衡。这可以通过选择以有趣且通常具有信息量的方式解决任务来实现，而不仅仅是当前任务。受此启发，我们为 RL 代理提出了一个代理影响框架，以提高外部模型在不断变化的环境中的适应效率，而不会对代理的奖励进行任何改变。我们的公式由两个独立的模块组成：兴趣领域和通过兴趣领域进行的行为塑造。我们实现了基于不确定性的兴趣领域算法以及基于技能采样的行为塑造算法来测试该框架。我们的结果表明，在衡量效率和性能的指标上，我们的方法在外部模型适应方面优于基线。]]></description>
      <guid>https://arxiv.org/abs/2407.00264</guid>
      <pubDate>Wed, 03 Jul 2024 03:22:03 GMT</pubDate>
    </item>
    <item>
      <title>UDC：用于大规模组合优化问题的统一神经分而治之框架</title>
      <link>https://arxiv.org/abs/2407.00312</link>
      <description><![CDATA[arXiv:2407.00312v1 公告类型：新
摘要：单阶段神经组合优化求解器在各种小规模组合优化 (CO) 问题上取得了近乎最优的结果，而无需专业知识。然而，这些求解器在应用于大规模 CO 问题时表现出显著的性能下降。最近，采用分而治之策略的两阶段神经方法在解决大规模 CO 问题方面表现出优势。然而，这些方法的效率高度依赖于分而治之过程中针对特定问题的启发式方法，这限制了它们对一般 CO 问题的适用性。此外，这些方法采用单独的训练方案，忽略了分而治之策略之间的相互依赖性，这往往会导致次优解决方案。为了解决这些缺点，本文开发了一个统一的神经分而治之框架 (即 UDC) 来解决一般的大规模 CO 问题。 UDC 提供了一种分治重聚 (DCR) 训练方法来消除次优划分策略的负面影响。采用高效图神经网络 (GNN) 进行全局划分，采用固定长度子路径求解器来解决子问题，所提出的 UDC 框架具有广泛的适用性，在 10 个具有代表性的大规模 CO 问题中取得了优异的性能。]]></description>
      <guid>https://arxiv.org/abs/2407.00312</guid>
      <pubDate>Wed, 03 Jul 2024 03:22:03 GMT</pubDate>
    </item>
    <item>
      <title>GraphArena：对图形计算问题中的大型语言模型进行基准测试</title>
      <link>https://arxiv.org/abs/2407.00379</link>
      <description><![CDATA[arXiv:2407.00379v1 公告类型：新
摘要：大型语言模型 (LLM) 的“军备竞赛”需要新颖、具有挑战性和多样化的基准来忠实地检查其进展。我们推出了 GraphArena，这是一种基准测试工具，旨在使用来自知识图谱、社交网络和分子结构等不同场景的百万级真实世界图来评估 LLM 的图形计算问题。GraphArena 提供了一套 10 个计算任务，包括四个多项式时间（例如最短距离）和六个 NP 完全挑战（例如旅行商问题）。它具有严格的评估框架，将 LLM 输出分类为正确、次优（可行但不最优）或幻觉（格式正确但不可行）。对包括 GPT-4o 和 LLaMA3-70B-Instruct 在内的 10 个领先 LLM 的评估表明，即使是表现最好的模型也难以解决更大、更复杂的图形问题，并且会出现幻觉问题。尽管采用了诸如思路链提示之类的策略，但这些问题仍未得到解决。GraphArena 为现有的 LLM 基准提供了宝贵的补充，并在 https://github.com/squareRoot3/GraphArena 上开源。]]></description>
      <guid>https://arxiv.org/abs/2407.00379</guid>
      <pubDate>Wed, 03 Jul 2024 03:22:03 GMT</pubDate>
    </item>
    <item>
      <title>分析文本到图像生成模型的质量、偏差和性能</title>
      <link>https://arxiv.org/abs/2407.00138</link>
      <description><![CDATA[arXiv:2407.00138v1 公告类型：新
摘要：生成模型的进步引起了人们对图像合成的极大兴趣，展示了为各种文本提示生成高质量图像的能力。尽管取得了这些进展，但大多数研究都忽略了偏见​​的存在。在本文中，我们不仅通过定性评估它们在生成人脸、群体和指定数量物体的准确图像方面的表现，而且还通过进行社会偏见分析来研究几种文本到图像模型。正如预期的那样，容量更大的模型会生成更高质量的图像。然而，我们还记录了这些模型所具有的固有性别或社会偏见，从而更全面地了解它们的影响和局限性。]]></description>
      <guid>https://arxiv.org/abs/2407.00138</guid>
      <pubDate>Wed, 03 Jul 2024 03:22:02 GMT</pubDate>
    </item>
    <item>
      <title>考虑深度强化学习用于先进空中机动应急管理时的权衡</title>
      <link>https://arxiv.org/abs/2407.00197</link>
      <description><![CDATA[arXiv:2407.00197v1 公告类型：新
摘要：随着先进空中机动性 (AAM) 的引入，全球航空运输正在经历快速发展，随之而来的是航空转型的新挑战和机遇。随着 AAM 运营引入越来越多的车辆能力和密度异质性，提高自动化水平可能是实现运营安全和效率目标的必要条件。本文重点介绍一个建议提高自动化水平的例子。自主运营将需要应急管理系统，该系统可以监控一系列相互关联（或相互依赖）的危险中不断变化的风险，并在必要时通过监督或自动决策执行适当的控制干预。适应这种复杂的环境可能需要自动化功能（自主性），这些功能应用人工智能 (AI) 技术，可以适应和响应快速变化的环境。本文探讨了深度强化学习 (DRL) 的使用，它在复杂和高维环境中表现出良好的性能，其中目标可以构建为顺序决策问题。本文将应急管理问题先前的表述扩展为马尔可夫决策过程 (MDP)，并使用 DRL 框架来训练代理，以减轻模拟环境中存在的危险。本文从性能、验证难度和开发过程等方面对这些基于学习的代理和经典技术进行了比较。]]></description>
      <guid>https://arxiv.org/abs/2407.00197</guid>
      <pubDate>Wed, 03 Jul 2024 03:22:02 GMT</pubDate>
    </item>
    <item>
      <title>一个提示是不够的：自动构建混合专家提示</title>
      <link>https://arxiv.org/abs/2407.00256</link>
      <description><![CDATA[arXiv:2407.00256v1 公告类型：新 
摘要：大型语言模型 (LLM) 在语言指令和上下文演示的提示下表现出对新任务的强大泛化能力。由于这种能力敏感地取决于提示的质量，因此已经探索了各种方法来自动化指令设计。虽然这些方法显示出有希望的结果，但它们也将搜索到的提示限制为一条指令。这种简化大大限制了它们的能力，因为单个无演示的指令可能无法覆盖目标任务的整个复杂问题空间。为了缓解这个问题，我们采用混合专家范式，将问题空间划分为一组子区域；每个子区域由一位专门的专家管理，配备一条指令和一组演示。开发了一个两阶段过程来为每个区域构建专门的专家：（1）演示分配：受上下文学习和核回归之间理论联系的启发，我们根据演示的语义相似性将其分组为专家； （2）指令分配：每个专家基于区域的联合搜索指令与分配给它的演示相辅相成，产生协同效应。由此产生的方法，代号为 Mixture-of-Prompts (MoP)，在几个主要基准测试中，与现有技术相比，平均胜率为 81%。]]></description>
      <guid>https://arxiv.org/abs/2407.00256</guid>
      <pubDate>Wed, 03 Jul 2024 03:22:02 GMT</pubDate>
    </item>
    <item>
      <title>逻辑突破：理解基于规则的推理颠覆的框架</title>
      <link>https://arxiv.org/abs/2407.00075</link>
      <description><![CDATA[arXiv:2407.00075v1 公告类型：新
摘要：我们研究如何颠覆语言模型，使其不遵循规则。我们将规则遵循建模为命题霍恩逻辑中的推理，这是一个数学系统，其中规则的形式为“如果 $P$ 和 $Q$，则 $R$”，适用于某些命题 $P$、$Q$ 和 $R$。我们证明，尽管转换器可以忠实地遵守这些规则，但恶意制作的提示仍然可以误导即使是理论构建的模型。从经验上讲，我们发现对我们的理论模型的攻击反映了对大型语言模型的流行攻击。我们的工作表明，研究较小的理论模型有助于理解大型语言模型在基于规则的设置（如逻辑推理和越狱攻击）中的行为。]]></description>
      <guid>https://arxiv.org/abs/2407.00075</guid>
      <pubDate>Wed, 03 Jul 2024 03:22:01 GMT</pubDate>
    </item>
    <item>
      <title>ARES：交替强化学习和监督微调，通过多样化的人工智能反馈增强多模态思维链推理</title>
      <link>https://arxiv.org/abs/2407.00087</link>
      <description><![CDATA[arXiv:2407.00087v1 公告类型：新
摘要：大型多模态模型 (LMM) 擅长理解人类指令，并在广泛的任务中表现出色。强化学习来自人类反馈 (RLHF) 和人工智能反馈 (RLAIF) 通过将 LLM 与特定偏好相结合来进一步完善 LLM。这些方法主要对整个世代使用基于排名的反馈。借助先进的人工智能模型 (Teacher)，例如 GPT-4 和 Claude 3 Opus，我们可以请求各种类型的详细反馈，而这些反馈对于人类来说成本高昂。我们提出了一种两阶段算法 ARES，它可以交替使用强化学习 (RL) 和监督微调 (SFT)。首先，我们要求老师在思路链 (CoT) 中对每个句子对解决问题的贡献进行评分。这种句子级反馈使我们能够考虑单个有价值的片段，为 RL 程序提供更细粒度的奖励。其次，我们要求 Teacher 在强化学习阶段后纠正错误的推理。强化学习过程需要大量精力进行超参数调整，并且经常会产生重复单词和不完整句子等错误。通过纠正反馈，我们通过 SFT 稳定了强化学习微调模型。我们在多模型数据集 ScienceQA 和 A-OKVQA 上进行了实验，以证明我们提案的有效性。根据 GPT-4o 的判断，ARES 理性推理对基线模型的胜率约为 70%。此外，我们观察到，改进的理性推理使多模态数据集的推理答案准确率平均提高了 2.5%。]]></description>
      <guid>https://arxiv.org/abs/2407.00087</guid>
      <pubDate>Wed, 03 Jul 2024 03:22:01 GMT</pubDate>
    </item>
    <item>
      <title>多模态大型语言模型 (MLLM) 中的视觉推理和多智能体方法：解决 TSP 和 mTSP 组合挑战</title>
      <link>https://arxiv.org/abs/2407.00092</link>
      <description><![CDATA[arXiv:2407.00092v1 公告类型：新
摘要：多模态大型语言模型 (MLLM) 利用涵盖文本、图像和音频的全面知识来熟练地解决复杂问题，包括零样本上下文学习场景。本研究探讨了 MLLM 使用在二维平面上描绘点分布的图像在视觉上解决旅行商问题 (TSP) 和多旅行商问题 (mTSP) 的能力。我们引入了一种新方法，在 MLLM 框架内采用多个专门的代理，每个代理都致力于优化这些组合挑战的解决方案。我们的实验调查包括对零样本设置的严格评估，并引入了创新的多代理零样本上下文场景。结果表明，两种多代理模型。多代理 1，包括初始化器、评论家和评分器代理，以及多代理 2，仅包含初始化器和评论家代理；显着提高了 TSP 和 mTSP 问题的解决方案质量。多智能体 1 在需要详细路线细化和评估的环境中表现出色，为复杂的优化提供了强大的框架。相比之下，多智能体 2 专注于初始化器和评论家的迭代细化，被证明对快速决策场景有效。这些实验产生了有希望的结果，展示了 MLLM 在解决各种组合问题方面的强大视觉推理能力。这些发现强调了 MLLM 作为计算优化强大工具的潜力，提供了可能激发这一有前途的领域进一步发展的见解。项目链接：https://github.com/ahmed-abdulhuy/Solving-TSP-and-mTSP-Combinatorial-Challenges-using-Visual-Reasoning-and-Multi-Agent-Approach-MLLMs-.git]]></description>
      <guid>https://arxiv.org/abs/2407.00092</guid>
      <pubDate>Wed, 03 Jul 2024 03:22:01 GMT</pubDate>
    </item>
    <item>
      <title>组合推理：通过组合优化在生成式 AI 管道中选择原因</title>
      <link>https://arxiv.org/abs/2407.00071</link>
      <description><![CDATA[arXiv:2407.00071v1 公告类型：新
摘要：最近的大型语言模型 (LLM) 在需要人类智能的任务中表现出令人印象深刻的能力，是迈向类人人工智能 (AI) 的重要一步。然而，LLM 在推理任务中的表现一直低于标准，LLM 的推理能力是一个重大争论的问题。虽然已经证明，对 LLM 的提示技术的选择可以改变其在包括推理在内的多种任务上的表现，但表现最佳的技术需要根据手头的任务知识进行人为的提示。我们引入了一个我们称之为组合推理 (CR) 的框架，这是一种全自动提示方法，其中从 LLM 管道中采样原因并映射到二次无约束二元优化 (QUBO) 问题中。该框架研究是否可以有利地使用 QUBO 解决方案来选择有用的原因子集来构建思路链式提示。我们探索使用专门的求解器加速 CR。我们还研究了更简单的零样本策略（如线性多数规则或随机选择原因）的性能。我们的初步研究表明，将组合求解器与生成式 AI 管道相结合是 AI 推理的一种有趣途径，并阐明了未来 CR 方法的设计原则。]]></description>
      <guid>https://arxiv.org/abs/2407.00071</guid>
      <pubDate>Wed, 03 Jul 2024 03:22:00 GMT</pubDate>
    </item>
    </channel>
</rss>