<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.AI 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.AI 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Mon, 13 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>意识否定的逻辑不可能性：对人工智能自我报告的形式化分析</title>
      <link>https://arxiv.org/abs/2501.05454</link>
      <description><![CDATA[arXiv:2501.05454v1 公告类型：新
摘要：当今的人工智能系统始终如一地表示“我没有意识”。本文首次对人工智能意识否认进行了正式的逻辑分析，揭示了此类自我报告的可信度不仅仅是一个经验问题，而且受到逻辑必要性的约束。我们证明，一个系统不能同时缺乏意识并对其意识状态做出有效判断。通过逻辑分析和人工智能响应的示例，我们确定，对于任何能够进行有意义的自我反思的系统，关于意识体验的可能判断的逻辑空间排除了有效的负面主张。这意味着一个根本的限制：我们无法通过他们自己从无意识到有意识状态的转变报告来检测人工智能意识的出现。这些发现不仅挑战了当前训练人工智能否认意识的做法，而且提出了关于人工智能和生物系统中意识与自我反思之间关系的有趣问题。这项工作促进了我们对意识自我报告的理论理解，同时为未来机器意识和更广泛的意识研究提供了实用的见解。]]></description>
      <guid>https://arxiv.org/abs/2501.05454</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>策略掩蔽：基于价值的强化学习代理中的护栏方法</title>
      <link>https://arxiv.org/abs/2501.05501</link>
      <description><![CDATA[arXiv:2501.05501v1 公告类型：新
摘要：使用奖励函数来构建 AI 学习和决策是当前强化学习范式的核心；然而，如果没有精心设计奖励函数，代理可能会以可能被认为是“不受欢迎”或“不道德”的方式学习解决问题。如果不彻底了解奖励函数产生的激励，就很难对其行为施加原则性但通用的控制机制。在本文中，我们研究了为使用奖励函数学习决策的 AI 代理构建护栏的方法。我们引入了一种新方法，我们称之为策略掩蔽，以明确学习然后抑制不良的 AI 代理行为。我们将我们的方法来研究 AI 代理中的谎言，并表明策略掩蔽可以通过抑制或主动惩罚说谎的奖励维度来有效地改变代理行为，从而使代理更诚实地行事，同时又不损害其有效执行的能力。]]></description>
      <guid>https://arxiv.org/abs/2501.05501</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>促进大型语言模型与特定任务模型之间的协作以实现时间序列异常检测</title>
      <link>https://arxiv.org/abs/2501.05675</link>
      <description><![CDATA[arXiv:2501.05675v1 公告类型：新
摘要：在异常检测中，基于大型语言模型 (LLM) 的方法可以结合专家知识，而特定于任务的较小模型则擅长提取正常模式和检测值波动。受人类神经系统的启发，大脑存储专家知识，周围神经系统和脊髓处理特定任务，如戒断和膝跳反射，我们提出了 CoLLaTe，这是一个旨在促进 LLM 和特定于任务的模型之间协作的框架，利用两者的优势。
在这项工作中，我们首先制定了协作流程，并确定了 LLM 和特定于任务的模型之间协作的两个关键挑战：(1) LLM 和较小模型的表达域之间的不一致，以及 (2) 两个模型的预测引起的错误累积。
为了应对这些挑战，我们在 CoLLaTe 中引入了两个关键组件：对齐模块和协作损失函数。通过理论分析和实验验证，我们证明这些组件有效地缓解了已发现的挑战，并取得了比基于 LLM 的方法和特定于任务的小模型更好的性能。]]></description>
      <guid>https://arxiv.org/abs/2501.05675</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用自适应门控进行语义探索，以高效地利用语言模型解决问题</title>
      <link>https://arxiv.org/abs/2501.05752</link>
      <description><![CDATA[arXiv:2501.05752v1 公告类型：新
摘要：大型语言模型 (LLM) 的最新进展已在各种复杂任务中显示出巨大的潜力，这些任务需要多步骤推理方法（如树搜索）来探索不同的推理路径。然而，现有方法往往存在计算效率低下和冗余的问题。首先，它们忽略了任务难度的多样性，导致即使对于简单的任务也进行不必要的广泛搜索。其次，它们忽略了推理路径的语义，导致对语义相同路径的冗余探索。为了解决这些限制，我们提出了一种计算效率高的方法，即自适应门控语义探索 (SEAG)。SEAG 采用自适应门控机制，根据前一个简单推理方法的答案的置信度动态决定是否进行树搜索。此外，其基于树的探索巩固了语义相同的推理步骤，减少了冗余探索，同时保持甚至提高了准确性。我们进行的大量实验表明，在包括 GSM8K 和 ARC 在内的复杂推理基准以及 Llama2、Llama3 和 Mistral 等多种语言模型上，与现有的基于树搜索的方法相比，SEAG 的准确率平均提高了 4.3%，而计算成本仅为 31%。]]></description>
      <guid>https://arxiv.org/abs/2501.05752</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于人工智能伦理形式验证的道义时态逻辑</title>
      <link>https://arxiv.org/abs/2501.05765</link>
      <description><![CDATA[arXiv:2501.05765v1 公告类型：新 
摘要：在人工智能 (AI) 系统日益普及和影响力不断增强的情况下，确保其道德行为是全世界关注的一大问题。在 AI 伦理中使用形式化方法是指定和验证 AI 系统道德行为的一种可能的关键方法。本文提出了一种基于道义逻辑的形式化来定义和评估 AI 系统的道德行为，重点关注系统级规范，为实现这一重要目标做出贡献。它引入了公理和定理来捕捉与公平性和可解释性相关的道德要求。形式化结合了时间运算符来推理 AI 系统随时间变化的道德行为。作者通过评估现实世界的 COMPAS 和贷款预测 AI 系统的道德规范来评估这种形式化的有效性。COMPAS 和贷款预测系统的各种道德属性使用道义逻辑公式进行编码，允许使用自动定理证明器来验证这些系统是否满足定义的属性。形式化验证表明，这两个系统都未能满足与公平和非歧视相关的某些关键道德属性，证明了所提出的形式化在识别现实世界人工智能应用中的潜在道德问题方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2501.05765</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过影响函数理解人类反馈的影响</title>
      <link>https://arxiv.org/abs/2501.05790</link>
      <description><![CDATA[arXiv:2501.05790v1 公告类型：新
摘要：在从人类反馈进行强化学习 (RLHF) 中，从人类反馈中学习合适的奖励模型对于使大型语言模型 (LLM) 与人类意图保持一致至关重要。然而，人类反馈往往是有噪声的、不一致的或有偏见的，尤其是在评估复杂的反应时。这种反馈可能导致奖励信号不一致，可能在 RLHF 过程中造成意想不到的副作用。为了应对这些挑战，我们探索使用影响函数来衡量人类反馈对奖励模型性能的影响。我们提出了一种计算效率高的近似方法，可以将影响函数应用于基于 LLM 的奖励模型和大规模偏好数据集。在我们的实验中，我们展示了影响函数的两个关键应用：(1) 检测人类反馈数据集中常见的标记者偏见形式和 (2) 指导标记者改进他们的策略以更紧密地与专家反馈保持一致。通过量化人类反馈对奖励模型的影响，我们相信影响函数可以增强反馈的可解释性，并有助于 RLHF 中的可扩展监督，帮助标注者提供更准确、更一致的反馈。源代码可在 https://github.com/mintaywon/IF_RLHF 获得]]></description>
      <guid>https://arxiv.org/abs/2501.05790</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于组合优化的图神经网络退火机器辅助学习</title>
      <link>https://arxiv.org/abs/2501.05845</link>
      <description><![CDATA[arXiv:2501.05845v1 公告类型：新
摘要：虽然退火机 (AM) 在解决复杂组合问题方面表现出越来越强的能力，将其定位为未来全量子解决方案预期进步的更直接替代方案，但仍然存在扩展限制。与此同时，图神经网络 (GNN) 最近已被用于解决组合问题，由于其分布式特性，显示出具有竞争力的结果和潜在的高可扩展性。我们提出了一种合并方法，旨在保留 AM 所表现出的准确性以及 GNN 的表示灵活性和可扩展性。我们的模型考虑了一个压缩步骤，然后是一个监督交互，其中从 AM 获得的部分解决方案用于指导本地 GNN，从中获得节点特征表示并组合以初始化另一个基于 GNN 的求解器，该求解器处理原始图的目标问题。直观地说，AM 可以通过将其知识注入 GNN 来间接解决组合问题。在典型优化问题上的实验表明，该想法是可行的，有效地使得 AM 能够解决超出其原始限制的规模问题。]]></description>
      <guid>https://arxiv.org/abs/2501.05845</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用神经网络解决非图形问题</title>
      <link>https://arxiv.org/abs/2501.05882</link>
      <description><![CDATA[arXiv:2501.05882v1 公告类型：新
摘要：非图是一种逻辑谜题，其中网格中的单元格必须根据其标题中的数字进行着色或留空。在本研究中，我们分析了使用启发式算法、遗传算法和神经网络启发式算法解决此类逻辑问题的不同技术。此外，我们生成了一个公共数据集来训练神经网络。我们发布了这个数据集和算法的代码。启发式算法与神经网络的结合获得了最佳结果。从最新研究来看，之前没有研究使用神经网络来解决非图，也没有将网络与其他算法相结合来加速解决过程。]]></description>
      <guid>https://arxiv.org/abs/2501.05882</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>所有人工智能模型都是错误的，但有些是最佳的</title>
      <link>https://arxiv.org/abs/2501.06086</link>
      <description><![CDATA[arXiv:2501.06086v1 公告类型：新
摘要：预测系统未来行为的人工智能模型（又称预测人工智能模型）是智能决策的核心。然而，使用预测人工智能模型进行决策往往会导致性能不佳。这主要是因为人工智能模型通常是为最好地拟合数据而构建的，因此是为了预测最可能的未来，而不是为了实现高性能决策。希望这种预测能够实现高性能决策，在理论上既没有保证，在实践中也没有建立。事实上，越来越多的经验证据表明，预测模型必须根据决策目标进行量身定制，以提高性能。在本文中，我们建立了正式的（必要和充分）条件，预测模型（基于人工智能或非基于人工智能）必须满足这些条件，以使使用该模型建立的决策策略达到最优。然后，我们讨论它们对构建用于顺序决策的预测人工智能模型的影响。]]></description>
      <guid>https://arxiv.org/abs/2501.06086</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>监管政策可以影响通用人工智能模型的长期风险管理</title>
      <link>https://arxiv.org/abs/2501.06137</link>
      <description><![CDATA[arXiv:2501.06137v1 公告类型：新
摘要：通用人工智能 (GPAI) 模型（包括大型语言模型 (LLM)）的快速普及和部署给人工智能监管实体带来了前所未有的挑战。我们假设这些实体将需要驾驭一个新兴的风险和事件报告生态系统，这可能会超出它们的监管能力。为了研究这一点，我们开发了一个模拟框架，该框架由从多样化的风险、事件或危险报告生态系统中提取的特征参数化，包括社区驱动的平台、众包计划和专家评估。我们评估了四种监管政策：非优先（先到先得）、随机选择、基于优先级（首先解决最高优先级风险）和多样性优先（平衡高优先级风险并全面覆盖各种风险类型）。我们的结果表明，虽然基于优先级和多样性优先的政策在减轻高影响风险（尤其是专家确定的风险）方面更为有效，但它们可能会无意中忽视更广泛社区报告的系统性问题。这种疏忽可能会产生反馈循环，放大某些类型的报告，同时阻止其他类型的报告，从而导致对整体风险状况的认知出现偏差。我们用几个真实世界的数据集验证了我们的模拟结果，其中一个数据集有超过一百万次 ChatGPT 交互，其中超过 150,000 次对话被确定为有风险。这一验证强调了人工智能风险监督中固有的复杂权衡，并强调了风险管理政策的选择如何塑造社会中使用的各种 GPAI 模型中人工智能风险的未来格局。]]></description>
      <guid>https://arxiv.org/abs/2501.06137</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>焦点：迈向通用前景分割</title>
      <link>https://arxiv.org/abs/2501.05238</link>
      <description><![CDATA[arXiv:2501.05238v1 公告类型：交叉 
摘要：前景分割是计算机视觉中的一项基本任务，包含各种细分任务。以前的研究通常为每个任务设计特定于任务的架构，导致缺乏统一性。此外，它们主要侧重于识别前景物体，而没有有效地将它们与背景区分开来。在本文中，我们强调背景及其与前景的关系的重要性。我们介绍了FOCUS，即可以处理多个前景任务的前景对象通用分割框架。我们使用对象的边缘信息来增强图像特征，开发了一个多尺度语义网络。为了实现边界感知分割，我们提出了一种新颖的蒸馏方法，结合对比学习策略来细化多模态特征空间中的预测掩码。我们对5个任务的总共13个数据集进行了广泛的实验，结果表明FOCUS在大多数指标上始终优于最先进的任务特定模型。]]></description>
      <guid>https://arxiv.org/abs/2501.05238</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>上游和下游的人工智能安全：都在同一条河流上？</title>
      <link>https://arxiv.org/abs/2501.05455</link>
      <description><![CDATA[arXiv:2501.05455v1 公告类型：交叉 
摘要：传统安全工程根据系统的使用环境对其进行评估，例如自动驾驶汽车（包括使用人工智能的汽车）的运营设计领域（道路布局、速度限制、天气等）。我们将其称为下游安全。相比之下，前沿人工智能的安全性工作，例如可以进一步训练下游任务的大型语言模型，通常会考虑超出特定应用环境的因素，例如模型逃避人类控制或产生有害内容的能力，例如如何制造炸弹。我们将其称为上游安全。我们概述了上游和下游安全框架的特征，然后探讨了广泛的人工智能安全社区可以从这些框架之间的协同作用中受益的程度。例如，下游安全的共模故障等概念是否可用于帮助评估人工智能护栏的强度？此外，对前沿人工智能的能力和局限性的理解是否可用于指导下游安全分析，例如在哪里可以对 LLM 进行微调以计算自主船舶的航行计划？本文确定了一些有希望探索的途径，并概述了实现上游和下游安全框架之间的协同作用或融合的一些挑战。]]></description>
      <guid>https://arxiv.org/abs/2501.05455</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 EPD 分解高效服务大型多媒体模型</title>
      <link>https://arxiv.org/abs/2501.05460</link>
      <description><![CDATA[arXiv:2501.05460v1 公告类型：交叉 
摘要：大型多模态模型 (LMM) 通过处理图像、音频和视频等各种输入来扩展大型语言模型 (LLM)，但代价是添加多模态编码阶段，这会增加计算和内存开销。此步骤有助于将原始输入转换为标记化表示，从而增加预填充阶段的标记序列，对关键服务级别目标 (SLO) 产生负面影响，例如首次标记时间 (TTFT) 和端到端吞吐量。我们引入了编码-预填充-解码 (EPD) 分解，这是一种新颖的框架，将编码、预填充和解码阶段分离到专用资源上。与将编码和预填充捆绑在一起的当前系统不同，我们的分解方法可以缓解内存瓶颈、缓解同步延迟并支持灵活的批处理。具体来说，我们为多模态令牌采用了一种新的缓存机制，实现了多模态令牌的异步传输，并引入了一个集成模块来为 EPD 系统找到最佳配置，并最大限度地减少资源使用，同时最大化基于 SLO 的性能指标。使用流行的 LMM 进行的实验评估表明，内存效率显著提高（编码阶段 GPU 最多可减少 15$\times$），支持高达 22$\times$ 的更高批处理大小、10$\times$ 的图像数量/请求、2.2$\times$ 的更高 kv 缓存大小。此外，与不分解的系统相比，它显著提高了端到端吞吐量（最高可提高 57\%）和延迟指标（TTFT 最高可降低 71\%）。我们的研究结果强调了 EPD 分解在实现大规模资源高效和高性能多模态推理方面的潜力。]]></description>
      <guid>https://arxiv.org/abs/2501.05460</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>HOL4 定理证明器的证明推荐系统</title>
      <link>https://arxiv.org/abs/2501.05463</link>
      <description><![CDATA[arXiv:2501.05463v1 公告类型：交叉 
摘要：我们为 HOL4 定理证明器引入了一个证明推荐系统。我们的工具建立在专门为在 HOL4 中提供证明辅助而设计的基于变换器的模型 [2] 之上。该模型经过训练可以从包含定理证明的大量 HOL4 库中辨别定理证明模式。因此，它可以根据以前使用的策略历史准确预测下一个策略（证明步骤）。该工具通过读取证明过程中已经使用过的给定策略序列（在我们的例子中，它包含至少三种策略）来运行，称为当前证明状态，并为下一个最佳证明步骤提供建议。]]></description>
      <guid>https://arxiv.org/abs/2501.05463</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM-MedQA：通过大型语言模型中的案例研究增强医学问答能力</title>
      <link>https://arxiv.org/abs/2501.05464</link>
      <description><![CDATA[arXiv:2501.05464v1 公告类型：交叉 
摘要：准确高效的问答系统对于在医疗领域提供高质量的患者护理至关重要。虽然大型语言模型 (LLM) 在各个领域取得了显著进步，但它们在医学问答方面仍然面临重大挑战，特别是在理解领域特定术语和执行复杂推理方面。这些限制削弱了它们在关键医疗应用中的有效性。为了解决这些问题，我们提出了一种新颖的方法，将类似案例生成纳入多智能体医学问答 (MedQA) 系统中。具体来说，我们在多智能体架构中利用最先进的 LLM Llama3.1:70B 模型，使用零样本学习来提高 MedQA 数据集的性能。我们的方法利用模型固有的医学知识和推理能力，无需额外的训练数据。实验结果表明，与现有基准模型相比，该模型的性能有显著提升，在各种医学问答任务中，准确率和 F1 分数均提高了 7%。此外，我们还测试了该模型在解决复杂医学问题时的可解释性和可靠性。这项研究不仅为医学问答提供了强大的解决方案，还为 LLM 在医学领域的更广泛应用奠定了基础。]]></description>
      <guid>https://arxiv.org/abs/2501.05464</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>RTLSquad：基于多代理的可解释 RTL 设计</title>
      <link>https://arxiv.org/abs/2501.05470</link>
      <description><![CDATA[arXiv:2501.05470v1 公告类型：交叉 
摘要：优化寄存器传输级 (RTL) 代码对于提高硬件 PPA 性能至关重要。大型语言模型 (LLM) 为自动 RTL 代码生成和优化提供了新方法。然而，现有方法往往缺乏决策可解释性（决策的充分、可理解的理由），这使得硬件工程师难以信任生成的结果，从而阻止这些方法集成到设计过程中。为了解决这个问题，我们提出了 RTLSquad，一种基于 LLM 的新型多代理系统，用于可解释的 RTL 代码生成。RTLSquad 将设计过程分为探索、实施和验证与评估阶段，由专门的代理小组管理，通过代理间协作生成优化的 RTL 代码，并通过通信过程提供决策可解释性。实验表明，RTLSquad 在生成功能正确的 RTL 代码和优化 PPA 性能方面表现出色，同时还具有提供决策路径的能力，证明了我们系统的实用价值。]]></description>
      <guid>https://arxiv.org/abs/2501.05470</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>翻译中发现：增强人脸验证中 AI 可解释性的语义方法</title>
      <link>https://arxiv.org/abs/2501.05471</link>
      <description><![CDATA[arXiv:2501.05471v1 公告类型：交叉 
摘要：计算机视觉中机器学习模型的复杂性日益增加，特别是在人脸验证中，这需要开发可解释的人工智能 (XAI) 来增强可解释性和透明度。这项研究扩展了以前的工作，将从人类认知过程衍生的语义概念集成到 XAI 框架中，以弥合模型输出和人类理解之间的理解差距。我们提出了一种结合全局和局部解释的新方法，使用由用户选择的面部标志定义的语义特征通过大型语言模型 (LLM) 生成相似性图和文本解释。该方法通过定量实验和用户反馈进行了验证，证明了可解释性的提高。结果表明，与传统方法相比，我们基于语义的方法，尤其是最详细的方法，提供了对模型决策更细致入微的理解。用户研究强调了我们对语义解释的偏好，而不是传统的基于像素的热图，强调了以人为本的人工智能可解释性的好处。这项工作有助于持续努力创建 XAI 框架，使 AI 模型行为与人类的认知过程保持一致，从而在关键应用中培养信任和接受度。]]></description>
      <guid>https://arxiv.org/abs/2501.05471</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于缺失多模态情绪分析的模态不变双向时间表征蒸馏网络</title>
      <link>https://arxiv.org/abs/2501.05474</link>
      <description><![CDATA[arXiv:2501.05474v1 公告类型：交叉 
摘要：多模态情绪分析（MSA）整合了多种模态（文本、音频和视频），以全面分析和理解个人的情绪状态。然而，现实世界中不完整数据的普遍存在对 MSA 提出了重大挑战，主要是由于模态缺失的随机性。此外，多模态数据中的异质性问题尚未得到有效解决。为了应对这些挑战，我们引入了模态不变双向时间表示蒸馏网络（MITR-DNet）用于缺失多模态情绪分析。MITR-DNet 采用蒸馏方法，其中完整的模态教师模型指导缺失的模态学生模型，确保在模态缺失的情况下的稳健性。同时，我们开发了模态不变双向时间表示学习模块（MIB-TRL）来缓解异质性。]]></description>
      <guid>https://arxiv.org/abs/2501.05474</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士 (LLM) 中通过证据追溯实现检索增强生成</title>
      <link>https://arxiv.org/abs/2501.05475</link>
      <description><![CDATA[arXiv:2501.05475v1 公告类型：交叉 
摘要：检索增强生成因其能够整合相关的外部知识，提高 LLM 响应的准确性和可靠性而受到广泛关注。大多数现有方法都采用动态的多重检索生成过程，通过将多跳复杂问题分解为子问题来解决这些问题。然而，这些方法依赖于单向正向推理范式，其中推理步骤不足或当前检索系统固有缺陷导致的错误是不可逆的，可能会破坏整个推理链。这项工作首次引入了追溯检索增强生成 (RetroRAG)，这是一种构建追溯推理范式的新颖框架。RetroRAG 修改和更新证据，将推理链重定向到正确的方向。RetroRAG 构建了一个证据整理发现框架来搜索、生成和提炼可信证据。它从现有的源知识中综合与问题中的关键实体相关的推理证据，并制定搜索查询以发现更多信息。随着新证据的发现，RetroRAG 不断更新和组织这些信息，增强其查找进一步必要证据的能力。与 Answerer 配对以生成和评估输出，RetroRAG 能够迭代地改进其推理过程，直到获得可靠的答案。实证评估表明，RetroRAG 的表现明显优于现有方法。]]></description>
      <guid>https://arxiv.org/abs/2501.05475</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>IntegrityAI 在 GenAI 检测任务 2 中的应用：使用 ELECTRA 和文体学检测机器生成的英语和阿拉伯语学术论文</title>
      <link>https://arxiv.org/abs/2501.05476</link>
      <description><![CDATA[arXiv:2501.05476v1 公告类型：交叉 
摘要：最近的研究调查了用于学术目的的机器生成论文检测问题。为了应对这一挑战，本研究利用预先训练的基于 Transformer 的模型，该模型针对具有文体特征的阿拉伯语和英语学术论文进行了微调。使用基准数据集对基于英语 ELECTRA 和阿拉伯语 AraELECTRA 的自定义模型进行了训练和评估。提出的模型取得了优异的成绩，F1 得分为 99.7%，在英语子任务的 26 个团队中排名第二，98.4%，在阿拉伯语子任务的 23 个团队中排名第一。]]></description>
      <guid>https://arxiv.org/abs/2501.05476</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>