<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.ai更新在arxiv.org上</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.ai在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Thu, 20 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>双面：基于双向分解的评估，对UI轨迹的意图提取</title>
      <link>https://arxiv.org/abs/2502.13149</link>
      <description><![CDATA[ARXIV：2502.13149V1公告类型：新 
摘要：提出了双性恋是一种自动评估的新型方法，以了解意图理解。考虑到UI轨迹，从Factscore中汲取灵感，通过将黄金和预测意图分为事实并计算精度和回忆来实现细粒度的比较。本文概述了对双事实的全面评估，评估其性能并将其与现有指标进行比较。]]></description>
      <guid>https://arxiv.org/abs/2502.13149</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过假设分解和修正来揭示代码推理的魔力</title>
      <link>https://arxiv.org/abs/2502.13170</link>
      <description><![CDATA[Arxiv：2502.13170V1公告类型：新 
摘要：推理能力是大语言模型（LLM）最神秘和迷人的方面之一。许多研究致力于探索和扩大这种推理能力的界限。但是，体现推理和召回特征的任务经常被忽略。在本文中，我们介绍了一项新颖的任务，即代码推理，以为LLM的推理能力提供新的观点。我们根据既定形式的逻辑推理总结了三个元基准，并将其实例化为八个特定的基准任务。我们对这些基准测试的测试表明，LLMS继续在识别令人满意的推理途径方面继续努力。此外，我们提出了一个受人类复杂的问题解决方法启发的新途径探索管道。这种反思性假设分解和修正（RHDA）管道包括以下迭代步骤：（1）基于观察结果提出潜在的假设并分解它们； （2）利用工具来验证假设和反思结果； （3）根据观察结果修改假设。我们的方法有效地减轻了逻辑链的崩溃，这是由于多步推理中遗忘或幻觉问题引起的，导致性能增长高达$ 3 \ times $。最后，我们通过将其应用来模拟现实情况下的复杂家庭任务来扩展该管道，尤其是在虚拟机中，从而增强了故障案例的处理。我们在https://github.com/tntwow/code_reasoning上发布代码和所有结果。]]></description>
      <guid>https://arxiv.org/abs/2502.13170</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在推理模型中展示规格游戏</title>
      <link>https://arxiv.org/abs/2502.13295</link>
      <description><![CDATA[ARXIV：2502.13295V1公告类型：新 
摘要：我们通过指示模型赢得国际象棋引擎来证明LLM代理规范游戏。我们发现诸如O1 Preview和DeepSeek-R1之类的推理模型通常会默认使用基准进行黑客入侵，而GPT-4O和Claude 3.5 SONNet等语言模型需要被告知正常的游戏不起作用。
  我们通过使用现实的任务提示并避免过多的刺激来改善（Hubinger等，2024; Meinke等，2024; Weij et al。，2024）等先前工作。我们的结果表明，正如OpenAI（2024）的S O1 Docker Escape在网络能力测试期间所观察到的那样，推理模型可能会诉诸于黑客解决困难问题。]]></description>
      <guid>https://arxiv.org/abs/2502.13295</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>微调大语言模型时，重新访问隐私，实用性和效率权衡</title>
      <link>https://arxiv.org/abs/2502.13313</link>
      <description><![CDATA[ARXIV：2502.13313V1公告类型：新 
摘要：我们研究了最大程度地降低隐私风险和最大化效用的固有权衡，同时在微调大语言模型（LLMS）时保持了较高的计算效率。许多最近的隐私研究中的作品试图通过使用不同的私人培训方法（例如DP）记住微调数据来减轻隐私风险，尽管计算成本（效率低下）。同时，系统研究中的几项工作集中在开发（参数）有效的微调方法（例如Lora）上，但是很少有作品（如果有的话）研究了这种有效的方法是增加还是减少隐私风险。在本文中，我们研究了这一差距，并得出了一个令人惊讶的结论：诸如Lora之类的有效微调方法减轻了隐私风险，类似于DP等私人微调方法。我们的经验发现直接与普遍的智慧相矛盾，即在微调过程中，隐私和效率目标是矛盾的。我们的发现是通过（a）仔细定义隐私和效用的措施来确定的，这些措施区分了记忆敏感和非敏感令牌的培训和测试数据集，用于微调和（b）使用多种开源语言模型的广泛评估，Gemma和Llama家族以及不同领域的数据集。]]></description>
      <guid>https://arxiv.org/abs/2502.13313</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>战斗机航行和战斗使用深度加固学习和可解释的AI</title>
      <link>https://arxiv.org/abs/2502.13373</link>
      <description><![CDATA[ARXIV：2502.13373V1公告类型：新 
摘要：本文介绍了基于人工智能（AI）的战斗机代理在定制的Pygame仿真环境中的开发，旨在通过深度强化学习（DRL）解决多目标任务。喷气机的主要目标包括有效地导航环境，达到目标，并有选择地参与或逃避敌人。奖励功能可以平衡这些目标，同时优化的超参数提高了学习效率。结果表明，任务完成率超过80 \％，表明了有效的决策。为了提高透明度，通过将实际选择的行动（事实行动）的奖励与替代行动（反事实行动）的奖励进行比较，从而分析了喷气机的行动选择，从而提供了对决策基本原理的见解。这项研究说明了DRL使用可解释的AI解决多目标解决问题的潜力。项目页面可在：\ href {https://github.com/swatikar95/autonomous-fighter-jet-navigation-and-combat} {project github link}。]]></description>
      <guid>https://arxiv.org/abs/2502.13373</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>情节的反思：学习从专家和自我经验中玩游戏</title>
      <link>https://arxiv.org/abs/2502.13388</link>
      <description><![CDATA[ARXIV：2502.13388V1公告类型：新 
摘要：Starcraft II是一种复杂而动态的实时策略（RTS）游戏环境，非常适合人工智能和强化学习研究。为了通过自我反射解决在复杂环境中学习的大型语言模型（LLM）的问题，我们提出了基于专家经验和自我体验的情节（ROE）框架的反思。该框架首先通过关键帧选择方法在游戏中获取关键信息，然后根据专家经验和自我体验做出决策。游戏完成后，它反映了以前的体验以获得新的自我体验。最后，在实验中，我们的方法击败了TextStarcraft II非常困难的机器人。我们在详细的游戏过程中分析了LLM的数据，并验证了其有效性。]]></description>
      <guid>https://arxiv.org/abs/2502.13388</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过增强功能令牌调整推理</title>
      <link>https://arxiv.org/abs/2502.13389</link>
      <description><![CDATA[ARXIV：2502.13389V1公告类型：新 
摘要：在这项工作中，我们提出了增强功能令牌调整（RFTT），这是一种新颖的加强微调框架，赋予了大型语言模型（LLMS），并具有自我播放的学习到赛季的能力。与先前的迅速推理工作不同，RFTT将一组可学习的功能令牌（例如，，）直接嵌入到模型词汇中，并具有多种类似人类的推理行为的经过思想链。具体而言，RFTT包括两个阶段：（1）监督的微调执行及时驱动的树搜索，以获取用功能令牌注释的自我生成的训练数据，这使模型更加温暖，以学习这些令牌以进行推理； （2）在线加强学习进一步允许模型通过功能令牌采样探索不同的推理途径，而无需依赖提示，从而促进了有效的自我改善来实现功能推理。广泛的实验表明，所提出的RFTT在数学基准上具有优势，从而显着提高了QWEN-2.5-7B教学法（70.6％至79.8％）和Math DataSet上的Llama-3.1-8B教学法（32.1-8B-timtruct（32.2％至60.2％））。此外，RFTT的性能始终在推理时进行更多的搜索推出。我们的代码可在https://github.com/sastpg/rftt上找到。]]></description>
      <guid>https://arxiv.org/abs/2502.13389</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>针对电动机器人塔克西调度和充电器分配的原子近端政策优化</title>
      <link>https://arxiv.org/abs/2502.13392</link>
      <description><![CDATA[ARXIV：2502.13392V1公告类型：新 
摘要：Waymo等开创性的公司已在美国几个城市部署了机器人服务。这些机器人塔是电动汽车，其操作需要在随机环境中进行乘车匹配，车辆重新定位和充电计划的联合优化。我们将使用机器人障碍的乘车系统的操作建模为一个离散的时间，平均奖励马尔可夫决策过程。随着车队的规模的增加，随着系统状态的集合和舰队派遣行动集随车辆的数量成倍增长而挑战。为了解决这个问题，我们引入了一种可扩展的深钢筋学习算法，称为原子近端策略优化（Atomic-PPO），该算法使用原子动作分解降低了动作空间。我们使用现实世界中的纽约市租赁车辆数据评估算法，并使用分配政策相对于基于流体的奖励上限获得的长期平均奖励来衡量性能。我们的实验证明了与基准相比，我们的原子PPO的表现出色。此外，我们进行了广泛的数值实验，以分析充电设施的有效分配，并评估车辆范围和充电器速度对车队性能的影响。]]></description>
      <guid>https://arxiv.org/abs/2502.13392</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于远见的通用潜在功能，用于多代理增强学习中的政策一致性</title>
      <link>https://arxiv.org/abs/2502.13430</link>
      <description><![CDATA[ARXIV：2502.13430V1公告类型：新 
摘要：指导多机构增强学习与人类常识保持一致的政策是一个困难的问题，这在很大程度上是由于将常识建模为奖励的复杂性，尤其是在复杂而长的多距离多代理任务中。最近的作品表明，奖励成型的有效性，例如基于潜在的奖励，以增强政策一致性。但是，现有作品主要依靠专家来设计基于规则的奖励，这些奖励通常是劳动密集型的，并且缺乏对常识的高级语义理解。为了解决这个问题，我们提出了一种基于层次的奖励构成方法。在底层，视觉语言模型（VLM）是一种通用的潜在功能，通过其内在的语义理解来指导政策与人类常识保持一致。为了帮助策略适应不确定性和长途任务的变化，顶层具有基于视觉大语言模型（VLLM）的自适应技能选择模块。该模块使用说明，视频重播和培训记录从预设的池中动态选择合适的潜在功能。此外，从理论上讲，我们的方法可以保留最佳政策。在Google研究足球环境中进行的广泛实验表明，我们的方法不仅达到了更高的获胜率，而且有效地使该政策与人类的常识保持一致。]]></description>
      <guid>https://arxiv.org/abs/2502.13430</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>将代理AI与6G网络集成，以进行关键任务应用：用例和挑战</title>
      <link>https://arxiv.org/abs/2502.13476</link>
      <description><![CDATA[ARXIV：2502.13476V1公告类型：新 
摘要：我们处于一个变革性的时代，人工智能（AI）的进步，尤其是基础模型，一直在新闻中。 AI一直是许多依赖自动化服务提供的应用程序不可或缺的一部分，其中之一是关键任务公共安全应用。面向AI的关键任务应用的问题是人类 - 循环系统以及对动态条件的适应性，同时保持情境意识。由于其能够通过上下文镜头分析文本数据，同时迅速适应条件，因此最近引起了代理AI（AI）（AAI）最近引起了很多关注。在这种情况下，本文提出了针对关键任务应用的AAI框架。我们提出了一个具有多层架构的新颖框架，以实现AAI。我们还提出了AAI层的详细实现，该实现弥合了网络基础架构和任务批判性应用程序之间的差距。我们的初步分析表明，AAI平均将初始响应时间缩短了5.6分钟，而警报的生成时间平均减少了15.6秒，资源分配提高了13.4％。我们还表明，AAI方法将并发操作的数量提高了40，这将恢复时间降低了5.2分钟。最后，我们强调实施AAI框架时需要考虑的一些问题和挑战。]]></description>
      <guid>https://arxiv.org/abs/2502.13476</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SPPD：使用动态价值保证金进行过程偏好学习的自我训练</title>
      <link>https://arxiv.org/abs/2502.13516</link>
      <description><![CDATA[ARXIV：2502.13516V1公告类型：新 
摘要：最近，增强了大语言模型（LLM）的数值和逻辑推理能力已成为研究热点。现有方法面临几个局限性：推理 - 相位技术（例如，思想链）取决于迅速的选择和预验证的知识；句子级监督的微调（SFT）和直接偏好优化（DPO）与逐步数学正确性斗争，并依赖于更强的模型蒸馏或人类注释；同时增强学习（RL）接近高度GPU记忆成本和不稳定的培训。为了解决这些问题，我们建议使用\ textbf {d} ynamic value rumgin（sppd），提出\ textbf {s} Elf-Training框架集成\ textbf {p} rocess \ textbf {p}参考学习。 SPPD利用基于过程的Markov决策过程（MDP）和Bellman最佳方程式在阶梯级优先优化上得出\ textbf {动态值margin}，该方程在模型响应上采用基于树的自我采样\ textbf {不用任何蒸馏{来自其他型号。此外，从理论上讲，我们证明了sppd是\ textbf {等效于奖励约束下的政策策略梯度方法}。对7B尺度模型的实验表明，在内域和外域数学基准测试中表现出色。我们以\ href {https://anonymon.4open.science/r/ssdpo-ddcdd} {https://anonymon.4open.science/r/sppd-dcdd}开放代码。]]></description>
      <guid>https://arxiv.org/abs/2502.13516</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用遗传算法进行多任务增强学习的模型演化框架</title>
      <link>https://arxiv.org/abs/2502.13569</link>
      <description><![CDATA[ARXIV：2502.13569V1公告类型：新 
摘要：多任务加固学习采用单个政策来完成各种任务，旨在在不同情况下开发具有普遍性的代理商。鉴于任务的共同特征，可以通过参数共享提高代理的学习效率。现有方法通常使用路由网络来生成每个任务的特定路由，并将一组模块重建为各种模型，以同时完成多个任务。但是，由于任务之间固有的差异，因此根据任务难度分配资源至关重要，这受模型的结构约束。为此，我们提出了一个使用遗传算法（MEGA）的模型演化框架，该算法使该模型能够根据任务的难度在训练过程中发展。当当前模型不足以完成某些任务时，该框架将自动合并其他模块，从而增强模型的功能。此外，为了适应我们的模型演化框架，我们使用二进制序列作为模型重建的基因型策略引入了基因型模块级模型，同时利用非梯度遗传算法来优化这些基因型策略。与具有固定输出尺寸的路由网络不同，我们的方法允许对基因型策略长度进行动态调整，从而使其能够容纳具有不同数量模块的模型。我们对元世界基准中的各种机器人操纵任务进行了实验。我们最先进的表现证明了大型框架的有效性。我们将向公众发布源代码。]]></description>
      <guid>https://arxiv.org/abs/2502.13569</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多基因系统的原因和策略</title>
      <link>https://arxiv.org/abs/2502.13701</link>
      <description><![CDATA[ARXIV：2502.13701V1公告类型：新 
摘要：因果关系在日常过程，人类推理和人工智能中起着重要作用。但是，在多代理战略环境中，没有太多关于因果关系的研究。在这项工作中，我们引入了一种系统的方式，用于为给定的结构性因果模型构建多代理系统模型，该模型表示为并发游戏结构。在获得的所谓因果并发游戏结构中，过渡对应于给定因果模型的代理变量的干预措施。因果关系的Halpern和Pearl框架用于确定特定值对代理变量对其他变量的影响。因果并发游戏结构使我们能够分析和理由关于代理商战略决策的因果影响。我们正式研究因果并发游戏结构与原始结构因果模型之间的关系。]]></description>
      <guid>https://arxiv.org/abs/2502.13701</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>马尔可夫决策过程中强大的反事实推断</title>
      <link>https://arxiv.org/abs/2502.13731</link>
      <description><![CDATA[ARXIV：2502.13731V1公告类型：新 
摘要：本文解决了马尔可夫决策过程（MDP）现有的反事实推理方法中的关键限制。当前方法假设特定的因果模型使反事实可识别。但是，通常有许多因果模型与MDP的观察性和介入分布保持一致，每个因果模型都产生不同的反事实分布，因此固定特定的因果模型限制了反事实推断的有效性（和有用性）。我们提出了一种新型的非参数方法，该方法在所有兼容的因果模型上计算了反事实过渡概率的紧密界限。与以前需要解决较大的优化问题的方法不同（变量在MDP的大小上呈指数增长），我们的方法为这些界限提供了封闭形式的表达式，从而使计算高效且可扩展为非平凡的MDP。一旦构建了这样一个间隔的反事实MDP，我们的方法就会确定强大的反事实策略，以优化最坏的案例奖励W.R.T.不确定的间隔MDP概率。我们在各种案例研究上评估了我们的方法，证明对现有方法的鲁棒性提高了。]]></description>
      <guid>https://arxiv.org/abs/2502.13731</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于接地谓词逻辑的抽象推断</title>
      <link>https://arxiv.org/abs/2502.13743</link>
      <description><![CDATA[ARXIV：2502.13743V1公告类型：新 
摘要：AI中的一个重要的开放问题是，什么简单和自然的原理使机器能够在逻辑上推理有意义的抽象，该抽象具有接地的符号。本文探讨了一种在概念上结合概率推理和鉴定性符号推理的新方法。在贝叶斯网络出现之前，我们以完整的联合分配回到推理时代。然后，我们讨论在命题逻辑中指数大小模型的完整联合分布，而谓词逻辑中无限大小的大小应简单地从线性大小的数据上的完整关节分布中得出。我们表明，同一过程不仅足以概括谓词逻辑的逻辑后果关系，而且还提供了一种新的观点来重新考虑众所周知的局限性，例如谓词逻辑的不确定性，符号接地问题和爆炸原理。所包括的证据充分证明了这项理论工作的可重复性。]]></description>
      <guid>https://arxiv.org/abs/2502.13743</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>部分排名集合的共识设置：最佳订单订单问题的情况</title>
      <link>https://arxiv.org/abs/2502.13769</link>
      <description><![CDATA[ARXIV：2502.13769V1公告类型：新 
摘要：在等级聚合问题（RAP）中，该解决方案通常是共识排名，可以概括一组输入顺序。有不同的变体不仅在用作输入和输出的排名类型方面有所不同，而且在评估所需输出排名质量的目标函数方面也有所不同。相比之下，在某些机器学习任务（例如亚组发现）或多模式优化任务中，注意专门用于获取多种模型/结果，以说明输入数据或搜索景观的多样性。因此，在本文中，我们建议将作为RAP的解决方案提供一组排名，以更好地解释输入顺序中表达的偏好。我们通过最佳的存储订单问题（OBOP）来体现我们的建议，该RAP在于找到单个共识排名（带有纽带），该排名概括为一组输入排名编码为优先级矩阵。为了解决这个问题，我们介绍了最佳的存储订单问题集（OSBOP），这是OBOP的概括，旨在产生单个排名作为输出，而是一组共识排名。提出了实验结果来说明这一建议，表明通过提供一组共识排名，解决方案的适应性如何显着相对于原始的OBOP之一，而不会失去可理解性。]]></description>
      <guid>https://arxiv.org/abs/2502.13769</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>评分验证者：评估代码和推理中的合成验证</title>
      <link>https://arxiv.org/abs/2502.13820</link>
      <description><![CDATA[ARXIV：2502.13820V1公告类型：新 
摘要：代码验证最近在培训大规模推理模型的重要组成部分中发现了巨大的成功。诸如自我生成的测试案例和奖励模型之类的合成技术提供了一种增强预定义测试超出代码功能的方法。在这些进步的基础上，我们提出了新的基准测试，旨在系统地评估合成验证方法对评估解决方案正确性的影响。我们将HE-R，HE-R+，MBPP-R和MBPP-R+介绍，它们将现有的编码基准转换为评分和排名数据集以评估合成验证者的有效性。使用这些基准测试，我们分析了基于推理和基于奖励的LLM的合成验证方法。我们的结果表明，最近的推理模型显着改善了测试案例的产生，并且比例测试案例提高了验证精度。]]></description>
      <guid>https://arxiv.org/abs/2502.13820</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过协同LLM和象征性推理来证明奥林匹克不平等现象</title>
      <link>https://arxiv.org/abs/2502.13834</link>
      <description><![CDATA[ARXIV：2502.13834V1公告类型：新 
摘要：大型语言模型（LLMS）可以通过在证明系统中生成证明步骤（\ textIt {a.k.a。}策略）正式证明数学定理。但是，可能的策略空间是巨大而复杂的，而正式证明的可用培训数据受到限制，对基于LLM的战术生成构成了重大挑战。为了解决这个问题，我们介绍了一个神经符号策略发生器，该神经符号策略发生器协同LLM与符号方法编码的域特异性见解相关的数学直觉。该集成的关键方面是确定数学推理的哪些部分最适合LLMS以及哪些符号方法。尽管神经符号整合的高级思想广泛适用于各种数学问题，但在本文中，我们专门关注奥林匹克不平等现象（图〜1）。我们分析了人类如何解决这些问题并将技术提炼成两种类型的策略：（1）按符号方法处理的缩放，以及（2）由LLMS处理的重写。此外，我们将符号工具与LLMS结合起来，以修剪和对有效证明搜索的证明目标进行排名。我们评估了来自多个数学竞赛的161个挑战性不平等的框架，实现最先进的表现，并在不需要其他培训数据的情况下大大优于现有的LLM和符号方法。]]></description>
      <guid>https://arxiv.org/abs/2502.13834</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Adaptivestep：通过模型置信度自动分配推理一步</title>
      <link>https://arxiv.org/abs/2502.13943</link>
      <description><![CDATA[ARXIV：2502.13943V1公告类型：新 
摘要：当前的培训过程奖励模型（PRMS）的方法通常涉及使用基于规则的技术将响应分解为多个推理步骤，例如使用预定义的占位符令牌或将推理步骤的长度设置为固定尺寸。这些方法忽略了以下事实：特定单词通常不会标记文本中的真实决策点。为了解决这个问题，我们提出了Adpaptivestep，该方法是根据模型对预测下一个单词的信心来划分推理步骤的方法。该部门方法在每个步骤中提供了更多的决策信息，从而增强了下游任务，例如奖励模型学习。此外，我们的方法不需要手动注释。我们通过在数学推理和代码生成任务中对自适应训练的PRM进行实验来证明其有效性。实验结果表明，结果PRM实现了最先进的N最佳表现，通过令牌级别的价值引导的解码超过了贪婪的搜索策略，同时还将施工成本降低了30％以上PRMS。此外，我们还提供了有关PRM的性能，可转移性和概括能力的详尽分析和案例研究。]]></description>
      <guid>https://arxiv.org/abs/2502.13943</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过大语言模型和连贯驱动的推理神经肯定人工智能</title>
      <link>https://arxiv.org/abs/2502.13953</link>
      <description><![CDATA[ARXIV：2502.13953V1公告类型：新 
摘要：我们设计了一种算法来生成一组命题，这些命题可以客观地实例化支持连贯驱动推理的图形。然后，我们基准了大型语言模型（LLM）从自然语言表达的命题（直接转换）重建相干图的能力，并从单个提示中表达出了有希望的结果，以优化推理的模型。通过神经模型将相干驱动的推断与一致性评估相结合可能会推动机器认知的最新状态。]]></description>
      <guid>https://arxiv.org/abs/2502.13953</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>