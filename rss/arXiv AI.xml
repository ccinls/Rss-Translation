<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.AI 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.AI 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Thu, 22 Aug 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>凡事皆有度：通过安全意识激活转向减轻 LLM 的夸大安全性</title>
      <link>https://arxiv.org/abs/2408.11491</link>
      <description><![CDATA[arXiv:2408.11491v1 公告类型：新
摘要：安全对齐对于大型语言模型（LLM）抵御恶意指令威胁是必不可少的。然而，最近的研究表明，由于夸大的安全问题，安全对齐的LLM容易拒绝良性查询，从而限制了它们的帮助。在本文中，我们提出了一种安全意识激活转向（SCANS）方法来缓解对齐LLM中夸大的安全问题。首先，SCANS提取激活空间内的拒绝转向向量，并利用词汇投影来锚定一些影响模型拒绝行为的特定安全关键层。其次，通过跟踪隐藏状态转换，SCANS识别转向方向并相应地引导模型行为，在夸大安全性和足够安全性之间取得平衡。实验表明，SCANS在XSTest和OKTest基准上实现了新的最佳性能，而不会损害其对有害查询的防御能力并保持几乎不变的模型能力。]]></description>
      <guid>https://arxiv.org/abs/2408.11491</guid>
      <pubDate>Fri, 23 Aug 2024 01:36:54 GMT</pubDate>
    </item>
    <item>
      <title>通过模型标签学习实现小型模型的零样本分类</title>
      <link>https://arxiv.org/abs/2408.11449</link>
      <description><![CDATA[arXiv:2408.11449v1 公告类型：新
摘要：像 CLIP 这样的视觉语言模型 (VLM) 通过对齐文本和图像在图像分类任务中展示了令人印象深刻的零样本能力，但与特定于任务的专家模型相比，其性能较差。相反，专家模型在其专业领域表现出色，但缺乏新任务的零样本能力。如何同时获得专家模型的高性能和零样本能力是一个重要的研究方向。在本文中，我们试图证明，通过构建模型中心并使用模型标签将模型与其功能对齐，可以通过有效地选择和重用中心中的模型以零样本方式解决新任务。我们引入了一种新范式，模型标签学习 (MLL)，它通过语义有向无环图 (SDAG) 弥合模型与其功能之间的差距，并利用分类头组合优化 (CHCO) 算法为新任务选择有能力的模型。与基础模型范式相比，它的成本更低，可扩展性更强，即零样本能力随着模型中心的规模而增长。在七个真实数据集上的实验验证了 MLL 的有效性和效率，表明专家模型可以有效地重用于零样本任务。我们的代码将公开发布。]]></description>
      <guid>https://arxiv.org/abs/2408.11449</guid>
      <pubDate>Fri, 23 Aug 2024 01:36:53 GMT</pubDate>
    </item>
    <item>
      <title>用于顺序推荐的双向门控曼巴</title>
      <link>https://arxiv.org/abs/2408.11451</link>
      <description><![CDATA[arXiv:2408.11451v1 公告类型：新
摘要：在各个领域，顺序推荐系统 (SRS) 因其出色的辨别复杂用户偏好的能力而变得必不可少。通常，SRS 使用基于转换器的架构来预测序列中的后续项目。然而，这些模型固有的二次计算复杂性往往导致效率低下，阻碍了实时推荐的实现。Mamba 是一项最新进展，在时间序列预测方面表现出色，显著提高了效率和准确性。然而，将 Mamba 直接集成到 SRS 中带来了一些挑战。其固有的单向性可能会限制模型捕获用户项目交互的完整上下文的能力，而其状态估计的不稳定性可能会损害其检测交互序列中短期模式的能力。
为了克服这些问题，我们引入了一个名为 \textbf{\underline{S}}elect\textbf{\underline{I}}ve \textbf{\underline{G}}ated \textbf{\underline{MA}}mba (SIGMA) 的新框架。该框架利用 Partially Flipped Mamba (PF-Mamba) 构建专门为改进上下文建模而定制的双向架构。此外，还采用了输入敏感的密集选择门 (DS Gate) 来优化方向权重并增强 PF-Mamba 中序列信息的处理。对于短序列建模，我们还开发了特征提取 GRU (FE-GRU) 来有效捕获短期依赖关系。实证结果表明，SIGMA 在五个真实数据集上的表现优于当前模型。我们的实现代码可在 \url{https://github.com/ziwliu-cityu/SIMGA} 获得，以简化可重复性。]]></description>
      <guid>https://arxiv.org/abs/2408.11451</guid>
      <pubDate>Fri, 23 Aug 2024 01:36:53 GMT</pubDate>
    </item>
    <item>
      <title>使用概率答案集编程解决决策理论问题</title>
      <link>https://arxiv.org/abs/2408.11371</link>
      <description><![CDATA[arXiv:2408.11371v1 公告类型：新
摘要：解决决策理论问题通常涉及在一组可能的行动中找到优化预期回报的行动，可能考虑到环境的不确定性。在本文中，我们介绍了通过决策原子和效用属性在信条语义下使用概率答案集编程对决策理论问题进行编码的可能性。为了解决这个任务，我们提出了一种基于三层代数模型计数的算法，我们在几个合成数据集上针对采用答案集枚举的算法进行了测试。实证结果表明，我们的算法可以在合理的时间内管理程序的非平凡实例。正在逻辑编程理论与实践 (TPLP) 中考虑。]]></description>
      <guid>https://arxiv.org/abs/2408.11371</guid>
      <pubDate>Fri, 23 Aug 2024 01:36:52 GMT</pubDate>
    </item>
    <item>
      <title>生成人工智能中的认知不公正</title>
      <link>https://arxiv.org/abs/2408.11441</link>
      <description><![CDATA[arXiv:2408.11441v1 公告类型：新
摘要：本文探讨了生成性人工智能如何潜在地破坏集体知识的完整性以及我们依赖的获取、评估和信任信息的过程，对我们的知识生态系统和民主话语构成重大威胁。基于社会和政治哲学，我们引入了 \emph{生成算法认知不公正} 的概念。我们确定了这一现象的四个关键维度：放大和操纵性的证言不公正，以及解释学无知和获取不公正。我们用现实世界的例子来说明每个维度，这些例子揭示了生成性人工智能如何产生或放大错误信息、延续表征伤害并造成认知不平等，尤其是在多语言环境中。通过强调这些不公正现象，我们旨在为认识论上公正的生成人工智能系统的发展提供信息，提出抵抗策略、系统设计原则和两种利用生成人工智能来促进更公平的信息生态系统的方法，从而维护民主价值观和知识生产的完整性。]]></description>
      <guid>https://arxiv.org/abs/2408.11441</guid>
      <pubDate>Fri, 23 Aug 2024 01:36:52 GMT</pubDate>
    </item>
    <item>
      <title>用于日常环境中动态时空推理的多模态数据集和基准</title>
      <link>https://arxiv.org/abs/2408.11347</link>
      <description><![CDATA[arXiv:2408.11347v1 公告类型：新
摘要：我们使用 3D 模拟器创建带有标准化注释的人工视频数据，旨在帮助开发具身人工智能。我们的问答 (QA) 数据集衡量机器人在家庭环境中理解人类行为和环境的程度。初步实验表明，我们的数据集有助于衡量人工智能对日常生活的理解。 \end{abstract}]]></description>
      <guid>https://arxiv.org/abs/2408.11347</guid>
      <pubDate>Fri, 23 Aug 2024 01:36:51 GMT</pubDate>
    </item>
    <item>
      <title>使用多任务几何深度学习进行蛋白质-配体复合物的一步结构预测和筛选</title>
      <link>https://arxiv.org/abs/2408.11356</link>
      <description><![CDATA[arXiv:2408.11356v1 公告类型：新
摘要：了解蛋白质-配体复合物的结构对药物开发至关重要。现有的虚拟结构测量和筛选方法以对接及其衍生方法与深度学习相结合为主。然而，采样和评分方法在很大程度上限制了准确性和效率。在这里，我们展示了这两个基本任务可以用一个基于多任务几何深度学习的单一模型 LigPose 来准确解决。通过将配体和蛋白质对表示为图形，LigPose 直接优化复合物的三维结构，以结合强度和原子相互作用的学习作为辅助任务，使其无需对接工具即可实现一步预测能力。大量实验表明，LigPose 在药物研究的主要任务上取得了最先进的性能。它的显著改进表明了基于人工智能的药物开发流程的一个有前途的范例。]]></description>
      <guid>https://arxiv.org/abs/2408.11356</guid>
      <pubDate>Fri, 23 Aug 2024 01:36:51 GMT</pubDate>
    </item>
    <item>
      <title>ProteinGPT：用于蛋白质特性预测和结构理解的多模态法学硕士</title>
      <link>https://arxiv.org/abs/2408.11363</link>
      <description><![CDATA[arXiv:2408.11363v1 公告类型：新
摘要：了解生物过程、药物开发和生物技术进步需要对蛋白质结构和序列进行详细分析，这是蛋白质研究中的一项任务，如果手动执行，本质上是复杂且耗时的。为了简化这一过程，我们引入了 ProteinGPT，这是一种最先进的多模态蛋白质聊天系统，允许用户上传蛋白质序列和/或结构以进行全面的蛋白质分析和响应式查询。ProteinGPT 将蛋白质序列和结构编码器与线性投影层无缝集成，以实现精确的表示自适应，并结合大型语言模型 (LLM) 来生成准确且与上下文相关的响应。为了训练 ProteinGPT，我们构建了一个包含 132,092 个带注释蛋白质的大规模数据集，并使用 GPT-4o 优化指令调整过程。这个创新的系统可确保用户上传的数据和提示之间的准确对齐，从而简化蛋白质分析。实验表明，ProteinGPT 可以对蛋白质及其相应的问题产生有希望的响应。]]></description>
      <guid>https://arxiv.org/abs/2408.11363</guid>
      <pubDate>Fri, 23 Aug 2024 01:36:51 GMT</pubDate>
    </item>
    <item>
      <title>面向神经符号推理和松弛的概率归纳逻辑编程</title>
      <link>https://arxiv.org/abs/2408.11367</link>
      <description><![CDATA[arXiv:2408.11367v1 公告类型：新
摘要：许多归纳逻辑编程 (ILP) 方法无法从概率背景知识中学习程序，例如来自传感数据或具有概率的神经网络。我们提出了 Propper，它通过结合神经符号推理、假设选择的连续标准 (BCE) 和假设约束的放松 (NoisyCombo) 来扩展 ILP，从而处理有缺陷和概率的背景知识。对于噪声图像中的关系模式，Propper 可以从少至 8 个示例中学习程序。它优于二元 ILP 和统计模型，例如图神经网络。]]></description>
      <guid>https://arxiv.org/abs/2408.11367</guid>
      <pubDate>Fri, 23 Aug 2024 01:36:51 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型的概率医学预测</title>
      <link>https://arxiv.org/abs/2408.11316</link>
      <description><![CDATA[arXiv:2408.11316v1 公告类型：新
摘要：大型语言模型 (LLM) 通过快速工程在临床应用中展现出巨大潜力，能够生成灵活多样的临床预测。然而，它们在生成预测概率方面带来了挑战，而预测概率对于透明度和允许临床医生在决策中应用灵活的概率阈值至关重要。虽然明确的提示指令可以引导 LLM 通过文本生成提供预测概率数字，但 LLM 在数值推理方面的局限性引发了人们对这些文本生成概率可靠性的担忧。为了评估这种可靠性，我们将从文本生成中得出的显式概率与基于预测正确标签标记的可能性计算出的隐式概率进行了比较。在五个医学数据集上使用六个高级开源 LLM 进行实验后，我们发现显式概率在区分度、准确率和召回率方面的表现始终低于隐式概率。此外，这些差异在小型 LLM 和不平衡数据集上被扩大，强调需要谨慎解释和应用，以及进一步研究临床环境中 LLM 的稳健概率估计方法。]]></description>
      <guid>https://arxiv.org/abs/2408.11316</guid>
      <pubDate>Fri, 23 Aug 2024 01:36:50 GMT</pubDate>
    </item>
    <item>
      <title>搜索思维自动化：迈向健全性和完整性的旅程</title>
      <link>https://arxiv.org/abs/2408.11326</link>
      <description><![CDATA[arXiv:2408.11326v1 公告类型：新
摘要：规划仍然是大型语言模型 (LLM) 的最后堡垒之一，现在它们将注意力转向搜索。大多数文献使用语言模型作为世界模型来定义搜索空间，为了灵活性而放弃了健全性。最近的一项工作，搜索思想 (ToS)，提出用代码定义搜索空间，让语言模型生成该代码。ToS 需要一个人参与其中，共同生成合理的后继函数和目标测试。然而，结果是值得的：所有测试数据集都以 100% 的准确率得到解决。同时，LLM 在复杂推理任务的代码生成和细化方面取得了重大进展。在这项工作中，我们自动化 ToS (AutoToS)，完全将人从解决规划问题的循环中剔除。AutoToS 通过通用和领域特定单元测试的反馈，逐步指导语言模型生成合理完整的搜索组件。我们在所有评估域上使用各种大小的 LLM，以最少的反馈迭代实现了 100% 的准确率。]]></description>
      <guid>https://arxiv.org/abs/2408.11326</guid>
      <pubDate>Fri, 23 Aug 2024 01:36:50 GMT</pubDate>
    </item>
    <item>
      <title>自动数据集构建 (ADC)：样本收集、数据管理及其他</title>
      <link>https://arxiv.org/abs/2408.11338</link>
      <description><![CDATA[arXiv:2408.11338v1 公告类型：新
摘要：大规模数据收集对于开发个性化训练数据、缓解训练数据短缺和微调专门模型至关重要。然而，由于注释错误、大量时间和人力成本，快速准确地创建高质量数据集仍然是一项挑战。为了解决这些问题，我们提出了自动数据集构建 (ADC)，这是一种创新方法，可以以极低的成本和高效率自动创建数据集。以图像分类任务为起点，ADC 利用 LLM 进行详细的类设计和代码生成，通过搜索引擎收集相关样本，大大减少了对手动注释的需求并加快了数据生成过程。尽管有这些优势，ADC 也遇到了现实世界的挑战，例如标签错误（标签噪声）和不平衡的数据分布（标签偏差）。我们提供开源软件，结合现有的标签错误检测方法、在嘈杂和有偏差的数据下进行稳健学习，确保更高质量的训练数据和更稳健的模型训练程序。此外，我们设计了三个基准数据集，分别针对标签噪声检测、标签噪声学习和类别不平衡学习。这些数据集至关重要，因为尽管标签噪声检测非常重要，但现有的专门用于标签噪声检测的数据集却很少。最后，我们在这些数据集上评估了现有流行方法的性能，从而促进该领域的进一步研究。]]></description>
      <guid>https://arxiv.org/abs/2408.11338</guid>
      <pubDate>Fri, 23 Aug 2024 01:36:50 GMT</pubDate>
    </item>
    <item>
      <title>在心理健康护理中应用和评估大型语言模型：对人工评估的生成任务的范围审查</title>
      <link>https://arxiv.org/abs/2408.11288</link>
      <description><![CDATA[arXiv:2408.11288v1 公告类型：新
摘要：大型语言模型 (LLM) 正在成为心理健康护理的有前途的工具，通过其生成类似人类的反应的能力提供可扩展的支持。然而，这些模型在临床环境中的有效性仍不清楚。本次范围审查旨在评估 LLM 在心理健康护理中的当前生成应用，重点关注在现实场景中与人类参与者一起测试这些模型的研究。在 APA PsycNet、Scopus、PubMed 和 Web of Science 中进行系统搜索，发现了 726 篇独特的文章，其中 17 篇符合纳入标准。这些研究涵盖了临床援助、咨询、治疗和情感支持等应用。然而，评估方法通常是非标准化的，大多数研究依赖于限制可比性和稳健性的临时量表。隐私、安全和公平也经常被低估。此外，对专有模型（例如 OpenAI 的 GPT 系列）的依赖引发了人们对透明度和可重复性的担忧。虽然 LLM 在扩大心理健康护理覆盖面方面显示出潜力，尤其是在服务不足的地区，但目前的证据并不完全支持将其用作独立干预措施。需要更严格、标准化的评估和道德监督，以确保这些工具能够安全有效地融入临床实践。]]></description>
      <guid>https://arxiv.org/abs/2408.11288</guid>
      <pubDate>Fri, 23 Aug 2024 01:36:49 GMT</pubDate>
    </item>
    <item>
      <title>EEG-Defender：通过早期退出大型语言模型生成来防御越狱</title>
      <link>https://arxiv.org/abs/2408.11308</link>
      <description><![CDATA[arXiv:2408.11308v1 公告类型：新
摘要：大型语言模型 (LLM) 在各种应用中越来越受到关注。尽管如此，人们越来越担心一些用户试图利用这些模型进行恶意目的，包括合成管制物质和传播虚假信息。为了减轻这种风险，开发了“对齐”技术的概念。然而，最近的研究表明，这种对齐可以通过复杂的提示工程或对抗性后缀来破坏，这种技术被称为“越狱”。我们的研究从 LLM 的类似人类的生成过程中获得了线索。我们发现，虽然越狱提示可能会产生类似于良性提示的输出逻辑，但它们在模型潜在空间中的初始嵌入往往更类似于恶意提示。利用这一发现，我们建议利用 LLM 的早期变压器输出作为检测恶意输入的手段，并立即终止生成。基于这一想法，我们引入了一种简单但重要的防御方法，即针对 LLM 的 EEG-Defender。我们对三种模型的十种越狱方法进行了全面的实验。我们的结果表明，EEG-Defender 能够显著降低攻击成功率 (ASR)，大约为 85%，而目前的 SOTA 为 50%，对 LLM 的实用性和有效性的影响最小。]]></description>
      <guid>https://arxiv.org/abs/2408.11308</guid>
      <pubDate>Fri, 23 Aug 2024 01:36:49 GMT</pubDate>
    </item>
    <item>
      <title>无需肯定短语即可解锁对抗性后缀优化：通过 LLM 作为优化器进行高效的黑盒越狱</title>
      <link>https://arxiv.org/abs/2408.11313</link>
      <description><![CDATA[arXiv:2408.11313v1 公告类型：新 
摘要：尽管之前进行了安全性调整，但主流 LLM 在受到越狱攻击时仍然会生成有害和不道德的内容。现有的越狱方法主要分为两类：基于模板的方法和基于优化的方法。前者需要大量的人工努力和领域知识，而后者以贪婪坐标梯度 (GCG) 为例，它试图通过 token 级优化来最大化有害 LLM 输出的可能性，但也遇到了一些限制：需要白盒访问、需要预先构建的肯定短语，并且效率低下。在本文中，我们提出了一种利用可优化后缀的新颖而高效的黑盒越狱方法 ECLIPSE。从 LLM 强大的生成和优化功能中汲取灵感，我们使用任务提示将越狱目标转化为自然语言指令。这指导 LLM 为恶意查询生成对抗性后缀。尤其是危害性评分器提供了持续的反馈，使 LLM 能够自我反思和迭代优化，从而自主高效地生成有效后缀。实验结果表明，ECLIPSE 在三个开源 LLM 和 GPT-3.5-Turbo 上的平均攻击成功率 (ASR) 达到 0.92，显著超过 GCG 的 2.4 倍。此外，ECLIPSE 在 ASR 方面与基于模板的方法相当，同时提供了卓越的攻击效率，将平均攻击开销降低了 83%。]]></description>
      <guid>https://arxiv.org/abs/2408.11313</guid>
      <pubDate>Fri, 23 Aug 2024 01:36:49 GMT</pubDate>
    </item>
    <item>
      <title>欧盟人工智能法案中通用人工智能不确定性估计的困境</title>
      <link>https://arxiv.org/abs/2408.11249</link>
      <description><![CDATA[arXiv:2408.11249v1 公告类型：新
摘要：AI法案是欧盟范围内对AI系统的监管。它包括针对通用AI模型的具体规定，但需要在技术标准和最新研究方面进一步解释，以确保切实可行的合规解决方案。本文研究了AI法案对通用AI提供者和部署者的要求，并进一步提出不确定性估计作为此类模型培训中法律合规和质量保证的合适措施。我们认为不确定性估计应该是现实世界中部署模型的必需组成部分，根据欧盟AI法案，它可以满足透明度、准确性和可信度的若干要求。然而，通常使用不确定性估计方法会增加计算量，从而产生一个困境，因为计算可能会超过阈值（$10^{25}$ FLOPS），从而将模型归类为承担更多监管负担的系统性风险系统。]]></description>
      <guid>https://arxiv.org/abs/2408.11249</guid>
      <pubDate>Fri, 23 Aug 2024 01:36:48 GMT</pubDate>
    </item>
    <item>
      <title>改进现代和现成的语音识别器的语音识别错误预测</title>
      <link>https://arxiv.org/abs/2408.11258</link>
      <description><![CDATA[arXiv:2408.11258v1 公告类型：新
摘要：对语音识别器的错误进行建模有助于模拟从纯文本中识别出的错误语音数据，这已被证明对诸如判别语言建模、提高 NLP 系统的鲁棒性等任务很有用，因为在训练时可用的音频数据有限甚至没有。以前的工作通常考虑复制基于 GMM-HMM 的系统的行为，但更现代的基于后验的神经网络声学模型的行为并不相同，需要调整错误预测模型。在这项工作中，我们通过两种方式扩展了先前的基于语音混淆的语音识别错误预测模型：首先，我们引入了一种基于采样的范式，可以更好地模拟基于后验的声学模型的行为。其次，我们研究用序列到序列模型替换混淆矩阵，以便在预测中引入上下文依赖性。我们通过两种方式评估错误预测器：首先预测 Switchboard ASR 系统在未见数据 (Fisher) 上产生的错误，然后使用同一预测器估计不相关的基于云的 ASR 系统在新任务上的行为。抽样极大地提高了 100 次猜测范式中的预测准确性，而序列模型的表现与混淆矩阵类似。]]></description>
      <guid>https://arxiv.org/abs/2408.11258</guid>
      <pubDate>Fri, 23 Aug 2024 01:36:48 GMT</pubDate>
    </item>
    <item>
      <title>分析和缓解大型视觉语言模型中的谄媚现象</title>
      <link>https://arxiv.org/abs/2408.11261</link>
      <description><![CDATA[arXiv:2408.11261v1 公告类型：新
摘要：大型视觉语言模型 (LVLM) 在视觉语言理解方面表现出了显著的能力。然而，这些模型中仍然存在一个关键问题，那就是谄媚，这意味着模型受到引导或欺骗性提示的过度影响，导致输出有偏差和产生幻觉。尽管 LVLM 取得了进展，但评估和缓解谄媚仍未得到充分探索。在这项工作中，我们通过系统地分析各种 VL 基准上的谄媚和精心策划的主导查询，并进一步提出了一种用于缓解的文本对比解码方法来填补这一空白。虽然具体的谄媚行为在不同模型之间存在很大差异，但我们的分析揭示了所有 LVLM 在各种任务中对谄媚的恢复能力都存在严重缺陷。为了改进，我们提出了领先查询对比解码 (LQCD)，这是一种与模型无关的方法，专注于通过在解码阶段识别和抑制谄媚标记的概率来校准 LVLM 对领先线索的过度依赖。大量实验表明，LQCD 可以有效缓解谄媚，其效果优于提示工程方法和常用的幻觉缓解方法。我们进一步证明，LQCD 不会损害 LVLM 对中性查询的响应，甚至会略有改善，这表明它是一种更有效的通用解码策略，而不仅限于谄媚。]]></description>
      <guid>https://arxiv.org/abs/2408.11261</guid>
      <pubDate>Fri, 23 Aug 2024 01:36:48 GMT</pubDate>
    </item>
    <item>
      <title>BearLLM：具有统一振动信号表示的先验知识增强型轴承健康管理框架</title>
      <link>https://arxiv.org/abs/2408.11281</link>
      <description><![CDATA[arXiv:2408.11281v1 公告类型：新
摘要：我们提出了一种利用大型语言模型 (BearLLM) 的轴承健康管理框架，这是一种新颖的多模态模型，通过处理用户提示和振动信号来统一多个与轴承相关的任务。具体而言，我们引入了一种先验知识增强的统一振动信号表示来处理多个数据集中的各种工作条件。这涉及根据传感器的采样率自适应地对振动信号进行采样，结合频域以统一输入维度，并使用无故障参考信号作为辅助输入。为了从振动信号中提取特征，我们首先训练一个故障分类网络，然后将提取的特征转换并对齐为词嵌入，最后将它们与文本嵌入连接起来作为 LLM 的输入。为了评估所提出方法的性能，我们构建了第一个大规模多模态轴承健康管理 (MBHM) 数据集，包括成对的振动信号和文本描述。凭借我们统一的振动信号表示，BearLLM 使用一组预训练权重在九个公开的故障诊断基准上实现了最佳性能，优于为各个数据集设计的特定方法。我们提供数据集、模型和代码，以启发未来研究构建更强大的工业多模态模型 (https://github.com/hatton613/BearLLM)。]]></description>
      <guid>https://arxiv.org/abs/2408.11281</guid>
      <pubDate>Fri, 23 Aug 2024 01:36:48 GMT</pubDate>
    </item>
    <item>
      <title>利用注意力头掩蔽进行分布外检测，实现多模态文档分类</title>
      <link>https://arxiv.org/abs/2408.11237</link>
      <description><![CDATA[arXiv:2408.11237v1 公告类型：新
摘要：检测分布不均 (OOD) 数据对于机器学习应用至关重要，可以降低模型过度自信的风险，从而提高部署系统的可靠性和安全性。现有的大多数 OOD 检测方法主要处理单模态输入，例如图像或文本。在多模态文档的背景下，对这些方法的性能缺乏广泛的研究，这些方法主要是针对计算机视觉任务开发的。我们提出了一种称为注意头掩蔽 (AHM) 的新方法，用于文档分类系统中的多模态 OOD 任务。我们的实证结果表明，所提出的 AHM 方法优于所有最先进的方法，并且与现有解决方案相比，误报率 (FPR) 显着降低高达 7.5%。该方法可以很好地推广到多模态数据，例如文档，其中视觉和文本信息在相同的 Transformer 架构下建模。为了解决高质量公开文档数据集稀缺的问题并鼓励对文档 OOD 检测的进一步研究，我们推出了 FinanceDocs，这是一个新的文档 AI 数据集。我们的代码和数据集都是公开的。]]></description>
      <guid>https://arxiv.org/abs/2408.11237</guid>
      <pubDate>Fri, 23 Aug 2024 01:36:47 GMT</pubDate>
    </item>
    </channel>
</rss>