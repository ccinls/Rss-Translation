<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.AI 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.AI 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Tue, 18 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>通过图嵌入进行最佳多智能体路径查找的算法选择</title>
      <link>https://arxiv.org/abs/2406.10827</link>
      <description><![CDATA[arXiv:2406.10827v1 公告类型：新
摘要：多智能体路径查找 (MAPF) 是为多个智能体查找路径以使它们不发生碰撞的问题。这个问题体现在现实世界的许多应用中，例如控制自动化仓库中的运输机器人、在视频游戏中移动角色以及在交叉路口协调自动驾驶汽车。寻找 MAPF 的最佳解决方案是 NP-Hard，但现代最优求解器可以扩展到数百个智能体，在某些情况下甚至数千个智能体。不同的求解器采用不同的方法，并且没有一种最先进的方法可以解决所有问题。此外，没有明确的、可证明的指导方针来选择何时使用每个最佳 MAPF 求解器。先前的工作采用算法选择 (AS) 技术从过去的数据中学习此类指导方针。使用 AS 选择最佳 MAPF 算法时的一个主要挑战是如何对给定的 MAPF 问题进行编码。先前的工作要么使用手工制作的特征，要么使用问题的图像表示。我们探索了基于图的 MAPF 问题编码，并展示了如何将它们与现代图嵌入算法 FEATHER 实时结合使用。然后，我们展示了如何将此编码与现有编码有效结合，从而产生一种新颖的 AS 方法，我们称之为通过图嵌入进行 MAPF 算法选择 (MAG)。在多个 MAPF 算法选择任务上对 MAG 进行了广泛的实验评估，结果表明它与现有方法相当或明显优于现有方法。]]></description>
      <guid>https://arxiv.org/abs/2406.10827</guid>
      <pubDate>Tue, 18 Jun 2024 06:27:36 GMT</pubDate>
    </item>
    <item>
      <title>TorchOpera：针对 LLM 安全的复合 AI 系统</title>
      <link>https://arxiv.org/abs/2406.10847</link>
      <description><![CDATA[arXiv:2406.10847v1 公告类型：新
摘要：我们介绍了 TorchOpera，这是一种复合 AI 系统，用于增强大型语言模型的提示和响应的安全性和质量。TorchOpera 确保所有用户提示都是安全的、基于上下文的和有效处理的，同时增强 LLM 响应的相关性和高质量。TorchOpera 利用向量数据库进行上下文基础，利用基于规则的包装器进行灵活修改，以及专门的机制来检测和调整不安全或不正确的内容。我们还提供了复合 AI 系统的视图以降低计算成本。大量实验表明，TorchOpera 在保持 LLM 响应效率的同时，确保了 LLM 在现实环境中的安全性、可靠性和适用性。]]></description>
      <guid>https://arxiv.org/abs/2406.10847</guid>
      <pubDate>Tue, 18 Jun 2024 06:27:36 GMT</pubDate>
    </item>
    <item>
      <title>演示笔记本：从互动中寻找最适合的情境学习示例</title>
      <link>https://arxiv.org/abs/2406.10878</link>
      <description><![CDATA[arXiv:2406.10878v1 公告类型：新
摘要：大型语言模型 (LLM) 从快速工程中受益匪浅，其中情境学习是关键技术。虽然以前的方法提供了构建用于情境学习的演示的各种方法，但它们通常忽略了数据集内固有的异质性，将相同的演示应用于所有推理问题。我们观察到演示的有效性因具体问题而异。这促使我们探索使用快速工程来选择合适的演示。为了应对自动创建和选择针对每个问题的演示的挑战，我们提出了一种围绕称为“演示笔记本”的新对象构建的新型快速工程工作流程。该笔记本通过收集和重用 LLM 过去交互中的信息来帮助确定最适合问题的情境学习示例。我们的实验表明，这种方法优于所有现有的自动演示构建和选择方法（据我们所知），在多个推理基准上取得了最先进的结果。该方法在文本摘要和即时压缩任务中的成功进一步证明了其多功能性。此外，我们还提供了一种严格的分析方法来揭示演示的“演示机制”，从而提供了有关演示如何与数据集内的不同问题类型相关的宝贵见解。]]></description>
      <guid>https://arxiv.org/abs/2406.10878</guid>
      <pubDate>Tue, 18 Jun 2024 06:27:36 GMT</pubDate>
    </item>
    <item>
      <title>一次性评估多个问题的法学硕士：探究法学硕士能力的新范式</title>
      <link>https://arxiv.org/abs/2406.10786</link>
      <description><![CDATA[arXiv:2406.10786v1 公告类型：新
摘要：当前 LLM 评估主要使用包含单个问题的提示进行评估。我们提出多问题评估作为研究 LLM 处理多个问题能力的另一种方法。我们通过全面检查 7 个 LLM 在 6 个分类基准构建的 4 种相关任务类型上的情况，对此进行了系统研究。这 4 种任务类型包括传统的单问题任务、同质多问题任务和嵌入多问题任务的两个索引选择任务。我们发现 LLM 是称职的多问题解决者：它们在多问题任务上的表现通常（几乎）与单问题任务一样好。此外，与普遍预期相反，它们通常不会受到长输入的位置偏差的影响。这使得多问题提示成为一种具有实际意义的简单且经济高效的提示方法。然而，我们的结果也强烈地表明，LLM 缺乏真正的理解：尽管它们通常可以进行索引选择，但它们在两个索引选择任务中的表现明显差于各种评估设置下的多问题任务。]]></description>
      <guid>https://arxiv.org/abs/2406.10786</guid>
      <pubDate>Tue, 18 Jun 2024 06:27:35 GMT</pubDate>
    </item>
    <item>
      <title>HiddenTables 和 PyQTax：TableQA 的合作游戏和数据集，可确保跨众多分类法的规模和数据隐私</title>
      <link>https://arxiv.org/abs/2406.10803</link>
      <description><![CDATA[arXiv:2406.10803v1 公告类型：新
摘要：无数不同的大型语言模型 (LLM) 在上下文分析表格问答任务时面临着共同的挑战。这些挑战源于 (1) 大型表格的有限上下文窗口、(2) 标记化模式与单元格边界之间的多方面差异，以及 (3) 在使用外部模型（如 gpt-3.5-turbo）的过程中，由于数据机密性而产生的各种限制。我们提出了一款名为“HiddenTables”的合作游戏作为这一挑战的潜在解决方案。本质上，“HiddenTables”是在代码生成 LLM“Solver”和“Oracle”之间进行的，后者评估 LLM 代理解决表格问答任务的能力。这个游戏基于自然语言模式，重要的是，它确保了底层数据的安全性。我们对一组不同的表格进行了证据实验，这些实验表明，当提供具体的表格模式时，LLM 无法概括和执行复杂的查询、处理组合依赖关系以及将自然语言与编程命令对齐。与基于编码器的模型不同，我们突破了“HiddenTables”的界限，不受行数的限制 - 因此我们在提示和完成标记方面表现出更高的效率。我们的基础设施催生了一个新的数据集“PyQTax”，它涵盖了 116,671 个问题表答案三元组，并为不同的问题分类法提供了额外的细粒度细分和标签。因此，与我们在 LLM 在 TableQA 任务中的缺陷方面的学术贡献相结合，“HiddenTables”是 LLM 如何与海量数据集交互的触觉体现，同时确保数据安全并最大限度地降低生成成本。]]></description>
      <guid>https://arxiv.org/abs/2406.10803</guid>
      <pubDate>Tue, 18 Jun 2024 06:27:35 GMT</pubDate>
    </item>
    <item>
      <title>用于交通系统优化基准测试的 GPU 加速大型模拟器</title>
      <link>https://arxiv.org/abs/2406.10661</link>
      <description><![CDATA[arXiv:2406.10661v1 Announce Type: new 
摘要：随着人工智能技术的发展，交通系统优化正从传统的依赖专家经验的方法演变为基于模拟和学习的决策优化方法。基于学习的优化方法需要与高度逼真的微观交通模拟器进行大量交互才能进行优化。然而，现有的微观交通模拟器在大规模场景下计算效率低下，因此大大降低了优化算法的数据采样过程的效率。此外，现有模拟器支持的优化场景有限，主要集中在交通信号控制上。针对这些挑战和限制，我们提出了第一个开源的GPU加速的大规模交通系统仿真微观模拟器。该模拟器能够以84.09Hz的频率迭代，与最佳基线相比，在超过一百万辆车的大规模场景中实现了88.92倍的计算加速。基于该模拟器，我们实现了一组微观和宏观可控对象和指标，以支持大多数典型的交通系统优化场景。这些可控对象和指标均通过 Python API 提供，方便使用。我们选择了五个重要且具有代表性的交通系统优化场景，并在四个城市对经典的基于规则的算法、强化学习和黑盒优化进行了基准测试。代码可在 \url{https://github.com/tsinghua-fib-lab/moss-benchmark} 上以 MIT 许可证获取。]]></description>
      <guid>https://arxiv.org/abs/2406.10661</guid>
      <pubDate>Tue, 18 Jun 2024 06:27:34 GMT</pubDate>
    </item>
    <item>
      <title>弥合药品安全数据分析的差距：用于 SQL 查询生成的大型语言模型</title>
      <link>https://arxiv.org/abs/2406.10690</link>
      <description><![CDATA[arXiv:2406.10690v1 公告类型：新
摘要：药物警戒 (PV) 对药物安全至关重要，主要侧重于不良事件监测。传统上，访问安全数据需要数据库专业知识，限制了更广泛的使用。本文介绍了大型语言模型 (LLM) 的一种新应用，以使非技术用户能够民主化数据库访问。利用 OpenAI 的 GPT-4，我们开发了一个聊天机器人，它可以从自然语言生成结构化查询语言 (SQL) 查询，从而弥合领域知识和技术要求之间的差距。拟议的应用程序旨在实现更具包容性和效率的数据访问，从而增强药物安全决策能力。通过为 LLM 提供专家知识的通俗易懂的语言摘要，我们的方法与仅依赖数据库模式的方法相比，查询准确性显著提高。在这种情况下，LLM 的应用不仅优化了 PV 数据分析，确保及时准确的药物安全报告——这是药物不良反应监测的重要组成部分——而且还促进了更安全的药理学实践和各个数据密集型领域的明智决策。]]></description>
      <guid>https://arxiv.org/abs/2406.10690</guid>
      <pubDate>Tue, 18 Jun 2024 06:27:34 GMT</pubDate>
    </item>
    <item>
      <title>SyntheT2C：生成合成数据以在 Text2Cypher 任务上微调大型语言模型</title>
      <link>https://arxiv.org/abs/2406.10710</link>
      <description><![CDATA[arXiv:2406.10710v1 公告类型：新
摘要：将大型语言模型 (LLM) 与现有的知识图谱 (KG) 数据库集成为增强 LLM 的功效和减轻其“幻觉”提供了一条有希望的途径。鉴于大多数 KG 驻留在只能通过专门的查询语言（例如 Cypher）访问的图形数据库中，因此迫切需要通过自动将自然语言转换为 Cypher 查询（通常称为“Text2Cypher”任务）来弥合 LLM 和 KG 数据库之间的鸿沟。先前的努力试图通过监督微调来增强 LLM 在 Cypher 生成方面的熟练程度。然而，这些探索因缺乏带注释的查询-Cypher 对数据集而受到阻碍，这是由于注释此类数据集的劳动密集型和领域特定性。在本研究中，我们提出了 SyntheT2C，这是一种构建合成查询密码对数据集的方法，包含两个不同的流程：(1) 基于 LLM 的提示和 (2) 模板填充。SyntheT2C 有助于生成大量查询密码对，其值取自底层 Neo4j 图形数据库。随后，SyntheT2C 应用于两个医学数据库，最终创建了一个合成数据集 MedT2C。综合实验表明，MedT2C 数据集有效提高了主干 LLM 在 Text2Cypher 任务上的性能。SyntheT2C 代码库和 MedT2C 数据集都将很快发布。]]></description>
      <guid>https://arxiv.org/abs/2406.10710</guid>
      <pubDate>Tue, 18 Jun 2024 06:27:34 GMT</pubDate>
    </item>
    <item>
      <title>为了科学解释黑箱：重温生成人工智能时代的科学方法</title>
      <link>https://arxiv.org/abs/2406.10557</link>
      <description><![CDATA[arXiv:2406.10557v1 公告类型：新
摘要：科学方法是人类在自然科学和应用科学所有分支领域进步的基石，从了解人体到解释宇宙如何运作。科学方法基于识别系统规则或原理，这些规则或原理以可重复的方式描述感兴趣的现象，可以通过实验证据进行验证。在人工智能 (AI) 时代，人们正在讨论人工智能系统如何发现新知识。我们认为，在人工智能出现之前，人类复杂的科学发现推理仍然至关重要。然而，人工智能可以通过可解释的人工智能用于科学发现。更具体地说，了解人工智能系统用于决策的数据可以成为与领域专家和科学家的接触点，这可能导致对给定科学问题的不同或趋同的观点。不同的观点可能会引发进一步的科学研究，从而产生新的科学知识。趋同的观点反而可以确保人工智能系统在人类认为合理的范围内运行。后一点涉及的是医学等应用科学中的关键应用所必需的可信度要求。]]></description>
      <guid>https://arxiv.org/abs/2406.10557</guid>
      <pubDate>Tue, 18 Jun 2024 06:27:33 GMT</pubDate>
    </item>
    <item>
      <title>QDA-SQL：针对多轮文本转 SQL 的问题增强对话增强</title>
      <link>https://arxiv.org/abs/2406.10593</link>
      <description><![CDATA[arXiv:2406.10593v1 公告类型：新
摘要：针对特定领域任务对大型语言模型 (LLM) 进行微调在文本到 SQL 任务中取得了巨大成功。然而，这些微调模型在多轮文本到 SQL 任务中经常面临由模糊或无法回答的问题引起的挑战。希望增强 LLM 以处理多轮文本到 SQL 任务中的多种类型问题。为了解决这个问题，我们提出了一种称为 QDA-SQL 的新型数据增强方法，它使用 LLM 生成多种类型的多轮问答对。在 QDA-SQL 中，我们引入了一种结合验证和纠正机制的新型数据增强方法来处理复杂的多轮文本到 SQL 任务。实验结果表明，QDA-SQL 使微调模型在 SQL 语句准确性方面表现出更高的性能，并增强了它们处理多轮文本到 SQL 任务中复杂、无法回答的问题的能力。生成脚本和测试集发布于https://github.com/mcxiaoxiao/QDA-SQL。]]></description>
      <guid>https://arxiv.org/abs/2406.10593</guid>
      <pubDate>Tue, 18 Jun 2024 06:27:33 GMT</pubDate>
    </item>
    <item>
      <title>任务层面学习：一种快速优化的结构化方法</title>
      <link>https://arxiv.org/abs/2406.10504</link>
      <description><![CDATA[arXiv:2406.10504v1 公告类型：新 
摘要：给定一个基本描述及其训练示例的任务，提示优化是将给定的信息合成为大型语言模型 (LLM) 的文本提示的问题。人类通过考虑定义任务的不同方面（例如反例、解释、类比）并将它们包含在提示中来解决这个问题。然而，目前尚不清楚现有的基于迭代编辑给定提示或自动选择一些上下文示例的算法方法是否可以涵盖解决复杂任务所需的多个方面。在这项工作中，我们将提示优化视为从一组训练示例中学习任务的多个方面。我们识别并利用提示优化问题中的结构——首先，我们发现提示可以分解为松散耦合的语义部分，这些部分对提示的性能具有相对独立的影响；其次，我们对输入空间进行聚类并使用聚类批次，以便优化过程可以跨批次学习任务的不同方面。 由此产生的算法 UniPrompt 由一个生成模型组成，用于为每个提示部分生成初始候选项；以及一个反馈机制，该机制将来自多个小批次的建议编辑聚合成该部分的概念描述。 对多个数据集和实际任务的实证评估表明，使用 UniPrompt 生成的提示比人工调整的提示和来自最先进方法的提示具有更高的准确性。 特别是，我们的算法可以生成现有方法无法生成的长而复杂的提示。 UniPrompt 的代码将在 \url{https://aka.ms/uniprompt} 提供。]]></description>
      <guid>https://arxiv.org/abs/2406.10504</guid>
      <pubDate>Tue, 18 Jun 2024 06:27:32 GMT</pubDate>
    </item>
    <item>
      <title>反应堆 Mk.1 性能：MMLU、HumanEval 和 BBH 测试结果</title>
      <link>https://arxiv.org/abs/2406.10515</link>
      <description><![CDATA[arXiv:2406.10515v1 Announce Type: new 
摘要：本文通过基准测试流程分析，展示了 ARC 旗舰大型语言模型 Reactor Mk.1 的性能结果。该模型采用 Lychee AI 引擎，拥有不到 1000 亿个参数，兼具效率和潜力。Reactor Mk.1 的表现优于 GPT-4o、Claude Opus 和 Llama 3 等模型，在 MMLU 数据集上取得了 92% 的得分，在 HumanEval 数据集上取得了 91% 的得分，在 BBH 数据集上取得了 88% 的得分。它在处理困难任务和推理方面都表现出色，成为当今尖端 AI 技术中一个突出的 AI 解决方案。]]></description>
      <guid>https://arxiv.org/abs/2406.10515</guid>
      <pubDate>Tue, 18 Jun 2024 06:27:32 GMT</pubDate>
    </item>
    <item>
      <title>使用大型语言模型生成和改进高速公路驾驶的奖励函数</title>
      <link>https://arxiv.org/abs/2406.10540</link>
      <description><![CDATA[arXiv:2406.10540v1 公告类型：新
摘要：强化学习 (RL) 通过最大化奖励函数来实现最佳策略，在推动自动驾驶技术方面发挥着至关重要的作用。然而，在许多实践中，制定这些奖励函数一直是一个复杂的手动过程。为了降低这种复杂性，我们引入了一个新颖的框架，将大型语言模型 (LLM) 与 RL 相结合，以改进自动驾驶中的奖励函数设计。该框架利用在其他领域得到验证的 LLM 编码功能来生成和发展高速公路场景的奖励函数。该框架首先指示 LLM 根据驾驶环境和任务描述创建初始奖励函数代码。然后通过涉及 RL 训练和 LLM 反思的迭代循环来完善此代码，这得益于它们审查和改进输出的能力。我们还开发了一个特定的提示模板，以提高 LLM 对复杂驾驶模拟的理解，确保生成有效且无错误的代码。我们在高速公路驾驶模拟器中对三种交通配置进行的实验表明，我们的方法超越了专家手工制作的奖励函数，平均成功率提高了 22%。这不仅表明驾驶更安全，还表明开发效率显著提高。]]></description>
      <guid>https://arxiv.org/abs/2406.10540</guid>
      <pubDate>Tue, 18 Jun 2024 06:27:32 GMT</pubDate>
    </item>
    <item>
      <title>基于法学硕士的生成物联网的有效提示</title>
      <link>https://arxiv.org/abs/2406.10382</link>
      <description><![CDATA[arXiv:2406.10382v1 公告类型：新
摘要：大型语言模型 (LLM) 在各种任务上都表现出了卓越的能力，将 LLM 的能力集成到物联网 (IoT) 应用中最近引起了很多研究关注。出于安全考虑，许多机构避免访问最先进的商业 LLM 服务，需要在本地网络环境中部署和使用开源 LLM。然而，开源 LLM 通常在性能方面有更多限制，例如算术计算和推理能力，而将 LLM 应用于物联网的实际系统尚未得到充分探索。因此，我们在本研究中提出了一种部署在本地网络环境中的基于文本的生成物联网 (GIoT) 系统。为了减轻 LLM 的局限性并提供具有竞争力的服务，我们应用提示工程方法来增强开源 LLM 的能力，设计提示管理模块和后处理模块来管理针对不同任务的定制提示并处理 LLM 生成的结果。为了证明所提系统的有效性，我们以具有挑战性的表格问答 (Table-QA) 任务作为所提系统的案例研究，因为表格数据通常比纯文本更具挑战性，因为它们的结构复杂、数据类型多样且有时规模巨大。我们在两个流行的 Table-QA 数据集上进行了全面的实验，结果表明，我们的提案可以实现与最先进的 LLM 相比具有竞争力的性能，这表明所提出的基于 LLM 的 GIoT 系统可以通过量身定制的提示方法提供具有竞争力的性能，并且无需训练即可轻松扩展到新任务。]]></description>
      <guid>https://arxiv.org/abs/2406.10382</guid>
      <pubDate>Tue, 18 Jun 2024 06:27:31 GMT</pubDate>
    </item>
    <item>
      <title>通过最大多样性微调释放大型语言模型的规划能力</title>
      <link>https://arxiv.org/abs/2406.10479</link>
      <description><![CDATA[arXiv:2406.10479v1 公告类型：新
摘要：大型语言模型 (LLM) 已经展示了令人印象深刻的任务解决能力，这是通过提示技术或系统设计实现的。然而，人们对它们在规划任务中的熟练程度产生了担忧，因为它们往往难以生成有效的计划。本文研究了微调对 LLM 规划能力的影响。我们的研究结果表明，LLM 可以通过大量（数千个具体示例）微调在规划中实现良好的性能。然而，微调会带来巨大的经济和计算成本。为了应对这一挑战，我们提出了最大多样性微调 (MDFT) 策略来提高规划领域微调的样本效率。具体来说，我们的算法称为 MDFT-g，它使用规划任务实例的图形表示对其进行编码，并在向量空间中选择最大化数据多样性的样本子集。我们通过实证研究证明，MDFT-g 在多个基准领域的不同规模上始终优于现有基线。]]></description>
      <guid>https://arxiv.org/abs/2406.10479</guid>
      <pubDate>Tue, 18 Jun 2024 06:27:31 GMT</pubDate>
    </item>
    <item>
      <title>ResearchArena：对法学硕士作为研究代理人收集和组织信息的能力进行基准测试</title>
      <link>https://arxiv.org/abs/2406.10291</link>
      <description><![CDATA[arXiv:2406.10291v1 公告类型：新
摘要：大型语言模型 (LLM) 在自然语言处理的各种任务中表现出色。然而，当这些任务需要特定领域的专业知识和高级分析技能时，挑战仍然存在，例如对指定主题进行研究调查。在这项研究中，我们开发了 ResearchArena，这是一个衡量 LLM 代理进行学术调查能力的基准，这是学术研究过程的第一步。具体来说，我们将调查过程解构为三个阶段 1) 信息发现：查找相关论文，2) 信息选择：评估论文对主题的重要性，3) 信息组织：将论文组织成有意义的结构。特别是，我们建立了一个离线环境，包括 12.0M 全文学术论文和 7.9K 调查论文，评估代理查找支持材料以编写主题调查的能力，根据其影响力对找到的论文进行排序，并将它们组织成分层知识思维导图。通过这个基准，我们对现有技术进行了初步评估，发现所有基于 LLM 的方法与基于关键字的基本检索技术相比表现不佳，这为未来的研究带来了巨大的机会。]]></description>
      <guid>https://arxiv.org/abs/2406.10291</guid>
      <pubDate>Tue, 18 Jun 2024 06:27:30 GMT</pubDate>
    </item>
    <item>
      <title>自动标记价值 2000 亿美元的救命数据集：大型临床试验结果基准</title>
      <link>https://arxiv.org/abs/2406.10292</link>
      <description><![CDATA[arXiv:2406.10292v1 公告类型：新
摘要：全球每年在药物发现和开发方面的成本超过 2000 亿美元。药物发现和开发的主要结果是临床试验的结果，这直接影响新药候选物的监管批准，并最终影响患者的治疗结果。尽管临床试验结果数据意义重大，但大规模、高质量的临床试验结果数据并不容易向公众提供。假设提供了一个大型临床试验结果数据集；机器学习研究人员可以使用过去的试验和结果标签开发准确的预测模型，这可以帮助确定治疗方案的优先次序和优化治疗方案，最终使患者受益。本文介绍了临床试验结果 (CTO) 数据集，这是最大的试验结果数据集，约有 479K 个临床试验，它汇总了来自多个弱监督标签来源的结果，最大限度地减少了来自各个来源的噪音，并且无需人工注释。这些来源包括大型语言模型 (LLM) 对试验相关文档的决策、新闻标题情绪、试验赞助商的股票价格、跨阶段试验联系以及其他信号（如患者退出率和不良事件）。CTO 的标签与监督 TOP 数据集测试拆分中的监督临床试验结果标签表现出前所未有的一致性，F1 为 91。]]></description>
      <guid>https://arxiv.org/abs/2406.10292</guid>
      <pubDate>Tue, 18 Jun 2024 06:27:30 GMT</pubDate>
    </item>
    <item>
      <title>桥牌叫牌 AI 的简单、可靠且可重复的基线</title>
      <link>https://arxiv.org/abs/2406.10306</link>
      <description><![CDATA[arXiv:2406.10306v1 公告类型：新
摘要：合约桥牌是一种以不完全信息和多智能体动态为特征的合作游戏，它带来了重大挑战，是人工智能 (AI) 研究的重要基准。要在这个领域取得成功，智能体必须有效地与合作伙伴合作。这项研究表明，现有方法的适当组合可以在桥牌竞标中与 WBridge5 表现出色，WBridge5 是桥牌竞标系统的领先基准，也是多次获得世界计算机桥牌锦标赛冠军的选手。我们的方法非常简单，但它的表现优于该领域目前最先进的方法。此外，我们已将我们的代码和模型作为开源软件公开提供。这一举措为未来的桥牌 AI 研究提供了坚实的起点基础，促进了该领域新战略和新进步的开发和验证。]]></description>
      <guid>https://arxiv.org/abs/2406.10306</guid>
      <pubDate>Tue, 18 Jun 2024 06:27:30 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士在商业领域的优势</title>
      <link>https://arxiv.org/abs/2406.10249</link>
      <description><![CDATA[arXiv:2406.10249v1 公告类型：新
摘要：大型语言模型 (LLM) 通过利用大量在线文本在语言理解和生成任务中取得了显著的表现。与传统模型不同，LLM 可以通过快速工程适应新领域，而无需重新训练，使其适用于各种业务功能，例如战略规划、项目实施和数据驱动的决策。然而，它们在偏见、语境理解和对提示的敏感性方面的局限性引发了人们对它们是否适合实际应用的担忧。本文彻底研究了 LLM 对业务流程的有用性和准备情况。通过使用真实数据对四个可访问的 LLM 进行的实验来评估 LLM 的局限性和能力。这些发现对于寻求利用生成式人工智能的组织具有重要意义，并为未来的研究方向提供了宝贵的见解。据我们所知，这是首次对应用于核心业务运营和挑战的 LLM 进行量化研究。]]></description>
      <guid>https://arxiv.org/abs/2406.10249</guid>
      <pubDate>Tue, 18 Jun 2024 06:27:29 GMT</pubDate>
    </item>
    <item>
      <title>使用自然语言处理自动评分数学归纳证明</title>
      <link>https://arxiv.org/abs/2406.10268</link>
      <description><![CDATA[arXiv:2406.10268v1 公告类型：新
摘要：在数学证明教育中，仍然需要干预措施来帮助学生学习编写数学证明。研究表明，及时反馈对学生学习新技能非常有帮助。虽然多年来自然语言处理模型一直难以在与数学文本相关的任务上表现出色，但自然语言处理的最新发展为完成向学生提供数学证明即时反馈的任务创造了机会。在本文中，我们提出了一套能够利用现有的大型语言模型和其他机器学习技术自动评分自由形式数学证明的训练方法和模型。这些模型是使用从四个不同的归纳证明问题中收集的证明数据进行训练的。我们使用四种不同的稳健大型语言模型来比较它们的性能，并且都在不同程度上实现了令人满意的性能。此外，我们招募了人类评分员对与训练数据相同的证明进行评分，发现最佳评分模型也比大多数人类评分员更准确。随着这些评分模型的发展，我们创建并部署了一个自动评分器，用于归纳证明问题，并对学生进行了用户研究。研究结果表明，学生能够利用自动评分器的反馈显著改进他们的证明，但学生对人工智能自动评分器的信任程度仍然不如对人类评分者的信任程度。未来的工作可以改进自动评分器的反馈，并找出帮助学生信任人工智能自动评分器的方法。]]></description>
      <guid>https://arxiv.org/abs/2406.10268</guid>
      <pubDate>Tue, 18 Jun 2024 06:27:29 GMT</pubDate>
    </item>
    </channel>
</rss>