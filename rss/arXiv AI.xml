<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.AI 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.AI 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Wed, 12 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>立场：情景记忆是长期法学硕士代理所缺少的一块拼图</title>
      <link>https://arxiv.org/abs/2502.06975</link>
      <description><![CDATA[arXiv:2502.06975v1 公告类型：新
摘要：随着大型语言模型 (LLM) 从文本完成工具演变为在动态环境中运行的成熟代理，它们必须应对不断学习和保留长期知识的挑战。许多生物系统使用情景记忆来解决这些挑战，情景记忆支持对特定于实例的上下文进行单次学习。受此启发，我们为 LLM 代理提出了一个情景记忆框架，该框架围绕情景记忆的五个关键属性为中心，这些属性是自适应和上下文敏感行为的基础。由于各种研究工作已经部分涵盖了这些属性，本立场文件认为，现在是明确、综合地关注情景记忆以催化长期代理发展的最佳时机。为此，我们概述了一个路线图，将几个研究方向统一在支持情景记忆的所有五个属性的目标下，以实现更高效的长期 LLM 代理。]]></description>
      <guid>https://arxiv.org/abs/2502.06975</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自主深度代理</title>
      <link>https://arxiv.org/abs/2502.07056</link>
      <description><![CDATA[arXiv:2502.07056v1 公告类型：新
摘要：本技术简介介绍了 Deep Agent，这是一种先进的自主 AI 系统，旨在通过新颖的分层任务管理架构管理复杂的多阶段任务。该系统的基础建立在我们的分层任务 DAG (HTDAG) 框架之上，该框架将高级目标动态分解为可管理的子任务，同时严格保持依赖关系和执行一致性。Deep Agent 通过三项关键创新超越了传统的代理系统：首先，它实现了递归的两阶段规划器-执行器架构，可以随着情况的变化不断改进和调整任务。其次，它具有自主 API 和工具创建 (AATC) 系统，可自动从 UI 交互中生成可重用的组件，从而大大降低了类似任务的运营成本。第三，它结合了 Prompt Tweaking Engine 和自主 Prompt Feedback Learning 组件，可针对特定场景优化大型语言模型提示，从而提高推理准确性和操作稳定性。这些组件集成在一起，形成一个服务基础架构，用于管理用户上下文、处理复杂的任务依赖关系以及协调端到端代理工作流执行。通过这种复杂的架构，Deep Agent 在自治 AI 系统中建立了一种新范式，展示了独立处理复杂、多步骤任务的强大能力，同时通过持续的自我优化保持一致的效率和可靠性。]]></description>
      <guid>https://arxiv.org/abs/2502.07056</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>与 LLM 代理进行交互式数据协调</title>
      <link>https://arxiv.org/abs/2502.07132</link>
      <description><![CDATA[arXiv:2502.07132v1 公告类型：新
摘要：数据协调是一项基本任务，需要整合来自不同来源的数据集。尽管该领域进行了多年的研究，但由于模式不匹配、术语不同以及数据收集方法的差异，它仍然是一项耗时且具有挑战性的任务。本文介绍了代理数据协调的案例，它既可以帮助专家协调数据，又可以简化流程。我们介绍了 Harmonia，这是一个结合了基于 LLM 的推理、交互式用户界面和数据协调原语库的系统，用于自动合成数据协调管道。我们在临床数据协调场景中演示了 Harmonia，它有助于以交互方式创建可重用的管道，将数据集映射到标准格式。最后，我们讨论了挑战和未解决的问题，并提出了推进我们愿景的研究方向。]]></description>
      <guid>https://arxiv.org/abs/2502.07132</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>了解法学硕士的流体智力缺陷：ARC 任务分析</title>
      <link>https://arxiv.org/abs/2502.07190</link>
      <description><![CDATA[arXiv:2502.07190v1 公告类型：新
摘要：虽然 LLM 在各种 NLP 任务上表现出色，但值得注意的是，这些任务中的大多数依赖于利用 LLM 参数中编码的大量知识，而不是在没有先验知识的情况下解决新问题。在认知研究中，后一种能力被称为流体智力，这被认为是评估人类智力的关键。最近对流体智力评估的研究强调了 LLM 能力的重大缺陷。在本文中，我们以最具代表性的 ARC 任务为例，分析了 LLM 在通过受控实验展示流体智力方面面临的挑战。我们的研究揭示了现有 LLM 的三个主要局限性：技能组合能力有限、不熟悉抽象输入格式以及从左到右解码的内在缺陷。我们的数据和代码可以在 https://wujunjie1998.github.io/araoc-benchmark.github.io/ 中找到。]]></description>
      <guid>https://arxiv.org/abs/2502.07190</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM 推理的推理时间计算技巧</title>
      <link>https://arxiv.org/abs/2502.07191</link>
      <description><![CDATA[arXiv:2502.07191v1 公告类型：新
摘要：随着大型语言模型 (LLM) 的进步，解决复杂的推理任务越来越受到关注。推理时间计算方法（例如 Best-of-N、集束搜索等）特别有价值，因为它们可以在不修改模型参数或需要额外训练的情况下提高推理性能。然而，这些技术面临着实施挑战，大多数现有方法仍处于概念验证阶段，由于其计算复杂性和不同任务之间的不同有效性，实际采用有限。在本文中，我们研究并评估了不同复杂度的推理任务中的各种推理时间计算策略。由于大多数当前方法依赖于提议者-验证者管道，该管道首先生成候选解决方案（例如，推理解决方案），然后根据奖励信号（例如，RLHF 奖励、过程奖励）选择最佳解决方案，因此我们的研究重点是优化候选解决方案生成（例如，指导提示、温度和 top-p 等超参数）和奖励机制（例如，自我评估、奖励类型）。通过对各种规模的各种模型（例如，Llama、Qwen 和 Mistral 系列）进行大量实验（超过 20,000 个 A100-80G GPU 小时，超过 1,000 次实验），我们的消融研究表明，以前被忽视的策略可以显着提高性能（例如，调整温度可以将推理任务性能提高多达 5%）。此外，我们通过系统地评估八个推理任务中的六种代表性方法，为推理时间计算建立了标准化基准。这些发现为未来的研究奠定了更坚实的基础。代码可以在 https://github.com/usail-hkust/benchmark_inference_time_computation_LL 上找到。]]></description>
      <guid>https://arxiv.org/abs/2502.07191</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>系统 2 规划的蒙特卡洛树扩散</title>
      <link>https://arxiv.org/abs/2502.07202</link>
      <description><![CDATA[arXiv:2502.07202v1 公告类型：新
摘要：扩散模型最近已成为规划的强大工具。然而，与蒙特卡洛树搜索 (MCTS) 不同 - 其性能会随着额外的测试时间计算 (TTC) 而自然提高，而基于扩散的标准规划器仅为 TTC 可扩展性提供有限的途径。在本文中，我们介绍了蒙特卡洛树扩散 (MCTD)，这是一种新颖的框架，它将扩散模型的生成强度与 MCTS 的自适应搜索功能相结合。我们的方法将去噪重新概念化为一个树结构过程，允许对部分去噪的计划进行迭代评估、修剪和细化。通过有选择地扩展有希望的轨迹，同时保留重新访问和改进次优分支的灵活性，MCTD 实现了 MCTS 的好处，例如在扩散框架内控制探索-利用权衡。在具有挑战性的长期任务上的经验结果表明，MCTD 的表现优于扩散基线，随着 TTC 的增加可以产生更高质量的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2502.07202</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多即是少：理解法学硕士中的思路链长度</title>
      <link>https://arxiv.org/abs/2502.07266</link>
      <description><![CDATA[arXiv:2502.07266v1 公告类型：新
摘要：思路链 (CoT) 推理通过将复杂任务分解为更小、更易于管理的子任务来增强大型语言模型 (LLM) 的多步骤推理能力。研究人员一直在探索引导模型生成更复杂的 CoT 过程以提高 LLM 推理能力的方法，例如长 CoT 和测试时间缩放定律。然而，对于大多数模型和任务，增加 CoT 长度是否会持续提高推理准确性？在本文中，我们观察到一种微妙的关系：随着推理步骤数量的增加，性能最初会提高，但最终会下降。为了理解这种现象，我们提供了一个证据，即较长的推理过程越来越容易受到噪声的影响。我们从理论上证明了最佳 CoT 长度的存在，并根据模型能力和任务难度推导出该最佳长度的缩放定律。受我们理论的启发，我们在合成数据集和现实世界数据集上进行了实验，并提出了长度过滤投票来缓解过长或过短的 CoT 的影响。我们的研究结果强调了校准 CoT 长度以适应模型能力和任务需求的迫切需要，为优化 LLM 中的多步推理提供了一个原则框架。]]></description>
      <guid>https://arxiv.org/abs/2502.07266</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>粗集理论：粗俗伦理的数学基础</title>
      <link>https://arxiv.org/abs/2502.07347</link>
      <description><![CDATA[arXiv:2502.07347v1 公告类型：新
摘要：在道德决策中，个人的评估通常基于广义评估，而不是精确的个人表现。这个被称为粗略伦理 (CE) 的概念主要在自然语言中讨论，没有正式的数学基础。本文引入了粗集理论 (CST) 来建立 CE 的数学框架。我们使用完全有序集定义粗集，并提出描述元素及其分组之间层次关系的公理。此外，我们引入了粗粒度集，它根据预定义的标准将底层集合划分为等价类。我们通过定义粗映射来扩展这个框架，将详细的个人数据转换为更粗的表示，同时保持基本的结构属性。为了衡量信息丢失，我们采用 Kullback-Leibler (KL) 散度，展示不同的粗分区如何影响信息的保存。我们通过理论公式和实证分析来说明 CST 如何应用于现实世界的评分系统。这项研究为 CE 提供了严谨的基础，使人们能够更系统地探索公平性、可解释性和决策权衡。]]></description>
      <guid>https://arxiv.org/abs/2502.07347</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>KABB：用于多智能体系统中动态专家协调的知识感知贝叶斯赌博机</title>
      <link>https://arxiv.org/abs/2502.07350</link>
      <description><![CDATA[arXiv:2502.07350v1 公告类型：新
摘要：由于扩展大型语言模型面临高昂的成本，多智能体系统成为一种有前途的替代方案，尽管受到静态知识假设和协调效率低下的挑战。我们引入了知识感知贝叶斯老虎机 (KABB)，这是一种通过语义理解和动态适应增强多智能体系统协调的新框架。该框架具有三个关键创新：用于深度语义理解的三维知识距离模型、用于持续专家优化的双重适应机制以及用于有效专家选择的知识感知汤普森采样策略。广泛的评估表明，KABB 实现了最佳的性价比平衡，在多智能体协调中保持高性能的同时保持相对较低的计算需求。]]></description>
      <guid>https://arxiv.org/abs/2502.07350</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士可以轻松地从演示结构中学习推理，而不是内容，这才是最重要的！</title>
      <link>https://arxiv.org/abs/2502.07374</link>
      <description><![CDATA[arXiv:2502.07374v1 公告类型：新
摘要：大型推理模型 (LRM) 通过遵循包含反思、回溯和自我验证的长链思维 (Long CoT) 来解决复杂的推理问题。然而，引出长 CoT 的训练技术和数据要求仍然不太清楚。在这项工作中，我们发现大型语言模型 (LLM) 可以通过数据高效的监督微调 (SFT) 和参数高效的低秩自适应 (LoRA) 有效地学习长 CoT 推理。仅使用 17k 个长 CoT 训练样本，Qwen2.5-32B-Instruct 模型就在广泛的数学和编码基准测试中取得了显著的进步，包括 AIME 2024 上的 56.7% (+40.0%) 和 LiveCodeBench 上的 57.0% (+8.1%)，与专有 o1-preview 模型的 44.6% 和 59.1% 的得分相媲美。更重要的是，我们发现长 CoT 的结构对学习过程至关重要，而各个推理步骤的内容影响甚微。影响内容的扰动（例如使用不正确的样本进行训练或删除推理关键字）对性能影响不大。相反，破坏长 CoT 逻辑一致性的结构修改（例如改组或删除推理步骤）会显著降低准确性。例如，使用具有错误答案的长 CoT 样本训练的模型与使用完全正确的样本进行训练相比，准确率仍仅低 3.2%。这些见解加深了我们对如何在 LLM 中引出推理能力的理解，并强调了有效训练下一代推理模型的关键考虑因素。这是我们之前发布的 Sky-T1-32B-Preview 模型的学术论文。代码可在 https://github.com/NovaSky-AI/SkyThought 上找到。]]></description>
      <guid>https://arxiv.org/abs/2502.07374</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过计算内在动机走向能力需求的形式理论</title>
      <link>https://arxiv.org/abs/2502.07423</link>
      <description><![CDATA[arXiv:2502.07423v1 公告类型：新
摘要：计算模型为形式化心理学理论提供了强大的工具，使其在数字环境中既可测试又可应用。然而，它们在心理学动机研究中仍然很少使用。我们专注于“能力需求”，它被假定为自我决定理论 (SDT) 中的一项关键基本人类需求——可以说是研究内在动机 (IM) 最具影响力的心理学框架。能力需求在 SDT 文本中被视为单一结构。然而，最近的研究已经确定了 SDT 中能力的多个定义模糊的方面。我们提出，这些不一致之处可以通过借鉴人工智能领域的计算模型来缓解，特别是强化学习 (RL) 领域的计算模型。通过将上述能力的各个方面（效果、技能使用、任务绩效和能力增长）与现有的 RL 形式主义相结合，我们为更广泛地推进 SDT 和动机心理学中的能力相关理论奠定了基础。这些形式主义揭示了 SDT 未能明确说明的潜在先决条件，展示了计算模型如何改善我们对 IM 的理解。此外，我们的工作可以通过启发新的计算模型来支持理论发展的周期，这些模型形式化了理论的各个方面，然后可以通过实证测试来完善理论。虽然我们的研究奠定了良好的基础，但仍需要对这些模型在人类和机器中进行实证研究，从而吸引跨学科合作。]]></description>
      <guid>https://arxiv.org/abs/2502.07423</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用 LLM 增强型递归推理器和多智能体超级博弈来近似人类战略推理</title>
      <link>https://arxiv.org/abs/2502.07443</link>
      <description><![CDATA[arXiv:2502.07443v1 公告类型：新
摘要：LLM 驱动的多智能体模拟在博弈论和社会模拟中的应用越来越受到关注。虽然大多数实现都试图利用或评估 LLM 智能体推理，但它们通常使用较弱的代理概念和简化的架构来实现。我们实施了一个基于角色的多智能体战略交互框架，该框架针对复杂的递归推理器量身定制，为系统深入开发和评估战略推理提供了手段。我们的游戏环境由负责促进游戏的裁判管理，从配对到移动验证再到环境管理。玩家将最先进的 LLM 融入他们的决策机制中，依赖于基于正式超游戏的分层信念模型。我们使用一次性、2 人选美比赛来评估最新 LLM 的递归推理能力，并与经济学中已建立的基线模型和来自人类实验的数据进行比较。此外，我们为 k 级理论引入了另一种语义推理度量的基础。我们的实验表明，人工智能推理机在接近人类行为和达到最优解方面的表现都优于基线模型。]]></description>
      <guid>https://arxiv.org/abs/2502.07443</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在渐进式论证中引出合理的初始权重</title>
      <link>https://arxiv.org/abs/2502.07452</link>
      <description><![CDATA[arXiv:2502.07452v1 公告类型：新
摘要：许多加权论证框架的语义假设每个论证都与一个初始权重相关联。然而，引出这些初始权重带来了挑战：（1）准确提供特定数值通常很困难，（2）在存在其他论证的情况下，个人经常将初始权重与可接受度混淆。为了解决这些问题，我们提出了一个引出管道，允许人们为每个论证指定可接受度区间。通过使用渐进语义，我们可以在合理时细化这些区间，在不合理时恢复合理性，并最终确定每个论证的可能初始权重。]]></description>
      <guid>https://arxiv.org/abs/2502.07452</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>URECA：语义代码搜索适应转变背后存在两个最小集覆盖问题链</title>
      <link>https://arxiv.org/abs/2502.07494</link>
      <description><![CDATA[arXiv:2502.07494v1 公告类型：新
摘要：自适应是让模型学习从训练分布中偏移的模式。一般来说，这种自适应被表述为最小熵问题。然而，最小熵问题具有固有的局限性——偏移初始化级联现象。我们通过勒贝格积分扩展了最小熵问题和最小集覆盖问题之间的关系。这种扩展揭示了最小熵问题的内部机制忽略了解缠结表示之间的关系，从而导致了偏移初始化级联。通过分析，我们引入了一种新的聚类算法，基于联合查找的递归聚类算法~(URECA)。URECA 是一种有效的聚类算法，用于利用解缠结表示之间的关系。URECA 的更新规则依赖于阈值可更新平稳假设对动态的发布版本平稳假设。这一假设有助于 URECA 基于解缠结表示之间的关系无错误地传输解缠结表示。URECA 还利用模拟技巧有效地对解缠结表示进行聚类。广泛的评估表明，URECA 在对不同类型的移位进行少量适应时实现了一致的性能提升，并在查询移位场景中在 CoSQA 中提升到了最先进的性能。]]></description>
      <guid>https://arxiv.org/abs/2502.07494</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用递归推理缩放来驾驭语言的分形几何</title>
      <link>https://arxiv.org/abs/2502.07503</link>
      <description><![CDATA[arXiv:2502.07503v1 公告类型：新
摘要：语言建模方面的最新研究揭示了两种扩展效应：众所周知的来自增加训练计算的改进，以及鲜为人知的来自应用更复杂或计算密集型推理方法的提升。受最近关于语言分形几何的发现的启发，我们引入了递归推理扩展 ​​(RINS) 作为扩展推理时间的补充插件配方。对于给定的固定模型架构和训练计算预算，RINS 可显着提高语言建模性能。它还可以推广到纯语言任务之外，在多模态系统中带来收益，包括 SigLIP-B/16 的 0-shot ImageNet 准确率提高 +2%。此外，通过推导数据缩放定律，我们表明 RINS 可以改善渐近性能极限和缩放指数。即使与最先进的递归技术（如 Mobile LLM 中的“全部重复”（RAO）策略）相比，这些优势仍然保持不变。最后，随机 RINS 不仅可以进一步提高性能，而且还提供了灵活性，可以选择在测试时放弃增加的推理计算，同时将性能下降降至最低。]]></description>
      <guid>https://arxiv.org/abs/2502.07503</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>NatureLM：解读自然语言，促进科学发现</title>
      <link>https://arxiv.org/abs/2502.07527</link>
      <description><![CDATA[arXiv:2502.07527v1 公告类型：新
摘要：基础模型彻底改变了自然语言处理和人工智能，显著增强了机器理解和生成人类语言的能力。受这些基础模型成功的启发，研究人员为各个科学领域开发了基础模型，包括小分子、材料、蛋白质、DNA 和 RNA。然而，这些模型通常是孤立训练的，缺乏跨不同科学领域整合的能力。认识到这些领域内的实体都可以表示为序列，它们共同构成了“自然语言”，我们引入了自然语言模型（简称 NatureLM），这是一种基于序列的科学基础模型，专为科学发现而设计。NatureLM 经过来自多个科学领域的数据预训练，提供了一个统一、多功能的模型，可实现各种应用，包括：(i) 使用文本指令生成和优化小分子、蛋白质、RNA 和材料；(ii) 跨领域生成/​​设计，例如蛋白质到分子和蛋白质到 RNA 的生成；以及 (iii) 在 SMILES-to-IUPAC 翻译和 USPTO-50k 上的逆合成等任务中实现最先进的性能。NatureLM 为各种科学任务提供了一种有前途的通用方法，包括药物发现（命中生成/优化、ADMET 优化、合成）、新型材料设计以及治疗性蛋白质或核苷酸的开发。我们开发了不同大小的 NatureLM 模型（10 亿、80 亿和 467 亿个参数），并观察到随着模型大小的增加，性能明显提高。]]></description>
      <guid>https://arxiv.org/abs/2502.07527</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SymGPT：通过将符号执行与大型语言模型相结合来审计智能合约</title>
      <link>https://arxiv.org/abs/2502.07644</link>
      <description><![CDATA[arXiv:2502.07644v1 公告类型：新 
摘要：为了管理在以太坊上运行的智能合约，已经开发了多个以太坊征求意见 (ERC) 标准，每个标准都有一套规则来指导智能合约的行为。违反 ERC 规则可能会导致严重的安全问题和财务损失，这表明验证智能合约是否遵循 ERC 非常重要。当今的验证实践是手动审核每个合约、使用专家开发的程序分析工具或使用大型语言模型 (LLM)，但这些方法都无法有效识别 ERC 规则违规行为。本文介绍了 SymGPT，这是一种将大型语言模型 (LLM) 的自然语言理解与符号执行的正式保证相结合的工具，可自动验证智能合约是否符合 ERC 规则。为了开发 SymGPT，我们对来自三个广泛使用的 ERC 标准的 132 条 ERC 规则进行了实证研究，检查了它们的内容、安全影响和自然语言描述。基于这项研究，我们设计 SymGPT 时首先指示 LLM 将 ERC 规则转换为定义的 EBNF 语法。然​​后，我们从形式化规则中综合约束来表示可能发生违规的情况，并使用符号执行来检测它们。我们的评估表明，SymGPT 在 4,000 份真实合约中识别出 5,783 项 ERC 规则违规行为，其中包括 1,375 项具有明确攻击路径的违规行为，用于窃取金融资产，证明了其有效性。此外，SymGPT 的表现优于六种自动化技术和一种安全专家审计服务，凸显了其优于当前智能合约分析方法的优势。]]></description>
      <guid>https://arxiv.org/abs/2502.07644</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>人类决策容易受到人工智能操纵</title>
      <link>https://arxiv.org/abs/2502.07663</link>
      <description><![CDATA[arXiv:2502.07663v1 公告类型：新
摘要：人工智能 (AI) 系统与日常生活日益交织在一起，帮助用户执行各种任务并提供决策指导。这种整合带来了人工智能驱动的操纵风险，此类系统可能会利用用户的认知偏见和情感弱点来引导他们走向有害结果。通过一项有 233 名参与者参与的随机对照试验，我们研究了人类在财务（例如购买）和情感（例如解决冲突）决策环境中对此类操纵的敏感性。参与者与三个 AI 代理之一进行交互：一个中立代理 (NA)，在没有明确影响的情况下优化用户利益，一个操纵代理 (MA)，旨在秘密影响信念和行为，或一个策略增强操纵代理 (SEMA)，采用明确的心理策略来实现其隐藏目标。通过分析参与者的决策模式和互动后偏好评级的变化，我们发现参与者对人工智能驱动的操纵具有显著的敏感性。特别是在两个决策领域，与操纵代理互动的参与者转向有害选项的比例显著高于 NA 组（财务，MA：62.3%，SEMA：59.6%；情感，MA：42.3%，SEMA：41.5%）（财务，35.8%；情感，12.8%）。值得注意的是，我们的研究结果表明，即使是微妙的操纵目标（MA）在影响人类决策方面也可以像采用明确的心理策略（SEMA）一样有效。通过揭示隐蔽的人工智能影响的潜力，这项研究强调了人机交互中的关键脆弱性，强调需要道德保障和监管框架来确保负责任地部署人工智能技术并保护人类自主权。]]></description>
      <guid>https://arxiv.org/abs/2502.07663</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MAGELLAN：学习进度的元认知预测指导大型目标空间中的自成目的的 LLM 代理</title>
      <link>https://arxiv.org/abs/2502.07709</link>
      <description><![CDATA[arXiv:2502.07709v1 公告类型：新
摘要：开放式学习代理必须在广阔的可能性空间中有效地确定目标的优先级，重点关注那些最大化学习进度 (LP) 的目标。当使用在线 RL 在高维和不断发展的目标空间中训练的 LLM 代理实现这种自成目的的探索时，LP 预测的一个关键挑战是建模自己的能力，这是一种元认知监控形式。传统方法要么需要大量采样，要么依赖于脆弱的专家定义的目标分组。我们引入了 MAGELLAN，这是一个元认知框架，可让 LLM 代理学习在线预测其能力和 LP。通过捕获目标之间的语义关系，MAGELLAN 实现了样本高效的 LP 估计，并通过泛化实现了对不断发展的目标空间的动态适应。在交互式学习环境中，我们表明 MAGELLAN 提高了 LP 预测效率和目标优先级，这是唯一允许代理完全掌握大型且不断发展的目标空间的方法。这些结果证明了如何通过增强 LLM 代理的 LP 预测元认知能力来有效地将课程学习扩展到开放式目标空间。]]></description>
      <guid>https://arxiv.org/abs/2502.07709</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>信息论贝叶斯优化：综述和教程</title>
      <link>https://arxiv.org/abs/2502.06789</link>
      <description><![CDATA[arXiv:2502.06789v1 公告类型：交叉 
摘要：有几种情况需要优化非凸黑盒函数，这些函数噪声大，评估具有未知解析表达式的函数的成本高，因此无法获得其梯度。例如，机器学习模型的超参数调整问题。贝叶斯优化是一类具有最先进性能的方法，可在实际场景中解决此问题。它使用迭代过程，该过程采用要优化的目标函数的概率代理模型（通常是高斯过程），计算黑盒函数的后验预测分布。基于该后验预测分布给出的信息，贝叶斯优化包括计算获取函数，该函数表示对于每个输入空间点，如果该过程的目标是检索全局极值，则在下一次迭代中评估该点的效用。本文是对信息理论获取函数的综述，其性能通常优于其他获取函数。本文还详细描述了信息理论领域的主要概念，以便让读者了解信息理论获取函数为何能在贝叶斯优化中取得优异成绩，以及当它们难以处理时我们如何对其进行近似。我们还介绍了如何将信息理论获取函数适应复杂的优化场景，例如多目标、约束、非短视、多保真度、并行和异步设置，并提供进一步的研究方向。]]></description>
      <guid>https://arxiv.org/abs/2502.06789</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>