<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.AI 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.AI 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Tue, 07 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>不同意和承诺：基于争论的同意程度</title>
      <link>https://arxiv.org/abs/2501.01992</link>
      <description><![CDATA[arXiv:2501.01992v1 公告类型：新
摘要：在人类合作决策中，协议通常不是完全的；只要人们有信心，在没有发生重大意外变化的情况下，相关各方在未来很可能会信守承诺，部分程度的一致就足以做出决定并继续前进。在本文中，我们引入了协议场景的概念，允许人工智能自主代理达成此类协议，使用正式的论证模型，特别是抽象论证和基于价值的论证。我们引入了满意度和（最小、平均和中位数）一致性的概念，以及基于价值的论证框架中的价值对这些概念的影响的度量。然后，我们分析当协议场景用新信息扩展时，一致性程度会受到怎样的影响，以阐明动态场景中部分协议的可靠性。作为基于论证的推理软件库的一部分，提供了引入的概念的实现。]]></description>
      <guid>https://arxiv.org/abs/2501.01992</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>表格作为思维：探索法学硕士推理中的结构化思维</title>
      <link>https://arxiv.org/abs/2501.02152</link>
      <description><![CDATA[arXiv:2501.02152v1 公告类型：新
摘要：大型语言模型的推理能力受益于组织其思维过程的方法，例如思维链提示，它采用顺序结构逐步指导推理过程。然而，现有的方法主要侧重于组织思维顺序，而对单个思维步骤的结构探索不足。为了解决这一差距，我们提出了“思维表”，这是一个受人类思维认知神经科学理论启发的框架。思维表在表格模式中组织推理，其中行代表连续的思维步骤，列捕获关键约束和上下文信息以增强推理。推理过程迭代填充表格，直到自我验证确保完整性和正确性。我们的实验表明，与非结构化思维基线相比，“思维表”在规划任务中表现出色，并显示出在提高 LLM 数学推理性能方面的巨大潜力。这项工作对改进法学硕士 (LLM) 中的思维表征进行了新颖的探索，为推理和人工智能认知的进步铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2501.02152</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CORD：通过角色多样性实现普遍合作</title>
      <link>https://arxiv.org/abs/2501.02221</link>
      <description><![CDATA[arXiv:2501.02221v1 公告类型：新
摘要：合作式多智能体强化学习 (MARL) 旨在开发能够有效协作的智能体。然而，大多数合作式 MARL 方法过度拟合训练智能体，使得学习到的策略不能很好地推广到看不见的合作者，这是实际部署的一个关键问题。一些方法试图解决泛化问题，但需要新队友的先验知识或预定义策略，从而限制了实际应用。为此，我们提出了一种分层 MARL 方法，通过角色多样性实现可推广的合作，即 CORD。CORD 的高级控制器通过最大化有约束的角色熵来将角色分配给低级智能体。我们表明，这个受约束的目标可以分解为角色中的因果影响，从而实现合理的角色分配，以及角色异质性，从而产生连贯、非冗余的角色集群。在各种合作多智能体任务上进行评估后，CORD 取得了比基线更好的表现，尤其是在泛化测试中。消融研究进一步证明了约束目标在泛化合作中的有效性。]]></description>
      <guid>https://arxiv.org/abs/2501.02221</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用人工智能代理提高工作场所生产力和幸福感</title>
      <link>https://arxiv.org/abs/2501.02368</link>
      <description><![CDATA[arXiv:2501.02368v1 公告类型：新
摘要：本文讨论了使用人工智能 (AI) 来提高工作场所生产力和员工福祉。通过将机器学习 (ML) 技术与神经生物学数据相结合，所提出的方法通过价值对齐模型和分层强化学习 (HRL) 确保与人类道德标准保持一致，以实现自主任务管理。该系统利用员工的生物特征反馈来生成个性化的健康提示，营造鼓励身体活动的支持性工作环境。此外，我们还探索了分散的多智能体系统，以改进协作和决策框架，从而提高透明度。讨论了将 ML 技术与 AI 实现结合使用的各种方法。这些创新旨在共同创造一个更高效、更注重健康的工作场所。这些成果有助于人力资源管理和组织为员工推出更合理的职业发展流程并促进组织转型。]]></description>
      <guid>https://arxiv.org/abs/2501.02368</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLMPC：大型语言模型预测控制</title>
      <link>https://arxiv.org/abs/2501.02486</link>
      <description><![CDATA[arXiv:2501.02486v1 公告类型：新
摘要：大型语言模型 (LLM) 提示技术的最新进展提高了它们的推理、规划和行动能力。本文通过模型预测控制 (MPC) 的视角研究了这些提示技术。我们表明，当使用规划提示时，LLM 充当隐式规划成本函数最小化器。在我们的框架下，我们证明，通过结合实际规划成本函数和评估器，可以进一步提高 LLM 规划性能。]]></description>
      <guid>https://arxiv.org/abs/2501.02486</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>测试时间计算：从系统 1 思维到系统 2 思维</title>
      <link>https://arxiv.org/abs/2501.02497</link>
      <description><![CDATA[arXiv:2501.02497v1 Announce Type: new 
摘要：o1模型在复杂推理中的出色表现表明测试时计算扩展可以进一步释放模型的潜力，实现强大的系统2思维。然而，对测试时计算扩展的全面调查仍然不足。我们将测试时计算的概念追溯到系统1模型。在系统1模型中，测试时计算通过参数更新、输入修改、表示编辑和输出校准来解决分布偏移并提高鲁棒性和泛化能力。在系统2模型中，它通过重复采样、自我修正和树搜索增强了模型解决复杂问题的推理能力。我们根据系统1到系统2思维的趋势组织了这次调查，强调了测试时计算在从系统1模型到弱系统2模型，再到强系统2模型的转变中的关键作用。我们还指出了一些可能的未来方向。]]></description>
      <guid>https://arxiv.org/abs/2501.02497</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>KG-CF：大型语言模型指导下通过上下文过滤实现知识图谱补全</title>
      <link>https://arxiv.org/abs/2501.02711</link>
      <description><![CDATA[arXiv:2501.02711v1 公告类型：新
摘要：大型语言模型 (LLM) 在各种任务中都表现出色，包括知识图谱补全 (KGC)。然而，目前的研究大多将 LLM 应用于分类任务，例如识别缺失三元组，而不是基于排名的任务，其中模型根据合理性对候选实体进行排名。这种关注限制了 LLM 在 KGC 中的实际使用，因为现实世界的应用程序优先考虑高度合理的三元组。此外，虽然图路径可以帮助推断缺失三元组的存在并提高补全准确性，但它们通常包含冗余信息。为了解决这些问题，我们提出了 KG-CF，这是一个专为基于排名的 KGC 任务量身定制的框架。KG-CF 利用 LLM 的推理能力过滤掉不相关的上下文，在现实世界的数据集上取得优异的结果。代码和数据集可在 \url{https://anonymous.4open.science/r/KG-CF} 获得。]]></description>
      <guid>https://arxiv.org/abs/2501.02711</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>创意产业中的人工智能：2025 年之前的进展</title>
      <link>https://arxiv.org/abs/2501.02725</link>
      <description><![CDATA[arXiv:2501.02725v1 公告类型：新
摘要：人工智能 (AI) 的快速发展，尤其是生成式人工智能和大型语言模型 (LLM) 的快速发展，通过实现创新内容创作、增强工作流程和使创意工具的访问民主化，对创意产业产生了深远影响。本文探讨了自 2022 年我们上次审查以来的重大技术转变，重点介绍了这些发展如何扩大了创意机会和效率。这些技术进步增强了文本到图像、文本到视频和多模式生成技术的功能。特别是，LLM 的关键突破为对话式人工智能建立了新的基准，而图像生成器的进步彻底改变了内容创作。我们还讨论了将人工智能集成到后期制作工作流程中，这大大加速和改进了传统流程。尽管有这些创新，但由于创意内容对通信流量的需求，挑战仍然存在，尤其是对于媒体行业而言。因此，我们在本文中包括数据压缩和质量评估。此外，我们强调了能够处理多项创意任务的统一 AI 框架的趋势，并强调了人类监督对于减轻 AI 产生的不准确性的重要性。最后，我们探讨了 AI 在创意领域的未来潜力，强调需要应对新兴挑战，以最大限度地发挥其优势，同时解决相关风险。]]></description>
      <guid>https://arxiv.org/abs/2501.02725</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>有限通信范围约束下动态引导的多智能体路径寻找</title>
      <link>https://arxiv.org/abs/2501.02770</link>
      <description><![CDATA[arXiv:2501.02770v1 公告类型：新
摘要：本文提出了一种新颖的框架来处理有限通信范围约束下的多智能体路径查找问题，其中所有智能体都必须具有与团队其他成员相连的通信通道。许多现有的多智能体路径查找方法（例如，领导者-追随者排）通过按固定顺序一次规划一个智能体来克服该领域规划的计算挑战。然而，固定的领导者-追随者方法可能会在规划过程中陷入困境，从而限制了它们在密集混乱环境中的实际效用。为了克服这一限制，我们开发了动态领先的多智能体路径查找，当无法取得进展时，可以在路径规划过程中动态重新选择领先智能体。实验表明我们的框架是有效的，它可以在五种基线经常失败的环境类型中处理多达 25 个智能体，成功率超过 90%。]]></description>
      <guid>https://arxiv.org/abs/2501.02770</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过匹配实现公平</title>
      <link>https://arxiv.org/abs/2501.02793</link>
      <description><![CDATA[arXiv:2501.02793v1 公告类型：新 
摘要：群体公平要求具有给定敏感属性的不同受保护群体总体上获得平等的结果。通常，群体公平的水平由不同受保护群体的预测之间的统计差距来衡量。在这项研究中，我们揭示了现有群体公平度量的一个隐式属性，这为了解群体公平模型的行为方式提供了参考。然后，我们基于这个隐式属性开发一个新的群体公平约束来学习群体公平模型。为此，我们首先介绍一个值得注意的理论观察：每个群体公平模型在每个受保护群体的输入空间之间都有一个隐式对应的传输图。基于这一观察，我们引入了一种新的群体公平度量，称为匹配人口统计奇偶校验 (MDP)，它量化了两个个体（来自不同的受保护群体）通过给定传输图匹配的预测之间的平均差距。然后，我们证明任何传输图都可以在 MDP 中用于学习群体公平模型，并开发一种称为“通过匹配实现公平”（FTM）的新算法，该算法使用 MDP 约束和用户指定的传输图来学习群体公平模型。我们根据最优传输理论，特别提出了两种适用于 MDP 的传输图类型，并讨论了它们的优势。实验表明，通过选择相应的传输图，FTM 可以成功地训练具有某些理想属性的群体公平模型。]]></description>
      <guid>https://arxiv.org/abs/2501.02793</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CALM：大型语言模型的好奇心驱动审计</title>
      <link>https://arxiv.org/abs/2501.02997</link>
      <description><![CDATA[arXiv:2501.02997v1 公告类型：新
摘要：审计大型语言模型 (LLM) 是一项至关重要且具有挑战性的任务。在本研究中，我们专注于审计黑盒 LLM，而无需访问其参数，而只能访问所提供的服务。我们将这种类型的审计视为黑盒优化问题，其目标是自动发现表现出非法、不道德或不安全行为的目标 LLM 的输入输出对。例如，我们可能会寻找目标 LLM 以有毒输出响应的无毒输入，或者从包含政治敏感个体的目标 LLM 中诱发幻觉反应的输入。由于可行点的稀缺性、提示空间的离散性质以及巨大的搜索空间，这种黑盒优化具有挑战性。为了应对这些挑战，我们提出了好奇心驱动的大型语言模型审计 (CALM)，它使用内在激励的强化学习来微调 LLM 作为审计代理，以发现目标 LLM 的潜在有害和有偏见的输入输出对。CALM 成功识别了涉及名人的贬义完成，并在黑盒设置下发现了引出特定名称的输入。这项工作为审计黑盒 LLM 提供了一个有希望的方向。我们的代码可在 https://github.com/x-zheng16/CALM.git 上找到。]]></description>
      <guid>https://arxiv.org/abs/2501.02997</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>分析多模态 LLM 的微调表征偏移控制对齐</title>
      <link>https://arxiv.org/abs/2501.03012</link>
      <description><![CDATA[arXiv:2501.03012v1 公告类型：新
摘要：多模态 LLM 在理解多模态输入方面已经达到了非凡的熟练程度，推动了广泛的研究以开发越来越强大的模型。然而，人们对理解和解释这些模型的底层机制的关注却少得多。大多数现有的可解释性研究仅在其最终状态下检查这些模型，而忽略了训练期间发生的动态表征转变。在这项工作中，我们系统地分析了隐藏状态表示的演变，以揭示微调如何改变模型的内部结构以专门用于新的多模态任务。使用基于概念的方法，我们将隐藏状态映射到可解释的视觉和文本概念，使我们能够在训练过程中跟踪跨模态编码概念的变化。我们还演示了使用移位向量来捕获这些概念变化。这些移位向量使我们能够通过移动原始模型中的概念来恢复微调的概念。最后，我们探讨了我们的研究结果对模型控制的实际影响，表明我们可以在不进行任何训练的情况下调整多模态 LLM 的行为，例如修改答案类型、标题样式或使模型偏向特定响应。我们的工作揭示了多模态表示如何通过微调演变，并为解释多模态任务中的模型自适应提供了新的视角。该项目的代码可在 https://github.com/mshukor/xl-vlms 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2501.03012</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>经过安全验证且可解释的深度强化学习策略的共激活图分析</title>
      <link>https://arxiv.org/abs/2501.03142</link>
      <description><![CDATA[arXiv:2501.03142v1 公告类型：新
摘要：深度强化学习 (RL) 策略可以展示不安全的行为，并且很难解释。为了应对这些挑战，我们将 RL 策略模型检查（一种确定 RL 策略是否表现出不安全行为的技术）与共激活图分析（一种通过分析神经元激活模式来映射神经网络内部工作原理的方法）相结合，以深入了解安全 RL 策略的顺序决策。这种组合让我们能够解释 RL 策略的内部工作原理，以实现安全决策。我们在各种实验中证明了它的适用性。]]></description>
      <guid>https://arxiv.org/abs/2501.03142</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通用人工智能 (AGI) 的大型语言模型：基础原理和方法的概述</title>
      <link>https://arxiv.org/abs/2501.03151</link>
      <description><![CDATA[arXiv:2501.03151v1 公告类型：新
摘要：基于大规模预训练基础模型 (PFM)（例如视觉语言模型、大型语言模型 (LLM)、扩散模型和视觉语言动作 (VLA) 模型）的生成人工智能 (AI) 系统已证明能够解决各种领域和环境中复杂且真正非平凡的 AI 问题。多模态大型语言模型 (MLLM) 尤其可以从庞大而多样的数据源中学习，从而能够对世界进行丰富而细致的表示，从而提供广泛的功能，包括推理能力、参与有意义的对话；与人类和其他代理合作共同解决复杂问题；并了解人类的社交和情感方面。尽管取得了这一令人印象深刻的成就，但在大型数据集上训练的最先进的 LLM 的认知能力仍然肤浅而脆弱。因此，通用 LLM 的通才能力受到严重限制。为了让 LLM 达到人类水平的通用智能，需要解决许多基础问题——具体化、符号基础、因果关系和记忆。这些概念更符合人类认知，并为 LLM 提供了固有的类似人类的认知属性，支持实现物理上合理、语义上有意义、灵活且更通用的知识和智能。在本文中，我们讨论了上述基础问题，并调查了在 LLM 中实现这些概念的最新方法。具体来说，我们讨论了如何利用具体化、符号基础、因果关系和记忆的原理以有机的方式实现通用人工智能 (AGI)。]]></description>
      <guid>https://arxiv.org/abs/2501.03151</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>回合制多智能体强化学习模型检查</title>
      <link>https://arxiv.org/abs/2501.03187</link>
      <description><![CDATA[arXiv:2501.03187v1 公告类型：新
摘要：在本文中，我们提出了一种新方法来验证回合制多智能体强化学习 (TMARL) 智能体是否符合随机多人游戏中的复杂要求。我们的方法克服了现有验证方法的局限性，这些方法不足以处理 TMARL 智能体，并且无法扩展到具有多个智能体的大型游戏。我们的方法依赖于 TMARL 和一种称为模型检查的验证技术的紧密集成。我们通过在不同类型环境中的实验证明了我们技术的有效性和可扩展性。我们的实验表明，我们的方法适合验证 TMARL 智能体，并且比简单的单片模型检查具有更好的扩展性。]]></description>
      <guid>https://arxiv.org/abs/2501.03187</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>STEAM-EEG：使用马尔可夫转移场和注意力 CNN 进行时空脑电图分析</title>
      <link>https://arxiv.org/abs/2501.01959</link>
      <description><![CDATA[arXiv:2501.01959v1 公告类型：交叉 
摘要：脑电图 (EEG) 信号在生物医学研究和临床应用中起着关键作用，包括癫痫诊断、睡眠障碍分析和脑机接口。然而，有效分析和解释这些复杂信号往往带来重大挑战。本文提出了一种将计算机图形技术与生物信号模式识别相结合的新方法，具体来说是使用马尔可夫转移场 (MTF) 进行 EEG 时间序列成像。所提出的框架 (STEAM-EEG) 利用 MTF 的功能来捕获 EEG 信号的时空动态，将其转换为视觉信息丰富的图像。然后使用最先进的计算机图形技术对这些图像进行渲染、可视化和建模，从而促进增强的数据探索、模式识别和决策。代码可以从 GitHub 访问。]]></description>
      <guid>https://arxiv.org/abs/2501.01959</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GAF-FusionNet：通过 Gramian 角场和分割注意力进行多模态心电图分析</title>
      <link>https://arxiv.org/abs/2501.01960</link>
      <description><![CDATA[arXiv:2501.01960v1 公告类型：交叉 
摘要：心电图 (ECG) 分析在诊断心血管疾病中起着至关重要的作用，但准确解释这些复杂信号仍然具有挑战性。本文介绍了一种用于 ECG 分类的新型多模态框架 (GAF-FusionNet)，该框架将时间序列分析与使用 Gramian Angular Fields (GAF) 的基于图像的表示相结合。我们的方法采用双层跨通道分割注意力模块来自适应地融合时间和空间特征，从而实现互补信息的细微整合。我们在三个不同的 ECG 数据集上评估了 GAF-FusionNet：ECG200、ECG5000 和 MIT-BIH 心律失常数据库。结果表明，与最先进的方法相比，我们的模型有显着的改进，在相应的数据集上实现了 94.5\%、96.9\% 和 99.6\% 的准确率。我们的代码将很快在https://github.com/Cross-Innovation-Lab/GAF-FusionNet.git 上提供。]]></description>
      <guid>https://arxiv.org/abs/2501.01960</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>统计学习并不总是需要知识</title>
      <link>https://arxiv.org/abs/2501.01963</link>
      <description><![CDATA[arXiv:2501.01963v1 公告类型：交叉 
摘要：在本文中，我们研究代理对真或假命题的学习和知识获取 (LKA)。我们使用贝叶斯方法，其中代理接收数据以根据后验分布更新他对命题的信念。LKA 以主动信息的形式表示，数据表示修改代理信念的外部或外生信息。假设数据提供与命题相关的许多特征的详细信息。我们表明，这会导致吉布斯分布后验，相对于先验，该分布的熵最大，条件是数据在特征方面提供的边约束。我们证明，当提取的特征数量太少时，完全学习有时是不可能的，完全的知识获取也是不可能的。我们还区分了初级学习（接收与命题相关的特征的数据）和次级学习（接收有关另一个代理学习的数据）。我们认为，这种二次学习并不代表真正的知识获取。我们的研究结果对统计学习算法有影响，我们声称此类算法并不总是产生真正的知识。该理论通过几个例子进行了说明。]]></description>
      <guid>https://arxiv.org/abs/2501.01963</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>永久投票中不满情绪的最佳界限</title>
      <link>https://arxiv.org/abs/2501.01969</link>
      <description><![CDATA[arXiv:2501.01969v1 公告类型：交叉 
摘要：在永久投票中，多个决策在不同时刻做出。考虑到先前决策的历史，我们可以满足一段时间内的比例等属性。在本文中，我们考虑以下问题：是否存在一种永久的赞成投票方法，可以保证没有选民多次不满意？我们确定了选民行为的充分条件——我们称之为“有界冲突”条件——在此条件下，不满情绪可能出现亚线性增长。我们使用 Kolmogorov 复杂性技术，为有界冲突下不满情绪的增长提供了严格的上限。我们还观察到，二元选择的赞成投票模仿了专家建议预测的机器学习设置。这使我们能够基于专家建议预测的标准技术，提出一种在有界冲突下对不满情绪具有亚线性保证的投票方法。]]></description>
      <guid>https://arxiv.org/abs/2501.01969</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>INFELM：大型文本转图像模型的深入公平性评估</title>
      <link>https://arxiv.org/abs/2501.01973</link>
      <description><![CDATA[arXiv:2501.01973v1 公告类型：交叉 
摘要：大型语言模型 (LLM) 和大型视觉模型 (LVM) 的快速发展推动了多模态 AI 系统的发展，这些系统通过模拟类似人类的认知展示了巨大的工业应用潜力。然而，它们也带来了重大的道德挑战，包括放大有害内容和强化社会偏见。例如，一些工业图像生成模型中的偏见凸显了对稳健公平性评估的迫切需求。大多数现有的评估框架都侧重于模型各个方面的全面性，但它们表现出严重的局限性，包括对内容生成一致性和社会偏见敏感领域的关注不足。更重要的是，它们对像素检测技术的依赖容易导致不准确。
为了解决这些问题，本文提出了 INFELM，这是一种对广泛使用的文本到图像模型的深入公平性评估。我们的主要贡献是：(1) 先进的肤色分类器结合了面部拓扑和精细的皮肤像素表示，可将分类精度提高至少 16.04%；(2) 偏差敏感的内容对齐测量，用于了解社会影响；(3) 适用于不同人口群体的可推广的表征偏差评估；(4) 大量实验，分析六个社会偏差敏感领域的大规模文本到图像模型输出。我们发现研究中的现有模型通常不符合经验公平标准，表征偏差通常比对齐误差更明显。INFELM 为公平性评估建立了强大的基准，支持符合道德和以人为本原则的多模态 AI 系统的开发。]]></description>
      <guid>https://arxiv.org/abs/2501.01973</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>