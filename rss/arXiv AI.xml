<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.ai更新在arxiv.org上</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.ai在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Fri, 21 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>调查LLM-AS-A-A-Gudge中的非转换性</title>
      <link>https://arxiv.org/abs/2502.14074</link>
      <description><![CDATA[ARXIV：2502.14074V1公告类型：新 
摘要：基于大语言模型（LLM）的自动评估方法正在成为评估基于LLM的代理的指导跟随能力的标准工具。在此范式中，最常见的方法与基线模型进行了成对比较，主要取决于及时偏好的假设。但是，该假设的有效性在很大程度上尚未开发。在这项研究中，我们研究了Alpacaeval框架内的非转变性，并分析其对模型排名的影响。我们发现LLM法官表现出非传递偏好，导致对基线模型选择敏感的排名。为了减轻此问题，我们表明圆形锦标赛与Bradley-Terry偏好模型相结合可以产生更可靠的排名。值得注意的是，我们的方法增加了Spearman的相关性和与聊天机器人竞技场的肯德尔相关性（分别为95.0％ - &gt; 96.4％和82.1％ - &gt; 86.3％）。为了满足循环锦标赛的计算成本，我们提出了瑞士迭代式对接（游泳）锦标赛，使用动态匹配策略来捕捉循环锦标赛的好处，同时保持计算效率。]]></description>
      <guid>https://arxiv.org/abs/2502.14074</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>可解释的分布式约束优化问题</title>
      <link>https://arxiv.org/abs/2502.14102</link>
      <description><![CDATA[ARXIV：2502.14102V1公告类型：新 
摘要：分布式约束优化问题（DCOP）公式是建模需要分布求解的合作多代理问题的强大工具。现有方法的核心假设是，DCOP解决方案可以很容易地理解，接受和采用，这可能无法持有，这是关于可解释的AI的大量文献所证明的。在本文中，我们提出了可解释的DCOP（X-DCOP）模型，该模型扩展了DCOP以包括其解决方案和该解决方案的对比度查询。我们正式定义了一些关键属性，即对比解释必须满足它们为X-DCOPS的有效解决方案，以及有关存在这种有效解释的理论结果。为了解决X-DCOPS，我们提出了一个分布式框架以及几种优化和次优型，以找到有效的解释。我们还提供了一项人类用户研究，该研究表明，用户毫不奇怪，更喜欢更短的解释而不是更长的解释。我们的经验评估表明，我们的方法可以扩展到大型问题，而不同的变体为交易较小的运行时间提供了不同的选择。因此，我们的模型和算法贡献通过减少用户了解DCOP解决方案的障碍，从而促进其在更真实的应用程序中的采用来扩展最新技术的状态。]]></description>
      <guid>https://arxiv.org/abs/2502.14102</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>给予人工智能个性导致更像人类的推理</title>
      <link>https://arxiv.org/abs/2502.14155</link>
      <description><![CDATA[ARXIV：2502.14155V1公告类型：新 
摘要：在计算认知建模中，捕获人类判断和决策过程的全部范围，而不仅仅是最佳行为，这是一个重大挑战。这项研究探讨了大型语言模型（LLMS）是否可以通过预测直观的，快速的系统1和故意的，慢的系统2过程来模仿人类推理的广度。我们研究了AI对模仿人口中各种推理行为的潜力，并解决了我们所谓的{\ em全部推理谱系问题}。我们使用自然语言推断（NLI）格式的新颖概括来设计推理任务，以评估LLMS复制人类推理的能力。这些问题是为了引起系统1和System 2响应而制定的。人类的反应是通过人群来收集的，整个分布都是建模的，而不仅仅是大多数答案。我们利用基于个性的提示灵感受到五巨头个性模型的启发，以引起反映特定人格特征，捕捉人类推理多样性的AI响应，并探索个性特征如何影响LLM的输出。结合遗传算法以优化这些提示的加权，该方法与传统的机器学习模型一起进行了测试。结果表明，LLM可以模仿人类反应分布，而诸如Llama和Mistral的开源模型都优于专有GPT模型。基于人格的提示，尤其是使用遗传算法进行优化时，显着增强了LLMS预测人类反应分布的能力，这表明捕获次优的自然主义推理可能需要建模技术，以结合各种推理风格和心理概况。研究得出结论，基于人格的提示与遗传算法相结合，有望在推理中增强AI的\ textit {Human-ness}。]]></description>
      <guid>https://arxiv.org/abs/2502.14155</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>因果均值野外多机构增强学习</title>
      <link>https://arxiv.org/abs/2502.14200</link>
      <description><![CDATA[ARXIV：2502.14200V1公告类型：新 
摘要：可伸缩性仍然是多代理增强学习的挑战，目前正在积极研究中。一个名为平均场增强学习（MFRL）的框架可以通过采用平均场理论将多个代理问题变成两种代理问题来减轻可伸缩性问题。但是，该框架缺乏在非组织环境下识别基本相互作用的能力。因果关系包含相互作用背后的相对不变的机制，尽管环境是非组织的。因此，我们提出了一种称为因果均值场Q学习（CMFQ）的算法，以解决可伸缩性问题。 CMFQ在继承了MFRL的动作状态空间的压缩表示方面，对代理数量的变化越来越强大。首先，我们将MFRL决策过程背后的因果关系建模为结构性因果模型（SCM）。然后，通过介入SCM来量化每种相互作用的基本程度。此外，我们根据其因果效应，设计了代理的行为信息的因果关系，作为代理的行为信息的契约表示。我们在混合竞争性游戏和合作游戏中测试CMFQ。结果表明，我们的方法在包含大量试剂和测试的环境中的两种训练中都具有出色的可伸缩性性能，并在包含更多代理的环境中进行测试。]]></description>
      <guid>https://arxiv.org/abs/2502.14200</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>调查LLM个性对自动决策任务中认知偏差表现的影响</title>
      <link>https://arxiv.org/abs/2502.14219</link>
      <description><![CDATA[ARXIV：2502.14219V1公告类型：新 
摘要：大型语言模型（LLM）越来越多地用于决策中，但它们对认知偏见的敏感性仍然是一个紧迫的挑战。这项研究探讨了人格特质如何影响这些偏见，并评估各种模型体系结构中缓解策略的有效性。我们的发现确定了六个普遍的认知偏见，而沉没的成本和群体归因偏见显示出最小的影响。人格特征在放大或减少偏见中起着至关重要的作用，从而显着影响LLM对偏见技术的反应。值得注意的是，尽职尽责和同意可能会提高缓解偏见策略的功效，这表明表现出这些特征的LLMS更容易受到纠正措施。这些发现涉及以人格驱动的偏见动态的重要性，并强调了对有针对性缓解方法的必要性，以提高AI辅助决策中的公平性和可靠性。]]></description>
      <guid>https://arxiv.org/abs/2502.14219</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Sprig：内部游戏动力学的stackelberg感知增强学习</title>
      <link>https://arxiv.org/abs/2502.14264</link>
      <description><![CDATA[ARXIV：2502.14264V1公告类型：新 
摘要：深入强化学习者通常会面临有效协调感知和决策组成部分的挑战，尤其是在特征相关性不同的具有高维感觉输入的环境中。这项工作介绍了Sprig（带有内部游戏动力学的Stackelberg感知 - 增强学习学习），该框架将单个代理中内部感知 - 政策互动建模为合作Stackelberg游戏。在Sprig中，感知模块充当领导者，在战略上处理原始感觉状态，而策略模块则遵循，根据提取的功能做出决策。 Sprig通过修改后的Bellman操作员提供理论保证，同时保留了现代政策优化的好处。关于Atari Beamrider环境的实验结果表明了Sprig的有效性，通过其功能提取和决策的游戏理论平衡，与标准PPO相比，获得了30％的回报。]]></description>
      <guid>https://arxiv.org/abs/2502.14264</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>流动：实现工作流动代理的合规性和灵活性</title>
      <link>https://arxiv.org/abs/2502.14345</link>
      <description><![CDATA[Arxiv：2502.14345V1公告类型：新 
摘要：将工作流与大语言模型（LLMS）的集成使基于LLM的代理能够执行预定义的过程，从而增强了现实世界应用程序中的自动化。传统的基于规则的方法倾向于限制LLM的固有灵活性，因为它们的预定义的执行路径限制了模型的动作空间，尤其是在遇到意外的工作流程（OOW）查询时。相反，基于及时的方法使LLM可以完全控制流动，从而导致程序合规性的执行减少。为了应对这些挑战，我们介绍了Flowagent，这是一个新型的代理框架，旨在保持合规性和灵活性。我们提出了过程描述语言（PDL），该语言将自然语言的适应性与代码的精确度相结合以制定工作流程。在PDL的基础上，我们开发了一个综合框架，该框架能够有效地管理OOW查询，同时将执行路径保持在一组控制器的监督下。此外，我们提出了一种新的评估方法，以严格评估LLM代理处理OOW方案的能力，超出了在现有基准测试中测试的常规流量合规性。在三个数据集上的实验表明，流动不仅遵守工作流程，而且还可以有效地管理OOW查询，从而突出了其在合规性和灵活性方面的双重强度。该代码可在https://github.com/lightblues/flowagent上找到。]]></description>
      <guid>https://arxiv.org/abs/2502.14345</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>可概括性数学推理的检索授权过程奖励模型</title>
      <link>https://arxiv.org/abs/2502.14361</link>
      <description><![CDATA[arxiv：2502.14361v1公告类型：新 
摘要：尽管大型语言模型（LLM）具有显着高级的数学推理，但已经开发了过程奖励模型（PRM）来评估推理步骤的逻辑有效性。但是，PRM仍在挑战（OOD）挑战方面挣扎。本文确定了关键的OOD问题，包括步骤OOD，是由模型类型和大小的推理模式差异以及问题OOD引起的，这是由于训练数据和现实世界中的问题之间的数据集变化引起的。为了解决这些问题，我们介绍了检索提升的过程奖励模型（检索），这是一个旨在解决这些OOD问题的新型框架。通过利用两阶段的检索增强机制，检索语义上相似的问题和步骤作为热身，增强了PRM评估目标步骤，改善不同模型和问题类型的概括和推理一致性的能力。我们的广泛实验表明，检索PRM在多个现实世界数据集上的表现优于现有基线。我们的开源贡献包括检索增强数据集，用于PRM培训的调谐框架以及检索模型，为PRM性能建立了新的标准。]]></description>
      <guid>https://arxiv.org/abs/2502.14361</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>HPS：人类偏好对齐的硬偏好抽样</title>
      <link>https://arxiv.org/abs/2502.14400</link>
      <description><![CDATA[ARXIV：2502.14400V1公告类型：新 
摘要：将大语言模型（LLM）响应与人类偏好的响应对齐对于构建安全可控制的AI系统至关重要。尽管基于Plackett-luce（PL）和Bradley-Terry（BT）模型的优先优化方法表现出了希望，但它们面临着诸如对有害内容的处理不佳，对分配响应的效率低下的挑战，尤其是PL，高度计算成本。为了解决这些问题，我们提出了硬偏好采样（HPS），这是一个新颖的人类偏好一致性的新型框架。 HPS引入了培训损失，该培训损失优先考虑最优选的响应，同时拒绝所有分配和有害的响应。它强调了“硬”的反应响应 - 这些响应非常类似于首选的响应 - 增强了模型的拒绝能力。通过利用单样本的蒙特卡洛抽样策略，HPS可以减少计算开销，同时保持对齐质量。从理论上讲，HPS比现有PL方法提高了样本效率，并最大程度地提高了首选和分配响应之间的奖励余量，从而确保了更清晰的区别。 HH-RLHF和PKU安全数据集的实验验证了HPS的有效性，实现了可比的BLEU和奖励得分，同时大大提高了奖励利润率，从而减少了有害内容的产生。]]></description>
      <guid>https://arxiv.org/abs/2502.14400</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>叙事驱动的旅行计划：具有进化行程优化的地理脚本生成</title>
      <link>https://arxiv.org/abs/2502.14456</link>
      <description><![CDATA[ARXIV：2502.14456V1公告类型：新 
摘要：为了增强游客的体验和沉浸式，本文提出了一个名为“叙事指导”的叙事驱动的旅行计划框架，该框架为旅行者提供了地理文化的叙事脚本，为他们的旅程提供了小说，扮演角色的经验。在初始阶段，叙事指导构建了一个城市中景点的知识图，然后根据知识图配置世界观，角色设置和博览会。使用此基础，将知识图合并为为每个景点生成一个独立的场景单元。在行程计划阶段，叙事指导将叙事驱动的旅行计划模型为优化问题，利用遗传算法（GA）来完善行程。在评估候选行程之前，为每对相邻景点生成过渡脚本，该景点与场景单元一起形成一个完整的脚本。然后将脚本连贯性，旅行时间和吸引力分数的加权总和用作更新候选解决方案集的健身值。四个城市的实验结果，即中国的南京和扬州，法国的巴黎和德国的柏林，表现出叙事连贯性和文化契合度的显着改善，同时显着减少了旅行时间和访问景点的质量。我们的研究强调，合并外部进化优化的研究可以有效地解决旅行计划中大型语言模型的局限性。]]></description>
      <guid>https://arxiv.org/abs/2502.14456</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多变量AI风险的统计场景建模和外观类似分布</title>
      <link>https://arxiv.org/abs/2502.14491</link>
      <description><![CDATA[Arxiv：2502.14491V1公告类型：新 
摘要：评估AI安全需要统计上严格的方法和风险指标，以了解AI的使用如何影响总风险。但是，许多AI安全文献都集中在孤立的AI模型引起的风险上，缺乏对AI的模块化使用如何影响工作流程组件的风险分布或整体风险指标的风险分布。在缺乏AI的情况下，缺乏统计基础，可以使风险模型的敏感性估计AI的因果贡献。这部分是由于AI影响数据拟合分布的缺乏。在这项工作中，我们通过两种方式解决了这些差距。首先，我们演示了场景建模（以既定的统计技术为基础，例如马尔可夫链，科普拉斯和蒙特卡洛模拟），以整体上的AI进行建模。其次，我们展示了如何使用类似于AI的现象的外观分布来估计AI在没有直接观察数据的情况下影响AI。我们证明了通过对逻辑场景模拟的风险分析，我们的方法实用了累积AI风险的实用性。]]></description>
      <guid>https://arxiv.org/abs/2502.14491</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计划范围：朝向平行的LLM代理计划</title>
      <link>https://arxiv.org/abs/2502.14563</link>
      <description><![CDATA[Arxiv：2502.14563V1公告类型：新 
摘要：大型语言模型（LLMS）在任务计划推理方面表现出了特殊的能力。但是，对于并行时间表，挑战仍然不足。本文介绍了一种新颖的范式，计划范围的范围，其中模型首先将真实的文本任务分解为可执行的子任务并构造抽象任务图。然后，该模型将此任务图理解为输入，并生成并行执行的计划。为了增强复杂，可扩展图的计划能力，我们设计了一条自动化和可控的管道，以生成合成图并提出了两阶段训练方案。实验结果表明，我们的计划范围的方法可显着提高基于API的LLM和可训练的开源LLM的任务性能。通过将复杂的任务标准化为图形，我们的方法自然支持并行执行，并证明全球效率。代码和数据可从https://github.com/zsq259/plan-over-graph获得。]]></description>
      <guid>https://arxiv.org/abs/2502.14563</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>反对经验人类对准的统计案例</title>
      <link>https://arxiv.org/abs/2502.14581</link>
      <description><![CDATA[ARXIV：2502.14581V1公告类型：新 
摘要：经验人类AI对准旨在使AI系统与观察到的人类行为保持一致。虽然贵族的目标是，我们认为经验一致性可以无意中引入统计偏见，以免谨慎。因此，该立场论文主张反对天真的经验一致性，提供规范性的一致性和后验经验对准作为替代方案。我们通过有形的例子（例如语言模型的以人为中心的解码）来证实我们的原则论点。]]></description>
      <guid>https://arxiv.org/abs/2502.14581</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过缩放自我播放来构建可靠的模拟驾驶剂</title>
      <link>https://arxiv.org/abs/2502.14706</link>
      <description><![CDATA[ARXIV：2502.14706V1公告类型：新 
摘要：模拟剂对于设计和测试与人类相互作用的系统至关重要，例如自动驾驶汽车（AV）。这些代理的目的是各种目的，从基准AV性能到压力测试系统的限制，但所有用例都有关键要求：可靠性。仿真代理应按照设计师的意图行事，最大程度地减少可能损害分析信噪比的碰撞的意外动作。作为可靠的模拟代理的基础，我们提出了在人类感知和控制的半现实限制下，将自我播放缩放到Waymo Open Motion数据集的数千个场景。从头开始训练单个GPU，我们的代理商几乎可以在一天之内解决完整的培训。他们有效地概括了看不见的测试场景，在10,000个持有的场景中，以低于0.8％的合并碰撞和越野事件达到了99.8％的进球率。除了分配概括之外，我们的代理商还表现出对分布场景的部分鲁棒性，并且可以在几分钟内进行微调以在这种情况下达到几乎完美的性能。可以在此链接中找到代理行为的演示。我们开源的预培训代理和完整的代码库。可以在\ url {https://sites.google.com/view/reliable-sim-agents}找到代理行为的演示。]]></description>
      <guid>https://arxiv.org/abs/2502.14706</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从知识生成到知识验证：研究Chatgpt的生物医学生成能力</title>
      <link>https://arxiv.org/abs/2502.14714</link>
      <description><![CDATA[ARXIV：2502.14714V1公告类型：新 
摘要：LLM模型的生成能力在加速任务和关注方面提供了与其产生的知识的真实性的机会。为了解决这些问题，我们提出了一种计算方法，该方法系统地评估了生物医学知识的事实准确性，即LLM模型已被促使生成。我们的方法包括两个过程：以疾病为中心的关联的产生以及使用生物医学本体学的语义知识对它们进行验证。我们使用Chatgpt作为选择的LLM模型，我们设计了一组及时的工程过程，以在疾病，药物，症状和基因之间建立联系，以建立评估的基础。实验结果表明，鉴定疾病术语（88％-97％），药物名称（90％-91％）和遗传信息（88％-98％）的精度很高。症状术语识别准确性明显较低（49％-61％），如DOID，CHEBI，症状和GOSOLOGISE验证。对关联的验证揭示了疾病 - 药物和疾病 - 基因协会中文献覆盖率（89％-91％）。症状术语的低鉴定精度也有助于验证与症状相关的关联（49％-62％）。]]></description>
      <guid>https://arxiv.org/abs/2502.14714</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Equivamap：利用LLMS自动等效检查优化配方</title>
      <link>https://arxiv.org/abs/2502.14760</link>
      <description><![CDATA[ARXIV：2502.14760V1公告类型：新 
摘要：组合优化的一个基本问题是识别等效的配方，这可以导致更有效的解决方案策略，并更深入地了解问题的计算复杂性。自动识别问题配方之间的等价性的需求已成为优化副驾驶，即从自然语言描述中产生问题表述的系统 - 散发出来。但是，依靠简单的启发式方法不足以进行严格的验证，现有的检查配方对等方法缺乏接地。受KARP减少的启发，在这项工作中，我们引入了准karp等效性，这是根据其决策变量之间的映射的存在，用于确定两个优化公式何时等效的形式标准。我们提出了EquivAmap，该框架利用大型语言模型自动发现此类映射，从而实现可扩展可靠的等价验证。为了评估我们的方法，我们构建了第一个相同优化公式的开源数据集，该数据集通过应用转换（例如添加松弛变量或有效的不平等现象）来生成。从经验上讲，EquivAmap显着胜过现有方法，在正确识别配方等效性方面取得了重大改进。]]></description>
      <guid>https://arxiv.org/abs/2502.14760</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>制定普遍政策通用</title>
      <link>https://arxiv.org/abs/2502.14777</link>
      <description><![CDATA[ARXIV：2502.14777V1公告类型：新 
摘要：能够解决各种顺序决策任务的通才代理的发展仍然是一个重大挑战。我们在跨代理设置中解决了这个问题，在该设置中，代理共享相同的观察空间，但其动作空间有所不同。我们的方法建立在通用政策框架上，该策略框架将政策学习分为两个阶段：基于扩散的计划者，生成观察序列和一个为这些计划分配动作的反向动态模型。我们提出了一种在由所有试剂的轨迹组成的联合数据集上训练计划者的方法。该方法通过汇总来自不同代理的数据来提供正转移的好处，而主要的挑战在于将共同计划调整到每个代理的独特约束。我们在Babyai环境中评估了我们的方法，涵盖了不同复杂性的任务，并表现出跨代理商的积极转移。此外，我们研究了计划者的概括能力，可以看不见的代理，并将我们的方法与传统的模仿学习方法进行比较。通过在多个代理商的汇总数据集上进行培训，我们的通用政策与单个代理商在数据集中训练的政策相比，任务完成准确性的提高最高为42.20 \％$。]]></description>
      <guid>https://arxiv.org/abs/2502.14777</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>优化化合物AI系统的模型选择</title>
      <link>https://arxiv.org/abs/2502.14815</link>
      <description><![CDATA[ARXIV：2502.14815V1公告类型：新 
摘要：结合多个LLM调用的复合AI系统，例如自我refine和Multi-Agent-debate，在许多AI任务上都实现了强大的性能。我们在优化复合系统时解决了一个核心问题：对于系统中的每个LLM调用或模块，应该如何决定使用哪个LLM？我们表明这些LLM选择对质量有很大影响，但是搜索空间是指数级的。我们提出了LLMSelector，这是复合系统中模型选择的有效框架，该框架利用了两个关键的经验见解：（i）端到端性能通常是每个模块在每个模块的性能方面的性能，并且所有其他模块都固定固定的固定效果，并且（ii）（ii）（ii）（ii） ）每模块性能可以通过LLM准确地估算。在这些见解的基础上，LLMSelector迭代选择一个模块，并根据LLM估计，以最高的模块性能分配模型，直到不可能进一步增益为止。 LLMSELECTOR适用于具有有界数模块数的任何复合系统，其API呼叫的数量与模块数量线性缩放，从经验和理论上实现高质量的模型分配。使用LLM，例如GPT-4O，Claude 3.5十四行诗和Gemini 1.5进行的流行复合系统（例如多代理争论和自我refine）进行的实验表明，LLMSELECTOR赋予5％-70％的精度提高，而不是使用相同的LLM用于所有模块。]]></description>
      <guid>https://arxiv.org/abs/2502.14815</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过基于图表的文档提问生成框架对多模式抹布进行基准测试</title>
      <link>https://arxiv.org/abs/2502.14864</link>
      <description><![CDATA[ARXIV：2502.14864V1公告类型：新 
摘要：多模式检索效果生成（MRAG）通过整合外部知识来增强推理能力。但是，现有的基准主要集中于简单的图像文本交互，俯瞰复杂的视觉格式，例如在现实世界应用中普遍存在的图表。在这项工作中，我们介绍了一个基于图表的MRAG的新任务，以解决此限制。为了半自动地生成高质量的评估样本，我们提出了基于图表的文档问题缠绕的生成（电荷），该框架是通过结构化关键点提取，跨模式验证和基于密钥的生成来产生评估数据的框架。通过将电荷与专家验证相结合，我们构建了Chart-MRAG基准，这是用于基于图表的MRAG评估的全面基准，其中包含来自现实世界文档的8个域中的4,738对撤退对。我们的评估揭示了当前方法中的三个关键局限性：（1）在基于图表的情况下统一的多模式嵌入方法挣扎，（2）即使使用基地检索，最先进的MLLM仅达到58.19％的正确性和73.877787覆盖率分数为％，（3）MLLM在基于图表的MRAG推理期间表现出一致的文本视图偏差。费用和图表-MRAG台在https://github.com/nomothings/carge.git上发布。]]></description>
      <guid>https://arxiv.org/abs/2502.14864</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>桥接模拟和现实：基于无人机的RF源本地化的基于3D聚类的深度学习模型</title>
      <link>https://arxiv.org/abs/2502.13969</link>
      <description><![CDATA[arxiv：2502.13969v1公告类型：交叉 
摘要：射频（RF）来源的定位具有关键的应用，包括搜索和救援，干扰器检测以及对敌对活动的监视。无人驾驶汽车（UAV）为RF源定位（RFSL）提供了显着优势，而不是陆地方法，利用自动驾驶3D导航和更高高度的信号捕获的改善。深度学习（DL）的最新进步进一步提高了本地化准确性，尤其是对于户外场景。 DL模型通常在现实世界中面临挑战，因为它们通常在无法完全复制现实世界条件的模拟数据集上进行培训。为了解决这个问题，我们首先提出了增强的两射线传播模型，从而通过提高传播环境建模的准确性来减少模拟到现实差距。对于RFSL，我们提出了基于3D群集的RealAdaptrnet，这是一种基于DL的方法，利用基于3D聚类的特征提取来鲁棒定位。实验结果表明，与传统的自由空间和两射线模型相比，提出的增强的两射线模型在模拟现实世界传播方案方面提供了卓越的准确性。值得注意的是，基于3D群集的RealAdaptrnet完全在模拟数据集上进行了训练，在使用Aerpaw物理测试台在实际环境中验证的情况下，在验证的情况下实现了出色的性能，平均定位误差为18.2 m。所提出的方法是计算上有效的，利用了少33.5倍的参数，并在各种轨迹之间证明了强大的概括能力，因此非常适合实际应用。]]></description>
      <guid>https://arxiv.org/abs/2502.13969</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>