<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.ai更新在arxiv.org上</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.ai在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Tue, 18 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>一种基于协调的方法，用于基于知识的系统</title>
      <link>https://arxiv.org/abs/2502.10394</link>
      <description><![CDATA[ARXIV：2502.10394V1公告类型：新 
摘要：通过阅读和机器阅读系统学习的最新学习进展已大大提高了基于知识的系统学习新事实的能力。在这项工作中，我们讨论了为这些基于知识的系统选择一组学习请求的问题，这将导致最大的Q/A性能。为了了解此问题的动态，我们模拟了学习策略的属性，该策略将学习请求发送给外部知识来源。我们表明，为这些学习系统选择一套最佳事实集类似于协调游戏，并使用强化学习来解决此问题。实验表明，这种方法可以显着提高Q/A性能。]]></description>
      <guid>https://arxiv.org/abs/2502.10394</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>位置：停止像语言模型代理一样行动是普通人</title>
      <link>https://arxiv.org/abs/2502.10420</link>
      <description><![CDATA[ARXIV：2502.10420V1公告类型：新 
摘要：语言模型代理（LMA）越来越多地视为能够自主与人类和工具进行互动。他们的设计和部署倾向于假设它们是能够维持连贯目标，跨环境调整并采取措施衡量意图的正常代理。这些假设对于工业，社会和政府环境中的潜在用例至关重要。但是LMA不是普通药物。他们继承了围绕它们建立的大语言模型（LLM）的结构性问题：幻觉，越狱，错位和不可预测性。在这个职位上，我们认为不应将LMA视为普通代理，因为这样做会导致破坏其效用和可信赖性的问题。我们列举了LMA的代理机构的病理。尽管脚手架（例如外部记忆和工具），它们仍然在本体学上无状态，随机，语义敏感并在语言上介导。这些病理破坏了LMA的本体论特性，包括可识别性，连续性，持久性和一致性，使他们对代理的主张有问题。作为响应，我们认为应在部署之前，之中和之后测量LMA本体论特性，以便可以减轻病理的负面影响。]]></description>
      <guid>https://arxiv.org/abs/2502.10420</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>动态思想链：迈向自适应深度推理</title>
      <link>https://arxiv.org/abs/2502.10428</link>
      <description><![CDATA[ARXIV：2502.10428V1公告类型：新 
摘要：为了减少由计算冗余和延迟奖励分配引起的计算资源的成本和消耗，本研究提出了动态的思想链，并具有自适应的推理时间和步骤。研究人员使用仿真实验来模拟通过Python 3.13闲置与基于GPT的Python模拟器的整合。同时，研究人员使用DeepSeek R1作为对照组来测试和比较D-COT模拟器在处理MIT OpenCourseware的线性代数考试问题时的性能。实验结果表明，基于三个指标的长COT，D-COT比DeepSeek R1好：推理时间，COT长度（推理步骤）和令牌计数，这可以显着降低计算资源消耗。此外，这项研究在深度推理优化中具有潜在的价值，可以用作将来动态深层推理框架的参考。]]></description>
      <guid>https://arxiv.org/abs/2502.10428</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>人工智能系统中的代理</title>
      <link>https://arxiv.org/abs/2502.10434</link>
      <description><![CDATA[ARXIV：2502.10434V1公告类型：新 
摘要：人们普遍担心人工智能（AI）研究的发展将导致有情的AI系统，这些系统可能对人类构成生存威胁。但是，为什么有知情的人工智能系统不能使人类受益呢？本文努力将这个问题以一种可行的方式提出。我问推定的AI系统是否会对我们的社会发展无私或恶意倾向，或者其代理机构的本质是什么？鉴于AI系统正在发展为强大的问题解决者，我们可以合理地期望这些系统优先考虑人类问题解决的有意识的方面。我确定了人类问题解决中代理的相关现象方面。意识理论提供的工具可以监视意识代理的功能方面。最近的一份专家报告（Butlin等，2023）根据这些理论确定了代理机构的功能主义指标。我展示了如何使用意识的综合信息理论（IIT）来监视该机构的惊人性质。如果我们能够随着AI系统的开发来监视AI系统的代理，那么我们可以劝阻他们成为社会的威胁，同时鼓励它们成为一种援助。]]></description>
      <guid>https://arxiv.org/abs/2502.10434</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AI按照您的酌情对准</title>
      <link>https://arxiv.org/abs/2502.10441</link>
      <description><![CDATA[ARXIV：2502.10441V1公告类型：新 
摘要：在AI对齐中，必须批准人类或算法的注释者，以判断哪些模型输出是“更好”或``更安全&#39;&#39;。我们将此纬度称为一致性酌处权。这种酌处权仍然在很大程度上没有审查，这带来了两个风险：（i）注释者可以任意使用其酌处权，并且（ii）模型可能无法模仿该酌处权。为了研究这种现象，我们借鉴了构成决策权的授权和行使的法律判断概念，特别是在原则冲突或其应用不清楚或无关紧要的情况下。扩展到AI的一致性，当一致性原则和规则（不可避免地）相互矛盾或柔和时，就需要酌处权。我们提出了一组指标，以系统地分析AI对准的何时以及如何行使酌处权，以便可以观察到风险（i）和（ii）。此外，我们区分人类和算法的酌处权，并分析它们之间的差异。通过测量对安全对准数据集的人类和算法的酌处权，我们揭示了以前未定为的对齐过程中的自由裁量权。此外，我们演示了在这些数据集上培训的算法如何在解释和运用这些原则时开发出自己的酌处权，这挑战了完全具有任何原则的目的。我们的论文介绍了在当前的一致过程中正式化这一核心差距的第一步，我们呼吁社区进一步审查和控制一致性酌处权。]]></description>
      <guid>https://arxiv.org/abs/2502.10441</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用财务算法方法进行离线增强学习的多种变压器解码</title>
      <link>https://arxiv.org/abs/2502.10473</link>
      <description><![CDATA[ARXIV：2502.10473V1公告类型：新 
摘要：离线增强学习（RL）算法使用固定的培训数据集学习策略，然后将其在线部署以与环境进行交互并做出决策。变形金刚是建模时间序列数据的标准选择，在离线RL中越来越受欢迎。在这种情况下，近似推理算法的Beam Search（BS）是首选的解码方法。离线RL消除了对昂贵或风险的在线数据收集的需求。但是，受限制的数据集引起不确定性，因为代理在训练数据中未涵盖的执行过程中遇到陌生的状态和行动序列。在这种情况下，BS缺乏离线RL必不可少的两个重要属性：它不能解释上述不确定性，其贪婪的左右搜索方法通常会导致序列的序列，但无法探索潜在的更好的替代方案。
  为了解决这些局限性，我们提出了投资组合束搜索（PBS），这是BS的一种简单有效的替代方案，可以在解码过程中平衡变压器模型中的探索和剥削。我们从金融经济学中汲取灵感，并应用这些原则来开发不确定性感知的多元化机制，我们将其集成到推理时期的顺序解码算法中。我们从经验上证明了PBS对D4RL运动基准的有效性，在该基准中，它可以实现更高的回报并显着降低结果的可变性。]]></description>
      <guid>https://arxiv.org/abs/2502.10473</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有上下文词典奖励偏好的多目标计划</title>
      <link>https://arxiv.org/abs/2502.10476</link>
      <description><![CDATA[ARXIV：2502.10476V1公告类型：新 
摘要：通常需要自主代理来计划根据上下文的偏好顺序有所不同的多个目标。代理商在其操作过程中可能会遇到多个上下文，每个上下文都对目标施加了独特的词典顺序，并具有与每个上下文相关的潜在奖励功能。现有的多目标计划的方法通常会考虑对目标，整个州空间的单个偏好顺序，并且不支持环境中多个客观订购下的计划。我们提出了上下文词典马尔可夫决策过程（CLMDP），该框架可以根据上下文，在不同的词典目标订购下进行计划。在CLMDP中，在一个状态下的客观排序和相关的奖励功能都取决于上下文。我们采用贝叶斯的方法来推断专家轨迹的状态映射。我们解决CLMDP的算法首先计算每个客观订购的策略，然后将它们结合到有效且无周期的单个上下文感知策略中。在模拟和使用移动机器人中评估了所提出方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2502.10476</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自动驾驶预测和计划中的知识整合策略：一项全面调查</title>
      <link>https://arxiv.org/abs/2502.10477</link>
      <description><![CDATA[ARXIV：2502.10477V1公告类型：新 
摘要：这项综合调查研究了基于知识的方法与自主驾驶系统的整合，重点是轨迹预测和计划。我们系统地回顾了将域知识，流量规则和常识性推理纳入这些系统的方法，纯粹涵盖了与混合神经符号架构的纯符号表示。特别是，我们分析了正式逻辑和差异逻辑编程，强化学习框架以及新兴技术的最新进展，这些技术利用了大型基础模型和扩散模型来进行知识表示。我们的讨论在统一文献调查部分下组织，将最新的最新概述综合为高级概述，并得到了一个详细的比较表，该表将密钥映射到各自的方法论类别。这项调查不仅强调了当前的趋势 - 包括越来越重视可解释的AI，对安全至关重要系统的正式验证以及在预测和计划中越来越多地使用生成模型 - 还概述了发展强大的知识的挑战和机会 - 增强自动驾驶系统。]]></description>
      <guid>https://arxiv.org/abs/2502.10477</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用交叉注意信号的自我监督的强化学习方法，用于微调大语言模型</title>
      <link>https://arxiv.org/abs/2502.10482</link>
      <description><![CDATA[ARXIV：2502.10482V1公告类型：新 
摘要：我们为训练大型语言模型提供了一个新颖的强化学习框架，该框架不依赖于循环反馈中的人。取而代之的是，我们的方法使用模型本身中的交叉注意信号来获得自我监督的奖励，从而指导模型策略的迭代微调。通过分析该模型在生成期间如何参与输入提示，我们构建了及时覆盖，重点和连贯性的度量。然后，我们使用这些措施来对候选响应进行排名或评分，提供奖励信号，鼓励模型在主题文本上产生良好的一致性。在与标准策略梯度方法的经验比较和合成偏好模型的RL微调中，我们的方法在非RL基线的迅速相关性和一致性方面显示出显着提高。虽然它尚未与完全监督的RLHF系统的性能相匹配，但它突出了以最少的人类标签来扩展对齐的重要方向。我们提供详细的分析，讨论潜在的局限性，并概述将未来的工作与较少的人类反馈相结合的未来工作。]]></description>
      <guid>https://arxiv.org/abs/2502.10482</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>石墨：带有提示优化的LLM的文本属性图上有效的节点分类</title>
      <link>https://arxiv.org/abs/2502.10522</link>
      <description><![CDATA[ARXIV：2502.10522V1公告类型：新 
摘要：大型语言模型（LLM）在图形数据上的应用引起了很多关注。 LLM允许我们在文本属性图中使用预验证模型的深层上下文嵌入，其中经常将浅嵌入用于节点的文本。但是，有效地将图形结构和特征纳入LLMS使用的顺序形式仍然是具有挑战性的。此外，仅LLM的性能高度取决于输入提示的结构，这限制了它们作为可靠的方法的有效性，并且通常需要迭代性的调整，这些调整可能会缓慢，乏味且难以以编程方式复制。在本文中，我们提出了图形（文本中的图形），这是将图形编码为文本格式的框架，并优化图形预测任务的LLM提示。在这里，我们重点介绍文本属性图的节点分类。我们将每个节点及其社区的图形数据编码为简洁的文本，以使LLMS能够更好地利用图中的信息。然后，我们进一步编程优化了LLM，提示了我们的DSPY框架以自动化此步骤并使其更有效和可重现。 Graphit在三个数据集上的基础优于基于LLM的基线，我们展示了Grushit中的优化步骤如何导致更好的结果，而无需手动及时调整。我们还证明，我们的图形编码方法与其他图形编码方法具有竞争力，同时又较便宜，因为它在同一任务中使用的令牌大大减少。]]></description>
      <guid>https://arxiv.org/abs/2502.10522</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基准使用传递性公理进行AI决策的合理性</title>
      <link>https://arxiv.org/abs/2502.10554</link>
      <description><![CDATA[ARXIV：2502.10554V1公告类型：新 
摘要：基本选择公理，例如偏好的传递性，为确定人类决策是否合理，即与效用代表一致，提供了可检验的条件。最近的工作表明，接受人类数据培训的AI系统可以表现出与人类类似的推理偏见，并且AI反过来又可以通过AI推荐系统偏向人类的判断。我们通过一系列选择实验来评估AI响应的合理性，旨在评估人类偏爱的传递性。我们考虑了Meta的Llama 2和3 LLM型号的十个版本。我们应用贝叶斯模型选择来评估这些AI生成的选择是否违反了两个突出的传递模型。我们发现，美洲驼（Llama 2和3）的模型通常满足传递性，但是当确实发生违规时，仅在LLMS的聊天/指示版本中发生。我们认为，理性公理（例如偏好的传递性）对于评估和基准测试AI生成的响应的质量和基准为理解AI系统中的计算合理性提供了基础。]]></description>
      <guid>https://arxiv.org/abs/2502.10554</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>观察者感知的概率计划在部分可观察性下</title>
      <link>https://arxiv.org/abs/2502.10568</link>
      <description><![CDATA[ARXIV：2502.10568V1公告类型：新 
摘要：在本文中，我们有兴趣的计划问题，代理商知道观察者的存在，以及该观察者处于部分观察性情况下。代理必须选择其策略，以优化观察结果传递的信息。在观察者感知的马尔可夫决策过程（OAMDP）的基础上，我们提出了一个框架来处理这种类型的问题，从而正式化了诸如可读性，明确性和可预测性之类的属性。将OAMDP扩展到部分可观察性不仅可以解决更现实的问题，而且还可以考虑动态的隐藏变量。这些动态目标变量允许使用可预测性工作，或者在执行过程中可能会改变目标的可读性问题。我们讨论了PO-OAMDP的理论特性，并通过实验基准问题，分析了HSVI的收敛行为，并研究了专用的初始化并研究了由此产生的策略。]]></description>
      <guid>https://arxiv.org/abs/2502.10568</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PROMRVL-CAD：主动对话系统，具有用于计算机辅助诊断的多轮视觉互动</title>
      <link>https://arxiv.org/abs/2502.10620</link>
      <description><![CDATA[ARXIV：2502.10620V1公告类型：新 
摘要：大型语言模型（LLM）的最新进展已证明了非凡的理解能力，并在各种视觉语言任务上取得了显着突破。但是，LLM在生成可靠的医学诊断报告中的应用仍处于早期阶段。目前，医疗LLM通常采用被动互动模型，医生对患者的查询做出反应，几乎没有参与分析医疗图像。相比之下，一些聊天机器人只是根据视觉输入对预定义的查询做出响应，缺乏交互式对话或对病史的考虑。因此，LLM生成的患者 - 肉酱相互作用与实际患者咨询中发生的患者相互作用之间存在差距。为了弥合这一差距，我们开发了一个基于LLM的对话系统，即用于计算机辅助诊断（Promrvl-CAD）的积极主动的多轮视觉相互作用，以生成患者友好的疾病诊断报告。提出的Promrvl-CAD系统允许主动对话通过将知识图集成到建议系统中为患者提供持续可靠的医疗访问。具体而言，我们设计了两个发电机：一个主动的问题生成器（Pro-Q Gen），以产生指导诊断程序的主动问题和多视Vision患者TEXT诊断报告生成器（MVP-DR GEN）生成高质量的诊断报告。评估两个现实世界可用的数据集，即Mimic-CXR和IU-XRAY，我们的模型在生成医疗报告方面具有更好的质量。我们进一步证明了在图像质量低下的情况下，Promrvl的性能可靠。此外，我们创建了一个合成的医学对话数据集，该数据集模拟了患者和医生之间的主动诊断互动，并作为培训LLM的宝贵资源。]]></description>
      <guid>https://arxiv.org/abs/2502.10620</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用户-VLM 360：个性化视觉语言模型，具有用户意识调整的社交人类机器人互动</title>
      <link>https://arxiv.org/abs/2502.10636</link>
      <description><![CDATA[ARXIV：2502.10636V1公告类型：新 
摘要：将视觉模型集成到机器人系统中构成了使机器以更直观的方式与周围环境相互作用的重大进步。虽然VLM提供丰富的多模式推理，但现有方法缺乏特定于用户的适应性，通常依靠无法说明个体行为，上下文或社会情感细微差别的通用交互范式。尝试自定义时，用户数据中的偏见产生了道德问题，冒着排除或不公平治疗的风险。为了应对这些双重挑战，我们建议用户-VLM 360 {\ deg}，这是一个整体框架，将多模式用户建模与偏置感知优化整合在一起。我们的方法功能：（1）使用视觉语言信号实时适应交互作用的用户意识调整； （2）通过偏好优化偏置缓解； （3）策划了360 {\ deg}，以人口统计学，情感和关系元数据注释的社会情感互动数据集。跨八个基准测试的评估表明了最新的结果：个性化VQA中的 +35.3％F1，面部特征理解中的 +47.5％F1，降低15％的偏置和30倍的速度，比基线速度加速30倍。消融研究证实了组件功效，而在胡椒机器人上的部署可以验证各种用户的实时适应性。我们开放源参数有效的3B/10B模型和负责调整的道德验证框架。]]></description>
      <guid>https://arxiv.org/abs/2502.10636</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有多模式预训练模型的社交机器人技术的人口统计用户建模</title>
      <link>https://arxiv.org/abs/2502.10642</link>
      <description><![CDATA[ARXIV：2502.10642V1公告类型：新 
摘要：本文根据视觉语言人口统计数据研究了用户分析任务中多模式预训练模型的性能。这些模型对于适应社会机器人技术中人类用户的需求和偏好至关重要，从而提供个性化的响应并提高互动质量。首先，我们介绍了两个专门策划的数据集，以表示从用户面部图像衍生的人口统计学特征。接下来，我们在这些数据集上（无论是在开箱即用的状态和微调之后）评估了这些数据集上突出的对比度多模式预训练的模型的性能。初始结果表明，剪辑在不进行微调的情况下将图像与人口统计学描述匹配时进行了次优。尽管微调显着提高了其预测能力，但该模型在有效地概括了细微的人口细微差别时仍表现出局限性。为了解决这个问题，我们建议采用蒙版的图像建模策略来改善概括并更好地捕获微妙的人口属性。这种方法为增强多模式用户建模任务中的人口敏感性提供了一种途径。]]></description>
      <guid>https://arxiv.org/abs/2502.10642</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>COPEFT：与参数效率微调的多代理协作感知的快速适应框架</title>
      <link>https://arxiv.org/abs/2502.10705</link>
      <description><![CDATA[ARXIV：2502.10705V1公告类型：新 
摘要：通过交换互补信息克服单人感知的局限性，预计多代理协作感知将显着提高感知表现。但是，培训强大的协作感知模型需要收集足够的培训数据，以涵盖所有可能的协作方案，这是由于无法容忍的部署成本而不切实际的。因此，训练有素的模型在新的流量情况下并不稳健，并且数据分布不一致，并且从根本上限制了其现实世界中的适用性。此外，现有的方法（例如域适应性）通过在培训阶段暴露部署数据来减轻此问题，但会产生高训练成本，这对于资源受限的代理人来说是不可行的。在本文中，我们提出了一个基于参数效率微调的轻量级框架COPEFT，用于快速适应训练有素的协作感知模型，以在低成本条件下针对新的部署环境。 Copeft开发了一个协作适配器和代理提示，以分别执行宏观和微观适应。具体而言，协作适配器利用培训数据中的固有知识和有限的部署数据来使功能映射适应新的数据分布。代理提示通过插入有关环境的细粒度上下文信息进一步增强了协作适配器。广泛的实验表明，我们的COPEFT超过了少于1 \％可训练参数的现有方法，证明了我们提出的方法的有效性和效率。]]></description>
      <guid>https://arxiv.org/abs/2502.10705</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>像孩子一样成长AI的哲学基础</title>
      <link>https://arxiv.org/abs/2502.10742</link>
      <description><![CDATA[ARXIV：2502.10742V1公告类型：新 
摘要：尽管在高水平推理方面表现出色，但当前的语言模型在现实世界中缺乏鲁棒性，并且在对人类直观的基本解决问题的任务上表现不佳。本文认为，这两种挑战源于人类和机器认知发展之间的核心差异。尽管这两种系统都依赖于增加代表权，但人类出现语言模型中缺乏核心知识基础的认知结构，从而开发了可靠的，可推广的能力，在这些能力中，复杂的技能在其各自领域内的简单技能基础上基于更简单。它探讨了人类核心知识的经验证据，分析了语言模型为什么无法获取它，并认为这种限制不是固有的建筑约束。最后，它概述了通过认知原型制定策略大规模生成的合成训练数据，可以将核心知识系统地整合到未来的多模式模型中。]]></description>
      <guid>https://arxiv.org/abs/2502.10742</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深度是您需要的吗？ LLMS中迭代推理的探索</title>
      <link>https://arxiv.org/abs/2502.10858</link>
      <description><![CDATA[ARXIV：2502.10858V1公告类型：新 
摘要：深层迭代链链（COT）推理使LLM可以通过逐步激活相关的预训练知识来解决复杂的任务。但是，它在确保不断改进和确定停止标准方面面临挑战。在本文中，我们研究了是否可以从初始推理路径中激活直接有助于解决给定问题的相关知识，从而规避迭代性改进的需求。我们的实验表明，提高初始推理路径的多样性可以实现可比性或优越的性能，这是我们称为\ textit {宽度推理}的概念。但是，现有的广度推理方法（例如自洽）提供了有限的多样性。为了解决这一限制，我们提出了一种简单而有效的方法，该方法通过将上下文探索与采样随机性减少结合来增强推理广度。广泛的实验表明，我们的方法明显优于深刻的迭代推理。我们的代码在https://github.com/zongqianwu/breadth中提供。]]></description>
      <guid>https://arxiv.org/abs/2502.10858</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM推理的教程：Chatgpt O1背后的相关方法</title>
      <link>https://arxiv.org/abs/2502.10867</link>
      <description><![CDATA[ARXIV：2502.10867V1公告类型：新 
摘要：OpenAI O1表明，应用强化学习在推理过程中直接整合推理步骤可以显着提高模型的推理能力。该结果令人兴奋，因为现场从传统的自回归方法过渡，即产生答案，以通过逐步推理训练对缓慢思考的过程进行建模。强化学习在模型的培训和解码过程中都起着关键作用。在本文中，我们介绍了推理问题的全面表述，并研究了基于模型和无模型的方法的使用，以更好地支持这种缓慢思考的框架。]]></description>
      <guid>https://arxiv.org/abs/2502.10867</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PCGRLLM：用于程序内容生成强化学习的大语模型驱动奖励设计</title>
      <link>https://arxiv.org/abs/2502.10906</link>
      <description><![CDATA[ARXIV：2502.10906V1公告类型：新 
摘要：奖励设计在游戏AIS的培训中起着关键作用，需要大量的领域特定知识和人类努力。近年来，一些研究探索了培训游戏代理商的奖励生成，并使用大语言模型（LLM）控制机器人。在内容生成文献中，已经有早期的努力为增强学习代理生成器产生奖励功能。这项工作介绍了PCGRLLM，这是一种基于早期工作的扩展体系结构，该架构采用了反馈机制和几种基于推理的及时工程技术。我们使用两个最先进的LLM在二维环境中对故事到奖励的生成任务进行评估，以证明我们方法的普遍性。我们的实验提供了有见地的评估，以证明LLMS对于内容生成任务所必需的功能。结果取决于语言模型的零拍功能，分别凸显了415％和40％的显着性能提高。我们的工作表明了减少人类AI开发中人类依赖的潜力，同时支持和增强创作过程。]]></description>
      <guid>https://arxiv.org/abs/2502.10906</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>