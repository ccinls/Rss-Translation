<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.AI 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.AI 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Tue, 10 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>重温你的记忆：通过脑电图引导的视听生成重建情感情境化记忆</title>
      <link>https://arxiv.org/abs/2412.05296</link>
      <description><![CDATA[arXiv:2412.05296v1 公告类型：新
摘要：在本文中，我们介绍了 RecallAffectiveMemory，这是一项新颖的任务，旨在通过从脑电图 (EEG) 信号中提取的情感引导的视听生成来重建自传体记忆。为了支持这项开创性的任务，我们提出了 EEG-AffectiveMemory 数据集，该数据集包含从九名参与者回忆记忆时收集的文本描述、视觉效果、音乐和 EEG 记录。此外，我们提出了 RYM（回忆你的记忆），这是一个三阶段框架，用于生成同步的视听内容，同时保持动态的个人记忆情感轨迹。实验结果表明，我们的方法可以忠实地重建所有受试者的情感情境化视听记忆，无论是定性和定量，参与者报告他们回忆的记忆和生成的内容之间存在很强的情感一致性。我们的方法推动了情感解码研究及其通过基于神经的情感理解在个性化媒体创作中的实际应用。]]></description>
      <guid>https://arxiv.org/abs/2412.05296</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DRL4AOI：基于位置服务中语义感知 AOI 分割的 DRL 框架</title>
      <link>https://arxiv.org/abs/2412.05437</link>
      <description><![CDATA[arXiv:2412.05437v1 公告类型：新
摘要：在基于位置的服务 (LBS) 中，例如食品配送，一项基本任务是分割兴趣区域 (AOI)，旨在将城市地理空间划分为不重叠的区域。传统的 AOI 分割算法主要依靠道路网络来划分城市区域。虽然基于道路网络的模型在建模地理语义方面很有前景，但它们忽略了 LBS 服务中的服务语义目标（例如，工作量平等）。在本文中，我们指出 AOI 分割问题可以自然地表述为马尔可夫决策过程 (MDP)，该过程逐渐为当前 AOI 边界中的每个网格选择一个附近的 AOI。基于 MDP，我们首次尝试将深度强化学习 (DRL) 推广到 AOI 分割，从而产生了一种基于 DRL 的新型框架，称为 DRL4AOI。 DRL4AOI 框架以灵活的方式引入不同的服务语义目标，将它们视为指导 AOI 生成的奖励。为了评估 DRL4AOI 的有效性，我们开发并发布了一个 AOI 分割系统。我们还介绍了 DRL4AOI 的代表性实现 - TrajRL4AOI - 用于物流服务中的 AOI 分割。它引入了双深度 Q 学习网络 (DDQN) 来逐步优化 AOI 生成以实现两个特定的语义目标：i) 轨迹模块化，即最大化 AOI 内轨迹连接的紧密度和 AOI 之间连接的稀疏度，ii) 与道路网络的匹配度，即最大化 AOI 与道路网络之间的匹配度。对合成数据和真实数据进行的定量和定性实验证明了我们方法的有效性和优越性。代码和系统可在 https://github.com/Kogler7/AoiOpt 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2412.05437</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>代数电路组合图集</title>
      <link>https://arxiv.org/abs/2412.05481</link>
      <description><![CDATA[arXiv:2412.05481v1 公告类型：新
摘要：基于和积结构的电路已成为一种普遍存在的表示，用于紧凑地编码知识，从布尔函数到概率分布。通过对此类电路的结构施加约束，某些推理查询变得易于处理，例如模型计数和最可能的配置。最近的研究探索了将概率和因果推理查询分析为基本运算符的组合以得出可处理性条件。在本文中，我们从代数的角度进行组合推理，并表明一大类查询 - 包括边际 MAP、概率答案集编程推理和因果后门调整 - 对应于半环上的基本运算符的组合：聚合、乘积和元素映射。利用这个框架，我们发现了这些运算符可处理组合的简单和一般的充分条件，包括电路属性（例如边际确定性、兼容性）和元素映射的条件。应用我们的分析，我们为许多此类组合查询推导出新的可处理性条件。我们的结果统一了电路上现有问题的可处理性条件，同时为分析新的组合推理查询提供了蓝图。]]></description>
      <guid>https://arxiv.org/abs/2412.05481</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>不仅仅是营销？人工智能基准对从业者的信息价值</title>
      <link>https://arxiv.org/abs/2412.05520</link>
      <description><![CDATA[arXiv:2412.05520v1 公告类型：新 
摘要：公共 AI 基准测试结果被模型开发人员广泛传播，作为不断增长且竞争激烈的市场中模型质量的指标。然而，这些宣传的分数并不一定反映最终将应用 AI 模型的人感兴趣的特征。在本文中，我们试图了解 AI 基准测试是否以及如何用于为决策提供信息。根据对 19 名在日常工作中使用或决定不使用基准测试的个人的访谈分析，我们发现在这些环境中，参与者使用基准测试作为模型之间相对性能差异的信号。然而，这个信号是否被认为是模型优越性的明确标志，是否足以用于下游决策，这一点各不相同。在学术界，公共基准测试通常被视为捕捉研究进展的合适指标。相比之下，无论是在产品还是政策方面，基准测试——即使是内部为特定任务开发的基准测试——也常常被发现不足以为实质性决策提供信息。对于被认为不令人满意的基准，受访者表示，他们的目标既不明确，也不反映实际使用情况。根据研究结果，我们得出结论，有效的基准应该提供有意义的、现实世界的评估，纳入领域专业知识，并保持范围和目标的透明度。它们必须捕捉多样化、与任务相关的能力，具有足够的挑战性以避免快速饱和，并考虑模型性能的权衡，而不是依赖单一的分数。此外，专有数据收集和污染预防对于产生可靠且可操作的结果至关重要。通过遵守这些标准，基准可以超越单纯的营销技巧，成为强大的评估框架。]]></description>
      <guid>https://arxiv.org/abs/2412.05520</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>人工智能规划：入门与调查（初步报告）</title>
      <link>https://arxiv.org/abs/2412.05528</link>
      <description><![CDATA[arXiv:2412.05528v1 公告类型：新
摘要：自动决策是一个涵盖人工智能多个子学科的基本主题：强化学习 (RL)、人工智能规划 (AP)、基础模型和运筹学等。尽管最近努力“弥合”这些社区之间的差距，但仍有许多见解尚未超越界限。我们在本文中的目标是提供一份简短且非详尽的入门知识，介绍 AP 中众所周知但在其他子学科中不太知名的想法。我们通过介绍经典的 AP 问题和表示，以及通过马尔可夫决策过程形式处理不确定性和时间的扩展来实现这一点。接下来，我们调查解决 AP 问题的最新技术和想法，重点关注它们利用问题结构的能力。最后，我们介绍 AP 中的子领域，用于从非结构化输入中学习结构并学习推广到看不见的场景和情况。]]></description>
      <guid>https://arxiv.org/abs/2412.05528</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>走向推理学习：法学硕士与神经符号在抽象推理中的算术关系方面的比较</title>
      <link>https://arxiv.org/abs/2412.05586</link>
      <description><![CDATA[arXiv:2412.05586v1 公告类型：新 
摘要：这项工作比较了大型语言模型 (LLM) 和神经符号方法在解决 Raven 渐进矩阵 (RPM) 方面的表现，RPM 是一种视觉抽象推理测试，涉及对数学规则（例如渐进或算术加法）的理解。将视觉属性直接作为文本提示提供，假设存在一个 oracle 视觉感知模块，这使我们能够单独测量模型的抽象推理能力。尽管从 oracle 视觉感知和高级提示技术中提供了这种组合结构化的表示，但 GPT-4 和 Llama-3 70B 都无法在 I-RAVEN 数据集的中心星座上实现完美的准确性。我们的分析表明，根本原因在于 LLM 在理解和执行算术规则方面的弱点。作为一种潜在的补救措施，我们分析了具有情境感知的溯因规则学习器 (ARLC)，这是一种学习使用向量符号架构 (VSA) 进行推理的神经符号方法。在这里，概念用分布式向量表示，编码向量之间的点积定义相似性核，对向量进行简单的元素运算对编码值执行加法/减法。我们发现 ARLC 在 I-RAVEN 的中心星座上实现了几乎完美的准确度，证明了算术规则的高保真度。为了强调模型的长度泛化能力，我们将 RPM 测试扩展到更大的矩阵（3x10 而不是典型的 3x3）和更大的属性值动态范围（从 10 到 1000）。我们发现 LLM 解决算术规则的准确率会下降到 10% 以下，尤其是在动态范围扩大的情况下，而 ARLC 可以保持较高的准确率，因为它在适当分布的表示上模拟符号计算。我们的代码可在 https://github.com/IBM/raven-large-language-models 上找到。]]></description>
      <guid>https://arxiv.org/abs/2412.05586</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从灵活性到可操纵性：XAI 评估的滑坡</title>
      <link>https://arxiv.org/abs/2412.05592</link>
      <description><![CDATA[arXiv:2412.05592v1 公告类型：新
摘要：缺乏基本事实解释标签是可解释人工智能 (XAI) 定量评估的一个基本挑战。当评估方法具有许多必须由用户指定的超参数时，这一挑战变得尤其成问题，因为没有基本事实来确定最佳超参数选择。通常无法对超参数进行详尽的搜索，因此研究人员通常根据文献中的类似研究做出规范选择，这为用户提供了极大的灵活性。在这项工作中，我们说明了如何利用这种灵活性来操纵评估结果。我们将这种操纵定义为对评估的对抗性攻击，其中超参数设置的看似无害的变化会显着影响评估结果。我们证明了我们在多个数据集上操纵的有效性，评估结果在多个解释方法和模型中发生了很大变化。最后，我们提出了一种基于跨超参数排名的缓解策略，旨在为此类操纵提供鲁棒性。这项工作强调了进行可靠的XAI评估的难度，并强调了在XAI中采用整体和透明的评估方法的重要性。]]></description>
      <guid>https://arxiv.org/abs/2412.05592</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>RL Zero：无需任何监督的零样本语言到行为</title>
      <link>https://arxiv.org/abs/2412.05718</link>
      <description><![CDATA[arXiv:2412.05718v1 公告类型：新
摘要：奖励仍然是一种无法解释的指定强化学习任务的方式，因为人类通常无法预测任何给定奖励函数的最佳行为，从而导致奖励设计和奖励黑客攻击。语言提供了一种向代理传达意图和绕过奖励设计的吸引人的方式，但之前的努力受到昂贵且不可扩展的标记工作的限制。在这项工作中，我们提出了一种完全无监督的替代方法，以零样本方式将语言指令扎根以获得策略。我们提出了一种采用想象、投影和模仿形式的解决方案：代理想象与任务的语言描述相对应的观察序列，将想象的序列投影到我们的目标域，并将其扎根到策略中。视频语言模型使我们能够想象利用从互联网规模的视频文本映射中学习到的任务知识的任务描述。挑战仍然是将这些代际扎根到策略中。在这项研究中，我们展示了如何实现零样本语言到行为策略，方法是首先将想象的序列建立在无监督 RL 代理的真实观察中，然后使用闭式模拟学习解决方案，使 RL 代理能够模仿这些观察结果。据我们所知，我们的方法 RLZero 是第一个在模拟领域的各种任务上展示零样本语言到行为生成能力的方法，无需任何监督。我们进一步展示了 RLZero 还可以从跨实体视频（例如从 YouTube 上抓取的视频）中生成零样本策略。]]></description>
      <guid>https://arxiv.org/abs/2412.05718</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一种跨不同数据检索增强生成的协作多智能体方法</title>
      <link>https://arxiv.org/abs/2412.05838</link>
      <description><![CDATA[arXiv:2412.05838v1 公告类型：新
摘要：检索增强生成 (RAG) 通过将外部特定于领域的数据纳入生成过程来增强大型语言模型 (LLM)。虽然 LLM 功能强大，但它们通常依赖于静态的、预先训练的数据集，从而限制了它们集成动态或私有数据的能力。传统的 RAG 系统通常使用单代理架构来处理查询生成、数据检索和响应合成。但是，在处理各种数据源（例如关系数据库、文档存储和图形数据库）时，这种方法变得效率低下，通常会导致性能瓶颈和准确性降低。本文提出了一种多代理 RAG 系统来解决这些限制。专门的代理（每个代理都针对特定数据源进行了优化）处理关系、NoSQL 和基于文档的系统的查询生成。这些代理在模块化框架内协作，查询执行委托给为兼容各种数据库类型而设计的环境。这种分布式方法通过确保每个代理专注于其专门的任务来提高查询效率、减少令牌开销并提高响应准确性。所提出的系统具有可扩展性和适应性，使其成为需要与多样化、动态或私有数据源集成的生成 AI 工作流的理想选择。通过利用专门的代理和模块化执行环境，该系统为处理生成 AI 应用程序中复杂、异构的数据环境提供了高效而强大的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2412.05838</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>人工智能的双重标准：人类根据一个人的行为来评判所有人工智能</title>
      <link>https://arxiv.org/abs/2412.06040</link>
      <description><![CDATA[arXiv:2412.06040v1 公告类型：新
摘要：机器人和其他人工智能 (AI) 系统被广泛视为对其行为负责的道德主体。随着人工智能的激增，这些看法可能会通过对一个人工智能的态度对其他人工智能的态度的道德溢出而纠缠在一起。我们在两个预先注册的实验中测试了人工智能或人类代理看似有害和不道德的行为如何影响对其他人工智能或人类的态度。在研究 1 (N = 720) 中，我们通过表明不道德行为增加了对负面道德主体的归因（即不道德的行为）并减少了对积极道德主体的归因（即道德行为）和道德耐心（即值得道德关注）对主体（聊天机器人或人类助手）及其所属群体（所有聊天机器人或人类助手）。人工智能和人类环境之间的溢出效应没有显著差异。在研究 2（N = 684）中，我们测试了当代理被赋予名称并被描述为人工智能或人类，而不是具体描述为聊天机器人或个人助理时，溢出效应是否持续存在。我们发现溢出效应在人工智能环境中持续存在，但在人类环境中不存在，可能是因为人工智能因其相对于人类的外群地位而被认为更加同质化。这种不对称表明存在双重标准，即当一个代理在道德上逾越时，人工智能会比人类受到更严厉的评判。随着多样化、自主的人工智能系统的激增，人机交互研究和设计应该考虑到这样一个事实：对一种人工智能的体验很容易推广到对所有人工智能的看法和负面的人机交互结果，例如信任度降低。]]></description>
      <guid>https://arxiv.org/abs/2412.06040</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用语言模型进行查询高效规划</title>
      <link>https://arxiv.org/abs/2412.06162</link>
      <description><![CDATA[arXiv:2412.06162v1 公告类型：新
摘要：在复杂环境中进行规划需要代理有效地查询世界模型，以找到从开始到目标的可行动作序列。最近的研究表明，大型语言模型 (LLM) 具有丰富的先验知识和推理能力，可以通过搜索有希望的状态并适应来自世界的反馈来帮助规划。在本文中，我们提出并研究了两个利用 LLM 进行查询高效规划的根本竞争框架。第一个使用 LLM 作为基于搜索的规划器中的启发式方法，以选择有希望的节点进行扩展并提出有希望的操作。第二个使用 LLM 作为生成规划器来提出从开始到目标的整个动作序列，查询世界模型并根据反馈进行调整。我们表明，虽然这两种方法都在可比基线上有所改进，但使用 LLM 作为生成规划器可以显着减少交互。我们的主要发现是，与 LLM 作为启发式方法相比，LLM 作为规划器可以更快地根据即时反馈调整其规划策略。我们对 Robotouille 和 PDDL 规划基准进行了评估和比较，并讨论了与查询高效规划算法现有理论的联系。代码可在 https://github.com/portal-cornell/llms-for-planning 上找到]]></description>
      <guid>https://arxiv.org/abs/2412.06162</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ACQ：在线广告自动化程序化创意的统一框架</title>
      <link>https://arxiv.org/abs/2412.06167</link>
      <description><![CDATA[arXiv:2412.06167v1 Announce Type: new 
摘要：在在线广告中，需求方平台（DSP）允许广告商创建不同的广告创意进行实时竞价。直观地讲，广告商倾向于为单张照片创建更多的广告创意以增加参与竞价的概率，从而进一步提高他们的广告成本。从DSP的角度来看，以下是两个被忽视的问题。一方面，广告创意的数量不能无限增长。另一方面，随着广告创意数量的增加，广告成本的边际效应会减小。为此，本文提出了一个两阶段的框架，称为自动创意配额（ACQ），以实现广告创意的自动创建和停用。ACQ在多个广告商之间动态分配创意配额，以最大化广告平台的收入。 ACQ 包含两个部分：预测模块，用于估算不同广告素材数量下照片的成本；分配模块，用于根据预测模块中照片的预估成本来决定照片的配额。具体而言，在预测模块中，我们开发了一个基于不平衡二叉树的多任务学习模型，以有效缓解目标变量不平衡问题。在分配模块中，我们将配额分配问题公式化为多项选择背包问题 (MCKP)，并开发了一个高效的求解器来解决这种涉及数千万广告的大规模问题。我们进行了大量的离线和在线实验来验证我们提出的框架的优越性，将成本提高了 9.34%。]]></description>
      <guid>https://arxiv.org/abs/2412.06167</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士作为辩论伙伴：利用遗传算法和对抗性搜索进行自适应辩论</title>
      <link>https://arxiv.org/abs/2412.06229</link>
      <description><![CDATA[arXiv:2412.06229v1 公告类型：新
摘要：本文介绍了 DebateBrawl，这是一个创新的人工智能辩论平台，它集成了大型语言模型 (LLM)、遗传算法 (GA) 和对抗性搜索 (AS)，以创造一种自适应且引人入胜的辩论体验。DebateBrawl 通过结合进化优化和博弈论技术解决了传统 LLM 在战略规划中的局​​限性。该系统在实时调整策略的同时，在生成连贯、上下文相关的论点方面表现出色。涉及 23 场辩论的实验结果显示，人工智能和人类参与者之间的结果均衡，人工智能系统的平均得分为 2.72，而人类平均得分为 2.67（满分 10 分）。用户反馈表明辩论技巧有显著提高，学习体验非常令人满意，85% 的用户报告辩论能力有所提高，78% 的用户发现人工智能对手具有适当的挑战性。该系统能够保持较高的事实准确性（92%，而人类辩论仅为 78%），同时生成多样化的论点，解决了人工智能辅助辩论中的关键问题。DebateBrawl 不仅是一种有效的教育工具，而且还有助于实现通过人工智能辅助辩论改善公共辩论的更广泛目标。本文讨论了人工智能在说服性环境中的伦理影响，并概述了为确保负责任地开发和部署该系统而实施的措施，包括强大的事实核查机制和决策过程的透明度。]]></description>
      <guid>https://arxiv.org/abs/2412.06229</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>面向自动化规划的高级建模</title>
      <link>https://arxiv.org/abs/2412.06312</link>
      <description><![CDATA[arXiv:2412.06312v1 公告类型：新
摘要：规划是一项基本活动，经常出现在从日常任务到工业流程等许多情况下。规划任务包括从指定的初始条件中选择一系列操作来实现指定的目标。规划域定义语言 (PDDL) 是自动规划领域中用于建模规划问题的主要语言。以前的工作强调了 PDDL 的局限性，特别是在其表达能力方面。我们的兴趣在于促进复杂问题的处理并增强自动规划系统的整体能力。Unified-Planning 是一个 Python 库，提供高级 API 来指定规划问题并调用自动规划器。在本文中，我们介绍了 UP 库的扩展，旨在增强其对高级问题建模的表达能力。特别是，我们添加了一个数组类型、一个用于计算布尔值的表达式以及操作中整数参数的允许。我们展示了这些设施如何实现三个经典规划问题的自然高级模型。]]></description>
      <guid>https://arxiv.org/abs/2412.06312</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GameArena：通过现场电脑游戏评估法学硕士推理能力</title>
      <link>https://arxiv.org/abs/2412.06394</link>
      <description><![CDATA[arXiv:2412.06394v1 公告类型：新
摘要：评估大型语言模型 (LLM) 的推理能力具有挑战性。现有的基准测试通常依赖于静态数据集，这些数据集容易受到数据污染并且可能会随着时间的推移而饱和，或者依赖于将推理与其他能力混为一谈的二进制实时人工反馈。作为最突出的动态基准测试，Chatbot Arena 评估了现实世界中的开放式问题，但缺乏评估特定推理能力的粒度。我们推出了 GameArena，这是一个动态基准测试，旨在通过与人类的互动游戏来评估 LLM 推理能力。GameArena 由三款游戏组成，旨在测试特定的推理能力（例如，演绎和归纳推理），同时让参与者保持娱乐和参与。我们回顾性地分析游戏数据，以揭示 LLM 的底层推理过程并衡量其细粒度的推理能力。我们收集了 2000 多个游戏会话，并对五款最先进的 LLM 的各种推理能力进行了详细评估。我们对 100 名参与者的用户研究表明，与 Chatbot Arena 相比，GameArena 提高了用户参与度。GameArena 首次实现了在野外收集分步 LLM 推理数据。]]></description>
      <guid>https://arxiv.org/abs/2412.06394</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过欲望驱动的自主性模拟人类的日常活动</title>
      <link>https://arxiv.org/abs/2412.06435</link>
      <description><![CDATA[arXiv:2412.06435v1 公告类型：新
摘要：现有的面向任务的人工智能代理通常依赖于明确的指令或外部奖励，这限制了它们像人类一样受内在动机驱动的能力。在本文中，我们提出了一个欲望驱动的自主框架，以指导基于大型语言模型 (LLM) 的代理模拟类似人类的日常活动。与之前的代理相比，我们的欲望驱动的自主代理 (D2A) 以内在欲望的原则运行，允许它自主提出和选择满足其动机框架的任务。受需求理论的启发，动机框架融入了对类似人类欲望的理解，例如对社交互动、个人成就感和自我照顾的需求。利用欲望驱动的任务生成机制，代理评估其当前状态并采取一系列与其内在动机相一致的活动。通过模拟，我们证明了我们的欲望驱动自主代理 (D2A) 能够生成连贯、情境相关的日常活动，同时表现出与人类行为相似的多变性和适应性。与其他基于 LLM 的框架的比较分析表明，我们的方法显著提高了模拟活动的合理性。]]></description>
      <guid>https://arxiv.org/abs/2412.06435</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型与形式化方法的融合，打造值得信赖的人工智能代理：路线图</title>
      <link>https://arxiv.org/abs/2412.06512</link>
      <description><![CDATA[arXiv:2412.06512v1 公告类型：新
摘要：大型语言模型 (LLM) 已成为一种变革性的 AI 范式，通过其出色的语言理解和上下文生成能力深刻影响着日常生活。尽管 LLM 性能卓越，但它面临着一个关键挑战：由于其基于学习的固有局限性，容易产生不可靠的输出。另一方面，形式化方法 (FM) 是一种成熟的计算范式，它提供了数学上严格的技术来建模、指定和验证系统的正确性。FM 已广泛应用于任务关键型软件工程、嵌入式系统和网络安全。然而，阻碍 FM 在现实世界中部署的主要挑战在于其陡峭的学习曲线、缺乏用户友好的界面以及效率和适应性问题。
本立场文件概述了通过利用 LLM 和 FM 的相互增强来推进下一代可信 AI 系统的路线图。首先，我们说明 FM（包括推理和认证技术）如何帮助 LLM 生成更可靠和正式认证的输出。随后，我们重点介绍了 LLM 的高级学习能力和适应性如何显著提高现有 FM 工具的可用性、效率和可扩展性。最后，我们表明，统一这两种计算范式——将 LLM 的灵活性和智能性与 FM 的严格推理能力相结合——对可信 AI 软件系统的开发具有变革性的潜力。我们承认，这种整合有可能提高软件工程实践的可信度和效率，同时促进能够应对复杂但现实挑战的智能 FM 工具的开发。]]></description>
      <guid>https://arxiv.org/abs/2412.06512</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ProcessBench：识别数学推理中的过程错误</title>
      <link>https://arxiv.org/abs/2412.06559</link>
      <description><![CDATA[arXiv:2412.06559v2 公告类型：新
摘要：由于语言模型在解决数学问题时经常会犯错误，因此自动识别推理过程中的错误对于可扩展的监督变得越来越重要。在本文中，我们引入了 ProcessBench 来衡量识别数学推理中错误步骤的能力。它由 3,400 个测试用例组成，主要针对竞赛和奥林匹克级别的数学问题。每个测试用例都包含一个分步解决方案，错误位置由人类专家注释。模型需要识别包含错误的最早步骤，或者得出所有步骤都是正确的结论。我们对 ProcessBench 进行了广泛的评估，涉及两种类型的模型：过程奖励模型 (PRM) 和批评模型，对于后者，我们提示通用语言模型逐步批评每个解决方案。我们得出两个主要观察结果：(1) 现有的 PRM 通常无法推广到 GSM8K 和 MATH 之外更具挑战性的数学问题。它们的表现不如批评模型（即提示通用语言模型）和我们自己训练的 PRM，后者直接在 PRM800K 数据集上进行了微调。（2）最好的开源模型 QwQ-32B-Preview 已证明其批评能力可与专有模型 GPT-4o 相媲美，尽管它仍然落后于专门用于推理的 o1-mini。我们希望 ProcessBench 能够促进未来在推理过程评估方面的研究，为可扩展的语言模型监督铺平道路。]]></description>
      <guid>https://arxiv.org/abs/2412.06559</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于 LLM 代理的交通系统建模：概念框架</title>
      <link>https://arxiv.org/abs/2412.06681</link>
      <description><![CDATA[arXiv:2412.06681v1 公告类型：新
摘要：在交通系统需求建模和模拟中，基于代理的模型和微观模拟是当前最先进的方法。然而，现有的基于代理的模型在行为现实主义和资源需求方面仍然存在一些限制，限制了它们的适用性。在本研究中，利用大型语言模型 (LLM) 和基于 LLM 的代理的新兴技术，我们提出了一种用于交通系统的通用 LLM 代理建模框架。我们认为 LLM 代理不仅具备作为代理的基本能力，而且还提供了有希望的解决方案来克服现有基于代理的模型的一些限制。我们的概念框架设计紧密复制了交通网络中人类旅行者的决策和交互过程和特征，并且我们使用相关研究和 LLM 代理在瓶颈设置中学习和调整的示范性示例证明了所提出的系统可以满足决策和学习行为的关键行为标准。尽管有必要进一步完善基于 LLM 代理的建模框架，但我们相信这种方法有可能改善交通系统建模和模拟。]]></description>
      <guid>https://arxiv.org/abs/2412.06681</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>不确定性条件下多轮文本到图像生成的主动代理</title>
      <link>https://arxiv.org/abs/2412.06771</link>
      <description><![CDATA[arXiv:2412.06771v1 公告类型：新
摘要：生成式 AI 模型的用户提示通常未指定，导致响应不理想。这个问题在文本到图像 (T2I) 生成中尤为明显，用户通常难以表达他们的精确意图。用户的视觉和模型的解释之间的这种脱节常常迫使用户煞费苦心地反复改进他们的提示。为了解决这个问题，我们提出了一种主动 T2I 代理的设计，该代理配备了一个界面，可以 (1) 在不确定时主动提出澄清问题，以及 (2) 将他们对用户意图的理解呈现为用户可以编辑的可理解的信念图。我们为此类代理构建了简单的原型，并通过人类研究和自动评估验证了它们的有效性。我们观察到至少 90% 的人类受试者发现这些代理及其信念图对他们的 T2I 工作流程有帮助。此外，我们开发了一种可扩展的自动评估方法，使用两个代理，一个代理具有地面实况图像，另一个代理尝试提出尽可能少的问题以与地面实况保持一致。在我们为艺术家和设计师创建的基准 DesignBench、COCO 数据集 (Lin et al., 2014) 和 ImageInWords (Garg et al., 2024) 上，我们观察到这些 T2I 代理能够提出信息丰富的问题​​并引出关键信息以实现成功对齐，并且 VQAScore (Lin et al., 2024) 至少比标准单轮 T2I 生成高出 2 倍。演示：https://github.com/google-deepmind/proactive_t2i_agents。]]></description>
      <guid>https://arxiv.org/abs/2412.06771</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>