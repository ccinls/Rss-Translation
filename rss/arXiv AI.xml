<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.AI 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.AI 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Tue, 26 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>人机协作中没有免费午餐定理</title>
      <link>https://arxiv.org/abs/2411.15230</link>
      <description><![CDATA[arXiv:2411.15230v1 公告类型：新
摘要：人机协作的黄金标准是互补性——当综合表现超过人类和算法本身时。我们在二元分类设置中研究这一挑战，目标是最大化 0-1 准确度。给定两个或多个可以做出校准概率预测的代理，我们展示了一个“没有免费午餐”式的结果。任何确定性协作策略（将校准概率映射到二元分类的函数）本质上并不总是服从同一个代理，有时会比最不准确的代理表现更差。换句话说，互补性不能“免费”实现。结果确实提出了一种有保证的协作模型，其中一个代理识别另一个代理的“明显”错误。我们还利用结果来了解使其他协作技术成功的必要条件，为人机协作提供指导。]]></description>
      <guid>https://arxiv.org/abs/2411.15230</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>监管机构-制造商 AI 代理建模：数学反馈驱动的多代理 LLM 框架</title>
      <link>https://arxiv.org/abs/2411.15356</link>
      <description><![CDATA[arXiv:2411.15356v1 公告类型：新
摘要：全球监管机构监管更新的复杂性日益增加，这对医疗器械制造商提出了重大挑战，需要制定敏捷战略来维持合规性和保持市场准入。同时，监管机构必须有效监控制造商的反应并制定战略监控计划。本研究采用多智能体建模方法，并辅以大型语言模型 (LLM)，以模拟监管动态并检查主要参与者（包括监管机构、制造商和竞争对手）的适应性行为。这些智能体在受监管流理论支配的模拟环境中运行，捕捉监管变化对合规决策、市场适应和创新战略的影响。我们的研究结果阐明了监管变化对行业行为的影响，并确定了改善监管实践、优化合规性和促进创新的战略机会。通过利用多智能体系统和 LLM 的集成，本研究提供了一个新颖的视角，并为驾驭医疗器械行业不断变化的监管格局的利益相关者提供了可行的见解。]]></description>
      <guid>https://arxiv.org/abs/2411.15356</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在有备选工艺方案的情况下设计单元制造系统</title>
      <link>https://arxiv.org/abs/2411.15361</link>
      <description><![CDATA[arXiv:2411.15361v1 公告类型：新
摘要：在单元制造系统 (CMS) 的设计中，必须在设计和操作阶段做出许多技术和管理决策。设计 CMS 的第一步是分组零件和机器。本文提出了四个整数规划公式，用于在设计和操作层面对 CMS 中的零件和机器进行分组，以解决广义分组问题，其中每个零件都有多个工艺计划，并且工艺计划的每个操作都可以在多台机器上执行。通过将零件类型的最大可能连续操作数分别分配给同一单元和同一台机器，可以实现单元间和单元内移动的最小化。与最小化机器投资成本、运营成本等其他目标相比，最小化单元间和单元内移动作为目标的适用性进行了讨论。文中包括数值示例以说明公式的工作原理。]]></description>
      <guid>https://arxiv.org/abs/2411.15361</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在道德推理语言模型中引入类似人类的偏见</title>
      <link>https://arxiv.org/abs/2411.15386</link>
      <description><![CDATA[arXiv:2411.15386v1 公告类型：新 
摘要：在这项工作中，我们研究了针对道德推理进行微调的大型语言模型 (LLM) 在执行相同任务的人类的行为数据和/或大脑数据上的对齐 (BrainScore)。我们还探索了在执行道德推理的人类的 fMRI 数据上微调几个 LLM 是否可以提高 BrainScore。我们根据来自 ETHICS 基准 [Hendrycks et al., 2020] 的道德推理行为数据、来自 Koster-Hale et al. [2013] 的道德推理 fMRI 数据或两者对几个 LLM (BERT、RoBERTa、DeBERTa) 进行了微调。我们研究了 ETHICS 基准的准确性以及模型激活和 fMRI 数据之间的 BrainScores。虽然较大的模型通常在两个指标上都表现更好，但 BrainScores 经过微调后并没有显着改善。]]></description>
      <guid>https://arxiv.org/abs/2411.15386</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM 是否同意对替代用途的创造力评估？</title>
      <link>https://arxiv.org/abs/2411.15560</link>
      <description><![CDATA[arXiv:2411.15560v2 公告类型：新
摘要：本文研究大型语言模型 (LLM) 在评估替代用途测试 (AUT) 的创造力方面是否表现出一致性。虽然 LLM 越来越多地用于评估创造性内容，但以前的研究主要集中在评估由同一模型或人类生成的响应的单一模型上。本文探讨 LLM 是否能够公正准确地评估由自身和其他模型生成的输出中的创造力。使用一组按创造力水平（普通、创造性和高度创造性）分类的 AUT 响应的 oracle 基准集，我们尝试使用四个最先进的 LLM 来评估这些输出。我们测试了评分和排名方法，并采用两种评估设置（综合和分段）来检查 LLM 是否同意替代用途的创造力评估。结果显示模型间一致性很高，Spearman 相关性在各个模型间平均高于 0.7，与 oracle 相比达到 0.77 以上，表明一致性很高，并验证了 LLM 在替代用途创造力评估中的可靠性。值得注意的是，模型并不偏向自己的反应，而是为其他模型生成的替代用途提供类似的创造力评估分数或排名。这些发现表明 LLM 在创造力评估中表现出公正性和高度一致性，为其在自动化创造力评估中的应用提供了有希望的意义。]]></description>
      <guid>https://arxiv.org/abs/2411.15560</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>协调人类与机器之间的泛化</title>
      <link>https://arxiv.org/abs/2411.15626</link>
      <description><![CDATA[arXiv:2411.15626v1 公告类型：新
摘要：人工智能的最新进展（包括生成方法）已经产生了可以支持人类进行科学发现和决策支持的技术，但也可能破坏民主制度并针对个人。负责任地使用人工智能越来越表明需要人机合作，需要人机之间进行有效的交互。这些交互中一个至关重要但经常被忽视的方面是人机泛化的不同方式。在认知科学中，人类泛化通常涉及抽象和概念学习。相比之下，人工智能泛化包括机器学习中的域外泛化、符号人工智能中的基于规则的推理和神经符号人工智能中的抽象。在这篇观点论文中，我们结合人工智能和认知科学的见解，从三个维度确定关键的共性和差异：泛化概念、泛化方法和泛化评估。我们沿着这三个维度映射人工智能和认知科学中泛化的不同概念，并考虑它们在人机合作中的作用。这导致了人工智能和认知科学之间的跨学科挑战，必须解决这些挑战才能为人机合作场景中有效且认知支持的协调奠定基础。]]></description>
      <guid>https://arxiv.org/abs/2411.15626</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>TableTime：通过大型语言模型将时间序列分类重新表述为零样本表理解</title>
      <link>https://arxiv.org/abs/2411.15737</link>
      <description><![CDATA[arXiv:2411.15737v1 公告类型：新
摘要：大型语言模型 (LLM) 已证明其在多变量时间序列分类 (MTSC) 中的有效性。有效适应 MTSC 的 LLM 需要信息丰富的数据表示。现有的基于 LLM 的方法直接从头开始对 LLM 潜在空间中的时间序列嵌入进行编码，以与 LLM 的语义空间对齐。尽管这些方法很有效，但我们发现它们隐藏了三个固有的瓶颈：(1) 它们难以以无损方式编码时间和通道特定信息，这两者都是多变量时间序列的关键组成部分；(2) 很难将学习到的表示空间与 LLM 的语义空间对齐；(3) 它们需要针对特定​​任务的再训练，这既耗费计算资源又耗费人力。为了弥补这些差距，我们提出了 TableTime，它将 MTSC 重新表述为表格理解任务。具体来说，TableTime 引入了以下策略：（1）将多变量时间序列转换为表格形式，从而最大程度地减少信息损失；（2）以文本格式表示表格时间序列，以实现与 LLM 语义空间的自然对齐；（3）设计一个集成上下文文本信息、邻域辅助、多路径推理和问题分解的推理框架，以增强 LLM 的推理能力并实现零样本分类。在 UEA 档案中的 10 个公开代表性数据集上进行的大量实验验证了 TableTime 的优越性。]]></description>
      <guid>https://arxiv.org/abs/2411.15737</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>解读城市工业复杂性：通过 IndustryScopeGPT 增强知识驱动的洞察力</title>
      <link>https://arxiv.org/abs/2411.15758</link>
      <description><![CDATA[arXiv:2411.15758v1 公告类型：新
摘要：工业园区对城市经济增长至关重要。然而，它们的发展经常遇到源于工业需求和城市服务之间不平衡的挑战，这凸显了战略规划和运营的必要性。本文介绍了 IndustryScopeKG，这是一种开创性的大规模多模式、多层次工业园区知识图谱，它整合了包括街景、企业、社会经济和地理空间信息在内的各种城市数据，捕捉了工业园区内复杂的关系和语义。除此之外，我们还提出了 IndustryScopeGPT 框架，该框架利用大型语言模型 (LLM) 和蒙特卡洛树搜索来增强工业园区规划和运营 (IPPO) 中的工具增强推理和决策。我们的工作显着改善了站点推荐和功能规划，展示了将 LLM 与结构化数据集相结合以推进工业园区管理的潜力。这种方法为智能 IPPO 研究树立了新的标杆，并为推进城市工业发展奠定了坚实的基础。数据集和相关代码可在https://github.com/Tongji-KGLLM/IndustryScope获取。]]></description>
      <guid>https://arxiv.org/abs/2411.15758</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>创建可扩展的AGI：开放式通用智能框架</title>
      <link>https://arxiv.org/abs/2411.15832</link>
      <description><![CDATA[arXiv:2411.15832v1 公告类型：新
摘要：本文介绍了一种新型通用人工智能系统架构，该架构提供了通用灵活性并解决了当前困扰该领域的可扩展性问题。该架构 OGI（开放式通用智能）利用动态处理系统来控制和委托专门的人工智能模块。它旨在用作智能系统的参考设计，为各种实际应用中的通用人工智能提供类似人类的认知灵活性。]]></description>
      <guid>https://arxiv.org/abs/2411.15832</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PIANIST：利用法学硕士 (LLM) 学习部分可观察的世界模型，实现多智能体决策</title>
      <link>https://arxiv.org/abs/2411.15998</link>
      <description><![CDATA[arXiv:2411.15998v1 公告类型：新
摘要：有效提取 LLM 中的世界知识以用于复杂的决策任务仍然是一个挑战。我们提出了一个框架 PIANIST，用于将世界模型分解为七个直观的组件，有利于零样本 LLM 生成。仅给定游戏的自然语言描述以及输入观察的格式，我们的方法就可以生成一个有效的世界模型，以进行快速高效的 MCTS 模拟。我们表明，我们的方法在两种不同的游戏中效果很好，这些游戏挑战了代理在基于语言和非语言的行动方面的规划和决策技能，而无需对特定领域的训练数据或明确定义的世界模型进行任何训练。]]></description>
      <guid>https://arxiv.org/abs/2411.15998</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>智能体为何做出该决定：使用视觉蒙版解释深度强化学习</title>
      <link>https://arxiv.org/abs/2411.16120</link>
      <description><![CDATA[arXiv:2411.16120v1 公告类型：新
摘要：由于深度神经网络本身缺乏透明度，深度强化学习 (DRL) 代理很难获得用户的信任和接受，尤其是在医疗诊断和军事行动等安全关键应用中。现有的解释代理决策的方法要么需要使用支持解释生成的模型重新训练代理，要么依靠基于扰动的技术来揭示不同输入特征在决策过程中的重要性。然而，重新训练代理可能会损害其完整性和性能，而基于扰动的方法性能有限，缺乏知识积累或学习能力。此外，由于每个扰动都是独立执行的，因此扰动输入的联合状态可能没有物理意义。为了应对这些挑战，我们引入了 $\textbf{VisionMask}$，这是一个独立的解释模型，经过端到端训练，可以识别代理视觉输入中可以解释其行为的最关键区域。 VisionMask 以自监督的方式进行训练，不依赖于人工生成的标签。重要的是，它的训练不会改变代理模型，因此可以保留代理的性能和完整性。我们在超级马里奥兄弟 (SMB) 和三款 Atari 游戏上评估了 VisionMask。与现有方法相比，VisionMask 在根据所选视觉解释重现原始动作时实现了 14.9% 更高的插入准确率和 30.08% 更高的 F1 分数。我们还提供了示例来说明如何使用 VisionMask 进行反事实分析。]]></description>
      <guid>https://arxiv.org/abs/2411.16120</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过条件模仿共同学习实现自动驾驶汽车的端到端转向</title>
      <link>https://arxiv.org/abs/2411.16131</link>
      <description><![CDATA[arXiv:2411.16131v1 公告类型：新
摘要：自动驾驶涉及复杂的任务，例如数据融合、物体和车道检测、行为预测和路径规划。与将各个子系统专用于处理每个任务的模块化方法相反，端到端方法使用深度神经网络将问题视为单个可学习的任务，从而降低系统复杂性并最大限度地减少对启发式方法的依赖。条件模仿学习 (CIL) 训练端到端模型以模仿人类专家考虑引导车辆到达目的地的导航命令，CIL 采用专门用于学习每个导航命令的驾驶任务的专家网络分支。然而，当部署到看不见的环境时，CIL 模型缺乏泛化。这项工作引入了条件模仿共同学习 (CIC) 方法来解决这个问题，通过使模型能够通过由门控双曲正切单元 (GTU) 生成的共同学习矩阵来学习 CIL 专家分支之间的关系。此外，我们建议将转向回归问题作为分类问题，使用分类-回归混合损失来弥合回归和分类之间的差距，我们还建议使用共存概率来考虑转向类别之间的空间趋势。与 CIL 方法相比，我们的模型在看不见的环境中平均将自动驾驶成功率提高了 62%。]]></description>
      <guid>https://arxiv.org/abs/2411.16131</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过第三方 LLM 集成增强多智能体共识：分析大型语言模型中的不确定性并减轻幻觉</title>
      <link>https://arxiv.org/abs/2411.16189</link>
      <description><![CDATA[arXiv:2411.16189v1 Announce Type: new 
摘要：大型语言模型（LLM）在处理复杂推理任务时仍面临挑战，经常导致幻觉现象，限制了LLM的实际应用。为了缓解这一问题，本文提出了一种新方法，通过整合不同的LLM来扩展知识边界，减少对单一模型的依赖，并促进代理之间的深入讨论。主要贡献包括：1）引入第三方LLM通过不确定性估计和置信度分析来调整代理的注意力权重，优化多代理系统中的共识形成；2）在算术数据集上的实验验证了该方法的有效性，超越了传统的多代理基线。这项研究为大型模型在处理复杂任务时缓解幻觉现象提供了一个新的视角。]]></description>
      <guid>https://arxiv.org/abs/2411.16189</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>探索机器意识</title>
      <link>https://arxiv.org/abs/2411.16262</link>
      <description><![CDATA[arXiv:2411.16262v1 公告类型：新
摘要：本研究探索了人工智能体发展核心意识的潜力，正如安东尼奥·达马西奥的意识理论所提出的。根据达马西奥的说法，核心意识的出现依赖于由情绪和感觉的表征形成的自我模型和世界模型的整合。我们假设，通过虚拟环境中的强化学习 (RL) 训练的人工智能体可以在其主要任务的副产品中开发出这些模型的初步形式。代理的主要目标是学习玩电子游戏和探索环境。为了评估世界和自我模型的出现，我们使用了探测前馈分类器，它使用训练有素的代理神经网络的激活来预测代理本身的空间位置。我们的结果表明，代理可以形成基本的世界和自我模型，这为发展机器意识提供了一条途径。这项研究为人工智能体在反映人类意识方面的能力提供了基础见解，对未来人工智能的发展具有重要意义。]]></description>
      <guid>https://arxiv.org/abs/2411.16262</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CATP-LLM：为成本感知工具规划提供大型语言模型支持</title>
      <link>https://arxiv.org/abs/2411.16313</link>
      <description><![CDATA[arXiv:2411.16313v1 公告类型：新 
摘要：利用大型语言模型 (LLM) 进行工具规划已成为开发通用 AI 系统的一种有前途的途径，其中 LLM 会自动安排外部工具（例如视觉模型）根据任务描述处理复杂任务。为了将这种范式推向实际应用，LLM 必须考虑工具执行成本（例如执行时间）以进行工具规划。不幸的是，先前的研究忽视了工具执行成本，导致生成昂贵的计划，其成本超过了任务性能。为了填补这一空白，我们提出了使用 LLM 的成本感知工具规划 (CATP-LLM) 框架，该框架首次提供了一个连贯的设计，使 LLM 能够进行成本感知的工具规划。具体而言，CATP-LLM 结合了一种工具规划语言来增强 LLM 生成多个分支的非连续计划的能力，从而实现高效的并发工具执行并降低成本。此外，它还设计了一种成本感知的离线强化学习算法来微调 LLM，以优化工具规划中的性能成本权衡。在缺乏公共成本相关数据集的情况下，我们进一步提出了第一个成本感知规划评估平台 OpenCATP。在 OpenCATP 上的实验表明，即使在使用 Llama2-7B 作为骨干时，CATP-LLM 的表现也优于 GPT-4，即使在具有挑战性的规划任务上，规划性能平均提高了 28.2%-30.2%，成本降低了 24.7%-45.8%。CATP-LLM 和 OpenCATP 的代码将公开提供。]]></description>
      <guid>https://arxiv.org/abs/2411.16313</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从生成到判断：法学硕士作为法官的机遇与挑战</title>
      <link>https://arxiv.org/abs/2411.16594</link>
      <description><![CDATA[arXiv:2411.16594v1 公告类型：新
摘要：评估和评价长期以来一直是人工智能 (AI) 和自然语言处理 (NLP) 中的关键挑战。然而，传统方法，无论是基于匹配还是基于嵌入，往往无法判断细微属性并提供令人满意的结果。大型语言模型 (LLM) 的最新进展激发了“LLM-as-a-judge”范式，其中 LLM 被用于在各种任务和应用程序中执行评分、排名或选择。本文对基于 LLM 的判断和评估进行了全面的调查，为推动这一新兴领域的发展提供了深入的概述。我们首先从输入和输出的角度给出详细的定义。然后，我们引入了一个全面的分类法，从三个维度探索 LLM-as-a-judge：判断什么、如何判断和在哪里判断。最后，我们编制了评估 LLM-as-a-judge 的基准，并强调了主要挑战和有希望的方向，旨在提供有价值的见解并启发这一有前途的研究领域的未来研究。有关 LLM-as-a-judge 的论文列表和更多资源可在 \url{https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge} 和 \url{https://llm-as-a-judge.github.io} 找到。]]></description>
      <guid>https://arxiv.org/abs/2411.16594</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>F——基于基础本体的事件模型DOLCE+DnS Ultralite</title>
      <link>https://arxiv.org/abs/2411.16609</link>
      <description><![CDATA[arXiv:2411.16609v1 公告类型：新
摘要：缺乏事件的形式化模型阻碍了分布式基于事件的系统中互操作性。在本文中，我们提出了一种事件的形式化模型，称为 Event-Model-F。该模型基于基础本体 DOLCE+DnS Ultralite (DUL)，为表示时间和空间、对象和人员以及事件之间的部分、因果和相关关系提供了全面的支持。此外，Event-Model-F 为事件组合、事件因果关系和事件相关性建模以及表示同一事件的不同解释提供了一种灵活的方法。Event-Model-F 是按照 DUL 的面向模式的方法开发的，在不同的本体中模块化，并且可以通过特定领域的本体轻松扩展。]]></description>
      <guid>https://arxiv.org/abs/2411.16609</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>dafny-annotator：Dafny 程序的 AI 辅助验证</title>
      <link>https://arxiv.org/abs/2411.15143</link>
      <description><![CDATA[arXiv:2411.15143v1 公告类型：交叉 
摘要：形式化验证有可能大幅减少软件错误，但其高额的额外成本阻碍了大规模采用。虽然 Dafny 有望大幅减少编写经过验证的程序的工作量，但用户通常需要提供逻辑注释来帮助验证者。在这里，我们探索使用大型语言模型和搜索的组合来构建 dafny-annotator：一种向 Dafny 方法添加逻辑注释的工具，直到验证者能够证明它是正确的。在 DafnyBench 程序集的测试集上，由 LLaMa 3.1 8B 指导的贪婪搜索仅成功注释了 15.7% 的方法。由于缺乏大规模训练数据，这种数据驱动方法受到阻碍，我们提出了一种在灵活的管道中开放式合成新 Dafny 程序的方法，其中 LLM 制定高级想法，实施它们，并逐步提出对现有程序的更改，Dafny 会对其进行验证。这为我们提供了一个合成数据集 DafnySynth，我们用它来增强 DafnyBench 的训练。对两个数据集进行微调将 LLaMa 8B 的成功率提高到 50.6%——明显优于基础模型或单独对任一数据集进行训练。我们的结果表明，对于尚未拥有大规模人工生成示例的语言，可以开发出功能强大的 AI 助手。反过来，这样的助手可能会减少用户的摩擦，并最终推动采用。]]></description>
      <guid>https://arxiv.org/abs/2411.15143</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>将责任委托给智能自主系统：挑战和好处</title>
      <link>https://arxiv.org/abs/2411.15147</link>
      <description><![CDATA[arXiv:2411.15147v1 公告类型：交叉 
摘要：随着人工智能系统越来越具有自主性和适应性，技术社会系统中道德责任的传统界限正受到挑战。本文探讨了将责任委托给智能自主代理的不断发展的论述以及此类实践的伦理含义。综合人工智能伦理学的最新发展，包括分布式责任和道德人工智能设计的概念，本文提出了一个功能主义观点作为框架。这种观点认为道德责任不是个人特质，而是社会技术系统中的角色，分布在人类和人工智能代理之间。作为“人工智能道德设计”的一个例子，我们介绍了 Basti 和 Vitiello 的实现。他们认为，人工智能可以通过学习道德准则并使用道义高阶逻辑来道德评估决策，从而充当人工智能道德代理。考虑到人工智能可能超越人类监督的速度和规模以及伦理影响，本文主张“人工智能伦理设计”，同时承认责任的分布式、共享性和动态性。这种功能主义方法提供了一个实用框架，用于在快速发展的技术环境中应对人工智能伦理的复杂性。]]></description>
      <guid>https://arxiv.org/abs/2411.15147</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>《人工智能法案》中的基本权利影响评估 (FRIA)：模型模板的根源、法律义务和关键要素</title>
      <link>https://arxiv.org/abs/2411.15149</link>
      <description><![CDATA[arXiv:2411.15149v1 公告类型：交叉 
摘要：在《人工智能法案》中，什么背景导致了进行基本权利影响评估 (FRIA) 的义务？欧盟立法者如何在《人工智能法案》中制定对基本权利影响的评估？在制定 FRIA 时应遵循哪些方法标准？这些是本文旨在通过对《人工智能法案》相关条款的法律分析和讨论评估人工智能对基本权利影响的各种可能模型来解决的三个主要研究问题。本文的总体目标是填补《人工智能法案》中概述的 FRIA 理论和方法阐述的现有空白。为了促进欧盟和国家机构以及人工智能运营商未来的工作，将这一以人为本和值得信赖的人工智能关键工具置于欧盟人工智能设计和开发方法的核心，本文概述了 FRIA 模型模板的主要构建块。虽然该提议符合《人工智能法案》的理由和范围，但它也适用于第 27 条所列的情况之外，并可作为其他国家和国际监管举措的蓝图，以确保人工智能完全符合人权。]]></description>
      <guid>https://arxiv.org/abs/2411.15149</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>