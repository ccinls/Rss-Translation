<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.AI 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.AI 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Thu, 24 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>遥感视觉语言模型的进展：数据集、功能和增强技术</title>
      <link>https://arxiv.org/abs/2410.17283</link>
      <description><![CDATA[arXiv:2410.17283v1 公告类型：新
摘要：最近，ChatGPT 的显著成功引发了人们对人工智能 (AI) 的新一轮兴趣，而视觉语言模型 (VLM) 的进步将这种热情推向了新的高度。与以前的 AI 方法通常将不同的任务制定为判别模型不同，VLM 将任务构建为生成模型，并将语言与视觉信息对齐，从而能够处理更具挑战性的问题。遥感 (RS) 领域是一个高度实用的领域，它也接受了这一新趋势，并引入了几种基于 VLM 的 RS 方法，这些方法表现出良好的性能和巨大的潜力。在本文中，我们首先回顾了与 VLM 相关的基本理论，然后总结了为遥感中的 VLM 构建的数据集及其处理的各种任务。最后，我们根据 VLM 的核心组件将改进方法分为三个主要部分，并对这些方法进行了详细的介绍和比较。]]></description>
      <guid>https://arxiv.org/abs/2410.17283</guid>
      <pubDate>Thu, 24 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>文献与数据：假设生成的协同方法</title>
      <link>https://arxiv.org/abs/2410.17309</link>
      <description><![CDATA[arXiv:2410.17309v1 公告类型：新
摘要：人工智能有望改变科学过程，包括假设生成。先前关于假设生成的工作可以大致分为理论驱动和数据驱动的方法。虽然两者都被证明能够有效地生成新颖且合理的假设，但它们是否可以相互补充仍是一个悬而未决的问题。为了解决这个问题，我们开发了第一种将基于文献的见解与数据相结合以执行 LLM 驱动的假设生成的方法。我们将我们的方法应用于五个不同的数据集，并证明整合文献和数据的效果优于其他基线（比少样本高 8.97\%，比仅基于文献高 15.75\%，比仅基于数据驱动高 3.37\%）。此外，我们进行了第一次人工评估，以评估 LLM 生成的假设在协助人类决策两个具有挑战性的任务中的效用：欺骗检测和 AI 生成的内容检测。我们的结果表明，人类在这些任务上的准确率分别显著提高了 7.44% 和 14.19%。这些发现表明，整合基于文献和数据驱动的方法可以为假设生成提供一个全面而细致的框架，并可能为科学探究开辟新的途径。]]></description>
      <guid>https://arxiv.org/abs/2410.17309</guid>
      <pubDate>Thu, 24 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型是否已为旅行规划做好准备？</title>
      <link>https://arxiv.org/abs/2410.17333</link>
      <description><![CDATA[arXiv:2410.17333v1 公告类型：新
摘要：虽然大型语言模型 (LLM) 在酒店和旅游业中前景光明，但它们是否能够为不同人口群体提供公正的服务仍不清楚。本文探讨了当 LLM 用作旅行计划助手时出现的性别和种族偏见。为了调查这个问题，我们应用机器学习技术来分析三个开源 LLM 生成的旅行建议。我们的研究结果表明，种族和性别分类器的表现大大超过了随机机会，这表明 LLM 与不同子群体的互动方式存在差异。具体而言，输出符合与某些种族和性别相关的文化期望。为了尽量减少这些刻板印象的影响，我们使用了一种停用词分类策略，减少了可识别的差异，没有发现不尊重的术语。然而，注意到与非裔美国人和性别少数群体有关的幻觉。总之，虽然 LLM 可以生成看似没有偏见的旅行计划，但验证其建议的准确性和适当性仍然至关重要。]]></description>
      <guid>https://arxiv.org/abs/2410.17333</guid>
      <pubDate>Thu, 24 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FairLoRA：利用公平性驱动的低秩自适应技术缓解视觉模型中的偏差</title>
      <link>https://arxiv.org/abs/2410.17358</link>
      <description><![CDATA[arXiv:2410.17358v1 公告类型：新摘要：低秩自适应 (LoRA) 等参数高效微调方法的最新进展因其能够有效地将大型基础模型适应各种下游任务而备受关注。这些方法因在聚合级指标上实现与完全微调相当的性能而受到赞赏，同时显着降低了计算成本。为了系统地解决 LLM 中的公平性问题，先前的研究使用比通常使用的更大的 LoRA 等级对公平性特定数据进行微调。在本文中，我们介绍了 FairLoRA，这是一种新颖的公平性特定正则化器，用于 LoRA，旨在通过最小化每个类的损失方差来减少数据子组之间的性能差异。据我们所知，我们是第一个通过 LoRA 引入基于公平性的微调的人。我们的结果表明，对更高等级来减轻偏见的需求并不是普遍的；它取决于预训练模型、数据集和任务等因素。更重要的是，我们在分布变化场景中系统地评估了各种视觉模型（包括 ViT、DiNO 和 CLIP）中的 FairLoRA。我们进一步强调了使用多个公平性指标来获得公平性整体评估的必要性，而不是仅仅依赖于训练期间优化的指标。]]></description>
      <guid>https://arxiv.org/abs/2410.17358</guid>
      <pubDate>Thu, 24 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DeLLiriuM：使用结构化 EHR 预测 ICU 中谵妄症的大型语言模型</title>
      <link>https://arxiv.org/abs/2410.17363</link>
      <description><![CDATA[arXiv:2410.17363v1 公告类型：新
摘要：谵妄是一种急性精神错乱状态，据证明，重症监护病房 (ICU) 中多达 31% 的患者受其影响。及早发现这种情况可以更及时地进行干预并改善健康结果。虽然人工智能 (AI) 模型已显示出使用结构化电子健康记录 (EHR) 预测 ICU 谵妄的巨大潜力，但其中大多数尚未探索使用最先进的 AI 模型，仅限于单个医院，或已在小群体中开发和验证。使用大型语言模型 (LLM)（具有数亿到数十亿个参数的模型）和结构化的 EHR 数据可能会提高预测性能。在这项研究中，我们提出了 DeLLiriuM，这是一种基于 LLM 的新型谵妄预测模型，使用 ICU 入院前 24 小时内可用的 EHR 数据来预测患者在其余 ICU 入院期间发生谵妄的概率。我们开发了 DeLLiriuM，并针对来自 195 家医院的 104,303 名重症监护患者在三个大型数据库中进行了验证：eICU 协作研究数据库、重症监护医学信息集市 (MIMIC)-IV 和佛罗里达大学健康综合数据存储库。通过受试者工作特征曲线下面积 (AUROC) 测量的性能表明，DeLLiriuM 在两个外部验证集中的表现优于所有基线，在 194 家医院的 77,543 名患者中，其得分为 0.77（95% 置信区间为 0.76-0.78）和 0.84（95% 置信区间为 0.83-0.85）。据我们所知，DeLLiriuM 是第一个基于结构化 EHR 数据的 ICU 谵妄预测工具，其表现优于采用结构化特征的深度学习基线，可以为临床医生提供有用的信息以便及时干预。]]></description>
      <guid>https://arxiv.org/abs/2410.17363</guid>
      <pubDate>Thu, 24 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>驾驭嘈杂反馈：使用易错语言模型增强强化学习</title>
      <link>https://arxiv.org/abs/2410.17389</link>
      <description><![CDATA[arXiv:2410.17389v1 公告类型：新
摘要：正确指定奖励模型是强化学习中众所周知的挑战。手工制作的奖励函数通常会导致低效或次优的策略，并且可能与用户价值观不一致。从人类反馈中进行强化学习是一种可以缓解此类问题的成功技术，但是，收集人类反馈可能很费力。最近的研究已经从预先训练的大型语言模型而不是人类那里征求反馈，以减少或消除人类的努力，然而，这些方法在存在幻觉和其他错误的情况下会产生糟糕的表现。本文研究了从大型语言模型反馈中进行强化学习的优势和局限性，并提出了一种简单而有效的方法来征求和应用反馈作为基于潜力的塑造函数。我们从理论上表明，使用我们的方法，不一致的排名（近似排名错误）会导致不具指导性的奖励。我们的方法通过经验提高了收敛速度和策略回报，即使存在显著的排名错误，也比常用的基线更高，并且无需对奖励函数进行复杂的后处理。]]></description>
      <guid>https://arxiv.org/abs/2410.17389</guid>
      <pubDate>Thu, 24 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>重新审视技术偏见缓解策略</title>
      <link>https://arxiv.org/abs/2410.17433</link>
      <description><![CDATA[arXiv:2410.17433v1 公告类型：新
摘要：人工智能 (AI) 社区减轻偏见和提高公平性的努力主要集中在技术解决方案上。虽然许多评论都涉及人工智能中的偏见，但这篇评论独特地关注医疗环境中技术解决方案的实际局限性，对影响其实际实施的五个关键维度进行了结构化分析：谁定义偏见和公平；在数十种不一致和不兼容的缓解策略中使用和优先考虑哪种缓解策略；在人工智能开发阶段，解决方案何时最有效；针对哪些人群；以及设计解决方案的背景。我们通过专注于医疗保健和生物医学应用的实证研究来说明每个限制。此外，我们讨论了价值敏感的人工智能（一种源自技术设计的框架）如何吸引利益相关者并确保他们的价值观体现在偏见和公平缓解解决方案中。最后，我们讨论了需要进一步调查的领域，并提供了解决研究中涉及的局限性的实用建议。]]></description>
      <guid>https://arxiv.org/abs/2410.17433</guid>
      <pubDate>Thu, 24 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 LLM 解码时间序列：跨域注释的多代理框架</title>
      <link>https://arxiv.org/abs/2410.17462</link>
      <description><![CDATA[arXiv:2410.17462v1 公告类型：新
摘要：时间序列数据在各个领域无处不在，包括制造业、金融业和医疗保健业。高质量的注释对于有效理解时间序列和促进下游任务至关重要；然而，获得这样的注释具有挑战性，特别是在关键任务领域。在本文中，我们提出了 TESSA，这是一个多代理系统，旨在自动生成时间序列数据的通用和领域特定注释。TESSA 引入了两个代理：通用注释代理和领域特定注释代理。通用代理捕获多个源域中的常见模式和知识，利用时间序列和文本特征来生成通用注释。同时，领域特定代理利用来自目标域的有限注释来学习领域特定术语并生成有针对性的注释。在多个合成和真实世界数据集上进行的大量实验表明，TESSA 可以有效地生成高质量的注释，优于现有方法。]]></description>
      <guid>https://arxiv.org/abs/2410.17462</guid>
      <pubDate>Thu, 24 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>人工智能、全球治理和数字主权</title>
      <link>https://arxiv.org/abs/2410.17481</link>
      <description><![CDATA[arXiv:2410.17481v1 公告类型：新
摘要：本文探讨了人工智能 (AI) 系统如何通过影响全球管理者如何行使权力和追求数字主权而成为国际事务中不可或缺的一部分。我们首先介绍了人工智能为政府和企业带来的多方面收益的分类，这些收益与暴力、市场和权利领域的工具性、结构性和话语权有关。接下来，我们利用不同的制度和实践视角来评估数字主权如何以不同的方式涉及人工智能赋权的全球治理。各国都在制度方法中寻求对人工智能基础设施的主权控制，而在实践方法中则通过人工智能基础设施建立主权能力。总的来说，我们将人工智能的数字主权利益与公私权力的纠缠联系起来。我们并不认为科技公司会取代国家，而是认为人工智能系统将嵌入全球治理中，创造公私合作与竞争的对立态势。最后，我们概述了人工智能和全球治理领域的 IR 研究的未来方向。]]></description>
      <guid>https://arxiv.org/abs/2410.17481</guid>
      <pubDate>Thu, 24 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Transformer 网络中上下文学习的符号处理机制</title>
      <link>https://arxiv.org/abs/2410.17498</link>
      <description><![CDATA[arXiv:2410.17498v1 公告类型：新
摘要：大型语言模型 (LLM) 通过上下文学习 (ICL) 展示了令人印象深刻的符号处理能力。这一成功与数十年来人工神经网络无法掌握抽象符号操作的预测背道而驰。我们试图了解能够在变压器网络中实现稳健符号处理的机制，阐明变压器在符号处理方面的意外成功和重大局限性。借鉴符号 AI 对生产系统架构强大功能的见解，我们开发了一种高级语言 PSL，它允许我们编写符号程序来执行复杂、抽象的符号处理，并创建编译器，精确实现变压器网络中的 PSL 程序，这些程序在构造上是 100% 机械可解释的。我们证明 PSL 是图灵通用的，因此这项工作可以促进对变压器 ICL 的总体理解。我们从 PSL 程序编译的变压器架构类型提出了多种增强变压器符号处理能力的途径。 （注：论文第一部分对整篇论文进行了扩展概要。）]]></description>
      <guid>https://arxiv.org/abs/2410.17498</guid>
      <pubDate>Thu, 24 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过神经网络学习公平且可取的分配</title>
      <link>https://arxiv.org/abs/2410.17500</link>
      <description><![CDATA[arXiv:2410.17500v1 公告类型：新
摘要：不可分割资源的公平分配是一个基本问题。现有研究已经开发了各种分配机制或算法来满足不同的公平概念。例如，提出了循环 (RR) 来满足公平标准，即无嫉妒性 (EF1)。现实世界的资源分配问题中使用没有数学公式的专家算法来为用户找到更好的结果。因此，我们的目标是设计严格满足良好属性的机制，并复制专家知识。然而，这个问题很有挑战性，因为这种启发式规则通常很难用数学形式化，这使得它们很难融入理论框架。此外，形式化算法很难找到更好的结果，而直接复制这些隐式规则可能会导致不公平的分配，因为人类的决策可能会引入偏见。在本文中，我们旨在从示例中学习隐式分配机制，同时严格满足公平性约束，具体侧重于通过对报告估值示例和隐式规则产生的相应分配结果进行监督学习来学习 EF1 分配机制。为了解决这个问题，我们开发了一种神经 RR (NRR)，这是一种参数化 RR 的新型神经网络。NRR 由 RR 的可微松弛构建而成，可以训练以学习用于 RR 的代理排序。我们进行了实验以从示例中学习 EF1 分配机制，表明我们的方法在预测分配的接近度和其他指标方面优于基线。]]></description>
      <guid>https://arxiv.org/abs/2410.17500</guid>
      <pubDate>Thu, 24 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于本体论的方法，用于以用户为中心、以知识为基础的人工智能系统解释</title>
      <link>https://arxiv.org/abs/2410.17504</link>
      <description><![CDATA[arXiv:2410.17504v1 公告类型：新
摘要：可解释人工智能 (AI) 专注于帮助人类理解 AI 系统或其决策的工作原理，几十年来一直是 AI 的基石。可解释性方面的最新研究主要集中在解释 AI 模型或模型可解释性的工作原理。也有几份立场声明和评论论文详细说明了最终用户对以用户为中心的可解释性的需求，但实现的较少。因此，本论文旨在弥合模型和以用户为中心的可解释性之间的一些差距。我们创建了一个解释本体 (EO)，通过其支持组件来表示文献衍生的解释类型。我们实现了知识增强问答 (QA) 管道，以支持临床环境中的上下文解释。最后，我们正在实施一个系统来结合来自不同 AI 方法和数据模式的解释。在 EO 中，我们可以表示十五种不同的解释类型，并且我们已经在六个示例用例中测试了这些表示。我们发现知识增强可以提高基础大型语言模型在情境化问答中的性能，并且性能在不同的疾病组中有所不同。在同样的环境中，临床医生还表示他们更愿意将可操作性视为解释的主要焦点之一。在我们的解释组合方法中，我们计划使用相似性指标来确定慢性病检测环境中解释的相似性。总的来说，通过这篇论文，我们设计了可以在不同用例中支持知识支持的解释的方法，考虑到当今人工智能时代可以生成这些解释的支持组件的方法和可以增强它们的领域知识源。]]></description>
      <guid>https://arxiv.org/abs/2410.17504</guid>
      <pubDate>Thu, 24 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FairDgcl：基于动态图对比学习的公平感知推荐</title>
      <link>https://arxiv.org/abs/2410.17555</link>
      <description><![CDATA[arXiv:2410.17555v1 公告类型：新
摘要：随着可信人工智能的不断发展，推荐中的公平性问题受到越来越多的关注。当推荐系统根据用户敏感属性（例如年龄、性别）为不同用户组产生不平等的结果时，它被认为是不公平的。一些研究人员提出了基于数据增强的方法，旨在通过改变不同用户组之间训练数据的倾斜分布来缓解用户级不公平。尽管取得了有希望的结果，但它们通常依赖于可能与现实不符的公平相关假设，从而可能降低数据质量并对模型有效性产生负面影响。为了解决这个问题，在本文中，我们研究如何实现高质量的数据增强以提高推荐公平性。具体来说，我们提出了 FairDgcl，这是一个动态图对抗对比学习框架，旨在提高推荐系统的公平性。首先，FairDgcl 开发了一个具有视图生成器和视图鉴别器的对抗性对比网络，以学习以对抗方式生成公平增强策略。然后，我们提出了两个动态、可学习的模型，用于在对比学习框架内生成对比视图，从而自动微调增强策略。同时，我们从理论上证明了 FairDgcl 可以同时生成兼具公平性和准确性的增强表示。最后，在四个真实数据集上进行的全面实验证明了所提出的 FairDgcl 的有效性。]]></description>
      <guid>https://arxiv.org/abs/2410.17555</guid>
      <pubDate>Thu, 24 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CLR-Bench：评估大学级推理中的大型语言模型</title>
      <link>https://arxiv.org/abs/2410.17558</link>
      <description><![CDATA[arXiv:2410.17558v1 公告类型：新
摘要：大型语言模型 (LLM) 在各种语言理解任务中都表现出色。虽然已经提出了新兴基准来评估数学和计算机科学等各个领域的 LLM，但它们仅仅衡量了多项选择题最终预测的准确性。然而，这仍然不足以验证给定选择时对 LLM 的基本理解。为了填补这一空白，我们提出了 CLR-Bench 来全面评估 LLM 在复杂的大学水平推理中的表现。具体来说，(i) 我们优先考虑计算机科学和人工智能领域的 16 个具有挑战性的大学学科。数据集包含 5 种类型的问题，而每个问题都与专家的详细解释相关。(ii) 为了量化对 LLM 推理能力的公平评估，我们用两个新颖的指标形式化了标准。 Q$\rightarrow$A 用于衡量直接答案预测的性能，而 Q$\rightarrow$AR 则有效地考虑了同时回答问题和提供理由的联合能力。对 40 名 LLM 进行了 1,018 个特定学科问题的广泛实验。结果表明，LLM，即使是最好的闭源 LLM，即 GPT-4 turbo，也倾向于“猜测”大学水平的答案。它显示准确率从 63.31% Q$\rightarrow$A 急剧下降到 39.00% Q$\rightarrow$AR，表明推理能力不令人满意。]]></description>
      <guid>https://arxiv.org/abs/2410.17558</guid>
      <pubDate>Thu, 24 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ImDy：通过模仿观察了解人类逆动力学</title>
      <link>https://arxiv.org/abs/2410.17610</link>
      <description><![CDATA[arXiv:2410.17610v1 公告类型：新
摘要：逆动力学 (ID) 旨在从人体运动学观察中重现驱动扭矩，已成为步态分析的重要工具。然而，由于其有限的可扩展性，它无法更广泛地应用于一般运动。传统的基于优化的 ID 需要昂贵的实验室设置，限制了它的可用性。为了缓解这个问题，我们建议利用最近进步的人体运动模仿算法以数据驱动的方式学习人体逆动力学。关键的见解是，运动模仿者隐含地拥有人类 ID 知识，尽管不能直接应用。鉴于此，我们设计了一个高效的数据收集管道，其中包含最先进的运动模仿算法和物理模拟器，从而产生了一个大规模的人体逆动力学基准，即模仿动力学 (ImDy)。ImDy 包含超过 150 小时的运动，包括关节扭矩和全身地面反作用力数据。使用 ImDy，我们以完全监督的方式训练数据驱动的人体逆动力学求解器 ImDyS(olver)，该求解器可同时进行 ID 和地面反作用力估计。在 ImDy 和真实世界数据上进行的实验证明了 ImDyS 在人体逆动力学和地面反作用力估计方面的出色能力。此外，ImDy(-S) 作为基本运动分析工具的潜力在下游应用中得到了展示。项目页面为 https://foruck.github.io/ImDy/。]]></description>
      <guid>https://arxiv.org/abs/2410.17610</guid>
      <pubDate>Thu, 24 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>流程监督引导的代码生成策略优化</title>
      <link>https://arxiv.org/abs/2410.17621</link>
      <description><![CDATA[arXiv:2410.17621v1 公告类型：新
摘要：具有单元测试反馈的强化学习 (RL) 增强了大型语言模型 (LLM) 代码生成，但依赖于仅在完成代码评估后提供的稀疏奖励，限制了学习效率和渐进式改进。当生成的代码未通过所有单元测试时，不会收到任何学习信号，从而阻碍复杂任务的进展。为了解决这个问题，我们提出了一个过程奖励模型 (PRM)，它在生成过程中提供有关代码正确性的密集、行级反馈，模仿人类代码改进并提供即时指导。我们探索了训练 PRM 并将其集成到 RL 框架中的各种策略，发现使用 PRM 作为密集奖励和价值函数初始化可以显着提高性能。我们的方法将我们内部 LLM 在 LiveCodeBench 上的通过率从 28.2% 提高到 29.8%，将我们内部基准测试中的通过率从 31.8% 提高到 35.8%。我们的实验结果强调了 PRM 在增强 RL 驱动代码生成的有效性，尤其是对于长期场景。]]></description>
      <guid>https://arxiv.org/abs/2410.17621</guid>
      <pubDate>Thu, 24 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>马尔可夫思维链实现高效数学推理</title>
      <link>https://arxiv.org/abs/2410.17635</link>
      <description><![CDATA[arXiv:2410.17635v1 公告类型：新
摘要：多步骤的思路链 (CoT) 得益于推理步骤和特定于任务的操作的逻辑结构，显著增强了大型语言模型的数学推理能力。随着长 CoT 的盛行，推理步骤的数量超出了可管理的标记限制，并导致更高的计算需求。受人类认知基本逻辑“推导，然后归约”的启发，我们将标准多步骤 CoT 概念化为一种新型马尔可夫思路链 (MCoT)。在本研究中，我们考虑数学推理任务，将每个推理步骤定义为带有 Python 代码片段的文本。为了促进更长的推理路径，通过与代码解释器的交互可以实现自我校正。我们的 MCoT 旨在将前面的推理步骤压缩为一个简化的问题，从而无需依赖冗长的 KV 缓存即可实现高效的下一步推理。在我们的实验中，我们整理了 \texttt{MCoTInstruct} 数据集，实证结果表明 MCoT 不仅显著提高了效率，而且还保持了相当的准确性。虽然还有许多地方有待探索，但这项工作为探索 LLM 的长 CoT 推理能力铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2410.17635</guid>
      <pubDate>Thu, 24 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>绘制媒体格局：通过网络互动预测事实报道和政治偏见</title>
      <link>https://arxiv.org/abs/2410.17655</link>
      <description><![CDATA[arXiv:2410.17655v1 公告类型：新
摘要：对依赖真实证据进行信息收集和报告的专业人士、组织和研究人员来说，对新闻来源的偏见评估至关重要。虽然某些偏见指标可以从内容分析中辨别出来，但政治偏见和假新闻等描述符带来了更大的挑战。在本文中，我们提出了一种最近提出的新闻媒体可靠性估计方法的扩展，该方法侧重于对渠道及其纵向网络交互进行建模。具体来说，我们评估了四种强化学习策略在大型新闻媒体超链接图上的分类性能。我们的实验针对两个具有挑战性的偏见描述符，即事实报道和政治偏见，在源媒体层面显示出显着的性能提升。此外，我们在 CLEF 2023 CheckThat 上验证了我们的方法！实验室挑战，在 F1 分数和官方 MAE 指标方面均优于报告的结果。此外，我们还发布了最大的带注释的新闻源媒体数据集，并按事实报道和政治偏见标签分类。我们的研究结果表明，根据新闻媒体来源在一段时间内的超链接交互对其进行分析是可行的，从而可以鸟瞰不断变化的媒体格局。]]></description>
      <guid>https://arxiv.org/abs/2410.17655</guid>
      <pubDate>Thu, 24 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AutoRNet：通过大型语言模型自动优化启发式方法以实现稳健的网络设计</title>
      <link>https://arxiv.org/abs/2410.17656</link>
      <description><![CDATA[arXiv:2410.17656v1 公告类型：新
摘要：由于其 NP-hard 特性和复杂的解决方案空间，实现稳健网络是一个具有挑战性的问题。从手工特征提取到深度学习，当前的方法取得了进展，但仍然僵化，需要手动设计和大量标记数据集。为了解决这些问题，我们提出了 AutoRNet，这是一个将大型语言模型 (LLM) 与进化算法相结合的框架，以生成稳健网络设计的启发式方法。我们设计网络优化策略，为 LLM 提供领域特定提示，利用领域知识生成高级启发式方法。此外，我们引入了一个自适应适应度函数来平衡收敛和多样性，同时保持度分布。AutoRNet 在稀疏和密集无标度网络上进行了评估，通过减少对手动设计和大型数据集的需求，其表现优于当前方法。]]></description>
      <guid>https://arxiv.org/abs/2410.17656</guid>
      <pubDate>Thu, 24 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PETAH：资源有限环境下混合 Transformer 的参数高效任务自适应</title>
      <link>https://arxiv.org/abs/2410.17661</link>
      <description><![CDATA[arXiv:2410.17661v1 公告类型：新
摘要：继自然语言处理 (NLP) 取得成功之后，计算机视觉领域开始转向 Transformer 模型。虽然 Transformer 性能良好且具有良好的多任务处理性能，但由于其计算要求高，许多资源受限的应用程序仍然依赖于卷积或混合模型，这些模型结合了卷积和注意层的优点，并在 100M 以下参数范围内实现最佳结果。同时，任务自适应技术允许将一个共享的 Transformer 主干用于多个下游任务，从而以可忽略不计的性能成本节省大量存储，但尚未用于混合 Transformer。在这项工作中，我们研究如何实现最佳任务自适应性能，并引入 PETAH：混合 Transformer 的参数高效任务自适应。我们进一步将 PETAH 自适应与修剪相结合，以实现高性能和存储友好的多任务模型。在我们对分类和其他视觉任务的广泛评估中，我们证明了我们的 PETAH 适配混合模型优于已建立的 ViT 任务适配技术，同时需要更少的参数并且在移动硬件上更高效。]]></description>
      <guid>https://arxiv.org/abs/2410.17661</guid>
      <pubDate>Thu, 24 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>