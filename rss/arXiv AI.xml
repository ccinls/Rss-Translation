<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.AI 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.AI 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Tue, 23 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>电信领域句子嵌入世界的导航指南针</title>
      <link>https://arxiv.org/abs/2406.12336</link>
      <description><![CDATA[arXiv:2406.12336v1 公告类型：交叉 
摘要：大量的句子嵌入模型使得选择一个模型变得具有挑战性，特别是对于电信等拥有丰富专业词汇的领域。我们评估了从公开可用的模型及其领域适应变体中获得的多个嵌入，包括点检索准确度及其（95\%）置信区间。我们建立了一种系统的方法来获取不同嵌入的相似度得分的阈值。我们观察到微调提高了平均引导准确度并缩小了置信区间。预训练与微调相结合使置信区间更加紧密。为了理解这些变化，我们分析并报告了 top-$K$、正确和随机句子相似度与检索准确度和相似度阈值之间的分布重叠之间的显着相关性。根据当前文献，我们分析了检索准确度变化是否可以归因于嵌入的各向同性。我们的结论是，嵌入的各向同性（通过两个独立的最先进的各向同性度量定义进行测量）不能归因于更好的检索性能。但是，提高检索准确度的域自适应也会提高各向同性。我们确定域自适应使特定域的嵌入与一般域嵌入的距离进一步拉大。]]></description>
      <guid>https://arxiv.org/abs/2406.12336</guid>
      <pubDate>Tue, 23 Jul 2024 06:28:26 GMT</pubDate>
    </item>
    <item>
      <title>部分可观测条件下的浅层规划</title>
      <link>https://arxiv.org/abs/2407.15820</link>
      <description><![CDATA[arXiv:2407.15820v1 公告类型：新
摘要：在强化学习框架下制定现实问题涉及非平凡的设计选择，例如为学习目标选择折扣因子（折扣累积奖励），这阐明了代理的规划范围。这项工作研究了给定底层马尔可夫决策过程的结构参数，折扣因子对偏差方差权衡的影响。我们的结果支持这样一种观点，即较短的规划范围可能是有益的，尤其是在部分可观测性的情况下。]]></description>
      <guid>https://arxiv.org/abs/2407.15820</guid>
      <pubDate>Tue, 23 Jul 2024 06:28:25 GMT</pubDate>
    </item>
    <item>
      <title>学习低模型复杂度的神经网络分类器</title>
      <link>https://arxiv.org/abs/1707.09933</link>
      <description><![CDATA[arXiv:1707.09933v3 公告类型：交叉 
摘要：用于大规模学习任务的现代神经网络架构具有更高的模型复杂度，这使得理解、可视化和训练这些架构变得困难。深度学习技术的最新贡献集中在架构修改上，以提高参数效率和性能。在本文中，我们为神经网络推导出一个连续且可微的误差函数，该函数最小化其经验误差以及模型复杂度的度量。后一个度量是通过推导一类深度网络分类器层的 Vapnik-Chervonenkis (VC) 维度的可微上界获得的。使用标准反向传播，我们实现了一条训练规则，该规则试图最小化训练样本上的误差，同时通过保持模型复杂度较低来提高泛化能力。我们证明了我们的公式（低复杂度神经网络 - LCNN）在几种深度学习算法和各种大型基准数据集中的有效性。我们表明，结果网络中的隐藏层神经元学习的特征非常清晰，对于图像数据集而言，其数量更加清晰。与 Dropout 和 Batch Normalization 等方法相比，我们提出的方法在各种架构中都具有优势，我们的结果有力地表明，深度学习技术可以从 LCNN 学习规则等模型复杂性控制方法中受益。]]></description>
      <guid>https://arxiv.org/abs/1707.09933</guid>
      <pubDate>Tue, 23 Jul 2024 06:28:25 GMT</pubDate>
    </item>
    <item>
      <title>TaskGen：使用 StrictJSON 的基于任务、内存注入的代理框架</title>
      <link>https://arxiv.org/abs/2407.15734</link>
      <description><![CDATA[arXiv:2407.15734v1 公告类型：新
摘要：TaskGen 是一个开源代理框架，它使用代理来解决任意任务，方法是将任务分解为子任务。每个子任务都映射到一个配备的函数或另一个要执行的代理。为了减少冗长（从而减少令牌使用），TaskGen 使用 StrictJSON 来确保从大型语言模型 (LLM) 输出 JSON，以及类型检查和迭代错误更正等附加功能。TaskGen 理念的关键是根据需要了解信息/内存进行管理。我们在各种环境下对 TaskGen 进行了实证评估，例如障碍物位置不断变化的 40x40 动态迷宫导航（解决率为 100%）、具有密集奖励和详细目标的 TextWorld 密室逃脱解决（解决率为 96%）、网页浏览（69% 的操作成功）、解决 MATH 数据集（100 个 5 级问题的解决率为 71%）、在 NaturalQuestions 数据集上进行检索增强生成（F1 得分为 47.03%）]]></description>
      <guid>https://arxiv.org/abs/2407.15734</guid>
      <pubDate>Tue, 23 Jul 2024 06:28:24 GMT</pubDate>
    </item>
    <item>
      <title>解释机器学习模型中的决策：参数化复杂性分析</title>
      <link>https://arxiv.org/abs/2407.15780</link>
      <description><![CDATA[arXiv:2407.15780v1 公告类型：新
摘要：本文对各种机器学习 (ML) 模型中解释问题的参数化复杂性进行了全面的理论研究。与普遍的黑箱感知相反，我们的研究重点是具有透明内部机制的模型。我们解决了两种主要类型的解释问题：溯因和对比，包括它们的局部和全局变体。我们的分析涵盖了各种 ML 模型，包括决策树、决策集、决策列表、有序二元决策图、随机森林和布尔电路及其集合，每种模型都提供了独特的解释挑战。这项研究通过提供对这些模型生成解释的复杂性的基础理解，填补了可解释人工智能 (XAI) 的重大空白。这项工作为 XAI 领域的进一步研究提供了至关重要的见解，有助于更广泛地讨论人工智能系统中透明度和问责制的必要性。]]></description>
      <guid>https://arxiv.org/abs/2407.15780</guid>
      <pubDate>Tue, 23 Jul 2024 06:28:24 GMT</pubDate>
    </item>
    <item>
      <title>奥德赛：赋予特工开放世界技能</title>
      <link>https://arxiv.org/abs/2407.15325</link>
      <description><![CDATA[arXiv:2407.15325v1 公告类型：新
摘要：最近的研究深入探讨了为 Minecraft 等开放世界具象环境构建通用代理。尽管结果令人鼓舞，但现有的努力主要集中在解决基本的程序化任务上，例如按照 Minecraft 技术树收集材料和制作工具，将 ObtainDiamond 任务视为最终目标。这种限制源于代理可用的操作集定义狭窄，要求他们从头开始学习有效的长期策略。因此，在开放世界中发现多样化的游戏机会变得具有挑战性。在这项工作中，我们引入了 ODYSSEY，这是一个新框架，它使基于大型语言模型 (LLM) 的代理具有开放世界技能，可以探索广阔的 Minecraft 世界。ODYSSEY 包含三个关键部分：(1) 一个交互式代理，具有一个开放世界技能库，该库包含 40 种原始技能和 183 种组合技能。 (2) 经过微调的 LLaMA-3 模型，该模型在大型问答数据集上进行训练，其中包含来自 Minecraft Wiki 的 390k+ 条指令。 (3) 新的开放世界基准包括数千个长期规划任务、数十个动态即时规划任务和一个自主探索任务。大量实验表明，所提出的 ODYSSEY 框架可以有效评估代理的规划和探索能力。所有数据集、模型权重和代码均公开可用，以激发未来对更高级自主代理解决方案的研究。]]></description>
      <guid>https://arxiv.org/abs/2407.15325</guid>
      <pubDate>Tue, 23 Jul 2024 06:28:23 GMT</pubDate>
    </item>
    <item>
      <title>代数反统一</title>
      <link>https://arxiv.org/abs/2407.15510</link>
      <description><![CDATA[arXiv:2407.15510v1 公告类型：新
摘要：抽象是人类和人工智能的关键，因为它允许人们在原本不同的对象或情况下看到共同的结构，因此它是人工智能普遍性的关键要素。反统一（或泛化）是理论计算机科学和人工智能研究抽象的一部分。它已成功应用于各种与人工智能相关的问题，最重要的是归纳逻辑编程。到目前为止，文献中仅从句法角度研究反统一。本文的目的是在一般代数中发起一种反统一的代数（即语义）理论。这是由最近对相似性和类比比例的应用所激发的。]]></description>
      <guid>https://arxiv.org/abs/2407.15510</guid>
      <pubDate>Tue, 23 Jul 2024 06:28:23 GMT</pubDate>
    </item>
    <item>
      <title>可解释的基于概念的记忆推理</title>
      <link>https://arxiv.org/abs/2407.15527</link>
      <description><![CDATA[arXiv:2407.15527v1 公告类型：新
摘要：深度学习系统决策过程缺乏透明度，这对现代人工智能 (AI) 提出了重大挑战，因为它削弱了用户依赖和验证这些系统的能力。为了应对这一挑战，概念瓶颈模型 (CBM) 通过将人类可解释的概念纳入深度学习架构取得了重大进展。这种方法允许将预测追溯到用户可以理解并可能干预的特定概念模式。然而，现有的 CBM 任务预测器并不完全可解释，从而阻碍了在部署之前对其决策过程进行彻底分析和任何形式的正式验证，从而引发了严重的可靠性问题。为了弥补这一差距，我们引入了基于概念的记忆推理器 (CMR)，这是一种新颖的 CBM，旨在提供人类可理解且可证明可验证的任务预测过程。我们的方法是将每个任务预测建模为可学习逻辑规则记忆的神经选择机制，然后对所选规则进行符号评估。显式记忆和符号评估的存在使领域专家能够检查并正式验证任务预测过程感兴趣的某些全局属性的有效性。实验结果表明，CMR 实现了与最先进的 CBM 相当的准确性-可解释性权衡，发现与基本事实一致的逻辑规则，允许规则干预，并允许部署前验证。]]></description>
      <guid>https://arxiv.org/abs/2407.15527</guid>
      <pubDate>Tue, 23 Jul 2024 06:28:23 GMT</pubDate>
    </item>
    <item>
      <title>人工智能中的问题、其哲学根源及其对科学和社会的影响</title>
      <link>https://arxiv.org/abs/2407.15671</link>
      <description><![CDATA[arXiv:2407.15671v1 公告类型：新
摘要：人工智能 (AI) 是当今最相关的新兴技术之一。鉴于此，本文提出应更加关注人工智能技术及其使用的哲学方面。有人认为，这种缺陷通常与对知识增长的哲学误解有关。为了识别这些误解，参考了科学哲学家卡尔波普尔和物理学家大卫德意志的思想。两位思想家的作品都旨在反对错误的知识理论，例如归纳主义、经验主义和工具主义。本文表明，这些理论与当前人工智能技术的运作方式有相似之处。它还表明，这些理论在人工智能的（公开）讨论中非常活跃，通常称为贝叶斯主义。与波普尔和德意志的观点一致，本文认为所有这些理论都是基于错误的知识哲学。本文分析了这些错误哲学对人工智能在科学和社会中的应用的影响，包括可能出现的一些问题情况。本文最后对通用人工智能 (AGI) 提出了现实的看法，并提出了关于 A(G)I 和哲学（即认识论）的三个命题。]]></description>
      <guid>https://arxiv.org/abs/2407.15671</guid>
      <pubDate>Tue, 23 Jul 2024 06:28:23 GMT</pubDate>
    </item>
    <item>
      <title>用于化学反应条件推荐的文本增强多模态 LLM</title>
      <link>https://arxiv.org/abs/2407.15141</link>
      <description><![CDATA[arXiv:2407.15141v1 公告类型：新
摘要：高通量反应条件 (RC) 筛选是化学合成的基础。然而，目前的 RC 筛选受到费力且成本高昂的反复试验工作流程的影响。由于数据稀疏和反应表示不足，传统的计算机辅助合成规划 (CASP) 工具无法找到合适的 RC。如今，大型语言模型 (LLM) 能够解决与化学相关的问题，例如分子设计和化学逻辑问答任务。然而，LLM 尚未实现对化学反应条件的准确预测。在这里，我们提出了 MM-RCR，这是一种文本增强的多模态 LLM，它从 SMILES、反应图和文本语料库中学习统一的反应表示，用于化学反应推荐 (RCR)。为了训练 MM-RCR，我们构建了 120 万个成对的问答指令数据集。我们的实验结果表明，MM-RCR 在两个开放基准数据集上实现了最佳性能，并在域外 (OOD) 和高通量实验 (HTE) 数据集上表现出强大的泛化能力。MM-RCR 有潜力加速化学合成中的高通量条件筛选。]]></description>
      <guid>https://arxiv.org/abs/2407.15141</guid>
      <pubDate>Tue, 23 Jul 2024 06:28:22 GMT</pubDate>
    </item>
    <item>
      <title>解释混合动机博弈中的代理决策</title>
      <link>https://arxiv.org/abs/2407.15255</link>
      <description><![CDATA[arXiv:2407.15255v1 公告类型：新
摘要：近年来，代理已经能够通过自然语言无缝交流并在涉及合作和竞争的环境中导航，这一事实可能会引发社会困境。由于合作与竞争的交织，理解代理在这种环境中的决策具有挑战性，人类可以从获得解释中受益。然而，在可解释的人工智能背景下，这种环境和场景很少被探索。虽然一些针对合作环境的解释方法可以应用于混合动机设置，但它们并没有解决代理间竞争、廉价谈话或通过行动进行的隐性沟通。在这项工作中，我们设计了解释方法来解决这些问题。然后，我们继续证明它们对人类的有效性和实用性，并使用非平凡的混合动机游戏作为测试案例。最后，我们建立了通用性并证明了这些方法对其他游戏的适用性，包括我们使用大型语言模型模仿人类游戏动作的游戏。]]></description>
      <guid>https://arxiv.org/abs/2407.15255</guid>
      <pubDate>Tue, 23 Jul 2024 06:28:22 GMT</pubDate>
    </item>
    <item>
      <title>利用背景知识进行因果识别的新规则</title>
      <link>https://arxiv.org/abs/2407.15259</link>
      <description><![CDATA[arXiv:2407.15259v1 公告类型：新
摘要：识别因果关系对于各种下游任务至关重要。除了观察数据之外，通常还会引入背景知识 (BK) 来揭示因果关系，这些知识可以从人类专业知识或实验中获得。这引发了一个开放性问题，即在存在潜在变量的情况下，从观察数据和 BK 中可以识别出哪些因果关系。在本文中，我们提出了两条纳入 BK 的新规则，为这个开放性问题提供了一个新的视角。此外，我们表明这些规则适用于一些典型的因果关系任务，例如使用观察数据确定可能的因果效应集。我们的基于规则的方法通过绕过枚举块集的过程来增强最先进的方法，否则该过程将带来指数级的复杂性。]]></description>
      <guid>https://arxiv.org/abs/2407.15259</guid>
      <pubDate>Tue, 23 Jul 2024 06:28:22 GMT</pubDate>
    </item>
    <item>
      <title>利用遗传编程揭示强化学习中的决策过程</title>
      <link>https://arxiv.org/abs/2407.14714</link>
      <description><![CDATA[arXiv:2407.14714v1 公告类型：新
摘要：尽管取得了巨大进步，但机器学习和深度学习仍然受到难以理解的预测的困扰。然而，对于现实世界中使用（深度）强化学习来说，难以理解并不是一个选择，因为不可预测的行为会严重伤害所涉及的个人。在这项工作中，我们提出了一个遗传编程框架，通过使用程序模仿已经训练过的代理来为他们的决策过程生成解释。程序是可解释的，可以执行以生成代理选择特定动作的原因的解释。此外，我们进行了一项消融研究，研究如何使用库学习扩展领域特定语言来改变该方法的性能。我们将我们的结果与这个问题的先前最新成果进行了比较，并表明我们在性能上是可比的，但需要的硬件资源和计算时间要少得多。]]></description>
      <guid>https://arxiv.org/abs/2407.14714</guid>
      <pubDate>Tue, 23 Jul 2024 06:28:21 GMT</pubDate>
    </item>
    <item>
      <title>TraveLLM：如果发生网络中断，您能为我规划新的公共交通路线吗？</title>
      <link>https://arxiv.org/abs/2407.14926</link>
      <description><![CDATA[arXiv:2407.14926v1 公告类型：新
摘要：假设时代广场地铁站附近的 1 号列车发生中断。您尝试在 Google 地图上查找前往肯尼迪机场的替代地铁路线，但该应用程序未能提供考虑到中断和您避开拥挤车站的偏好的合适建议。我们发现在许多这样的情况下，当前的导航应用程序可能会出现问题并且无法给出合理的建议。为了填补这一空白，在本文中，我们开发了一个原型 TraveLLM，用于在中断的情况下规划公共交通路线，该原型依赖于大型语言模型 (LLM)。LLM 在各个领域的推理和规划方面表现出了卓越的能力。在这里，我们希望研究 LLM 的潜力，即将多模式用户特定查询和约束纳入公共交通路线建议中。在不同的场景下设计了各种测试用例，包括不同的天气条件、紧急事件和新交通服务的引入。然后，我们比较了最先进的 LLM（包括 GPT-4、Claude 3 和 Gemini）在生成准确路线方面的表现。我们的比较分析证明了 LLM（尤其是 GPT-4）在提供导航计划方面的有效性。我们的研究结果有潜力使 LLM 增强现有的导航系统，并在出现中断时提供更灵活、更智能的方法来满足各种用户需求。]]></description>
      <guid>https://arxiv.org/abs/2407.14926</guid>
      <pubDate>Tue, 23 Jul 2024 06:28:21 GMT</pubDate>
    </item>
    <item>
      <title>基于可观察系统行为的自主性水平测量</title>
      <link>https://arxiv.org/abs/2407.14975</link>
      <description><![CDATA[arXiv:2407.14975v1 公告类型：新
摘要：当代人工智能系统在提高各个领域的人类效率和安全性方面发挥着关键作用。其中一个领域是自主系统，尤其是在汽车和国防用例中。人工智能为自主系统的目标导向行为和人类独立性带来了学习和增强的决策能力。然而，对自主系统能力缺乏清晰的理解，阻碍了人机或机器之间的交互和拦截。这需要不同程度的人为参与，以实现安全、问责和可解释性。然而，衡量自主系统的自主能力水平是一个挑战。存在两种测量尺度，但衡量自主性需要预先假定各种在野外无法获得的元素。这就是为什么现有的自主性水平测量仅在设计或测试和评估阶段实施的原因。目前还没有基于观察到的系统行为的自主性水平测量。为了解决这个问题，我们概述了一种使用可观察动作预测自主性水平的潜在测量方法。我们还提出了一种结合所提测量方法的算法。该方法和算法对于对运行时盲目比较自主系统的方法感兴趣的研究人员和从业者具有重要意义。基于防御的实现同样是可能的，因为反自主性依赖于对自主系统的可靠识别。]]></description>
      <guid>https://arxiv.org/abs/2407.14975</guid>
      <pubDate>Tue, 23 Jul 2024 06:28:21 GMT</pubDate>
    </item>
    <item>
      <title>使用大型语言模型进行多智能体因果发现</title>
      <link>https://arxiv.org/abs/2407.15073</link>
      <description><![CDATA[arXiv:2407.15073v1 公告类型：新
摘要：大型语言模型 (LLM) 通过利用来自大量文本语料库的大量专家知识，在因果发现任务中表现出巨大潜力。然而，LLM 在因果发现中的多智能体能力仍未得到充分开发。本文介绍了一个研究这一潜力的通用框架。第一个是元代理模型，它完全依赖于 LLM 代理之间的推理和讨论来进行因果发现。第二个是编码代理模型，它利用代理的计划、编写和执行代码的能力，利用高级统计库进行因果发现。第三个是混合模型，它集成了元代理模型和编码代理模型方法，结合了多个代理的统计分析和推理技能。我们提出的框架通过有效利用 LLM 的专家知识、推理能力、多智能体合作和统计因果方法显示出有希望的结果。通过探索 LLM 的多智能体潜力，我们旨在为进一步研究利用 LLM 多智能体解决因果相关问题奠定基础。]]></description>
      <guid>https://arxiv.org/abs/2407.15073</guid>
      <pubDate>Tue, 23 Jul 2024 06:28:21 GMT</pubDate>
    </item>
    <item>
      <title>面向自动化函数方程证明：基准数据集和领域特定上下文代理</title>
      <link>https://arxiv.org/abs/2407.14521</link>
      <description><![CDATA[arXiv:2407.14521v1 公告类型：新
摘要：自动定理证明 (ATP) 因其复杂性和计算需求而面临挑战。最近的研究探索了使用大型语言模型 (LLM) 进行 ATP 动作选择，但这些方法可能耗费大量资源。本研究介绍了 FEAS，这是一种增强 Lean 中 COPRA 上下文学习框架的代理。FEAS 改进了提示生成、响应解析，并结合了特定领域的函数方程启发式方法。它引入了 FunEq，这是一个具有不同难度的函数方程问题的精选数据集。FEAS 在 FunEq 上的表现优于基线，尤其是在集成了特定领域的启发式方法后。结果证明了 FEAS 在生成和将高级证明策略形式化为 Lean 证明方面的有效性，展示了针对特定 ATP 挑战的定制方法的潜力。]]></description>
      <guid>https://arxiv.org/abs/2407.14521</guid>
      <pubDate>Tue, 23 Jul 2024 06:28:20 GMT</pubDate>
    </item>
    <item>
      <title>实现基于规则的解释器与黑盒模型的一致性——规则归纳与基于XAI的特征重要性的融合</title>
      <link>https://arxiv.org/abs/2407.14543</link>
      <description><![CDATA[arXiv:2407.14543v1 公告类型：新
摘要：基于规则的模型提供了人类可理解的表示，即它们是可解释的。因此，它们用于解释不可解释的复杂模型（称为黑盒模型）的决策。此类解释的生成涉及通过基于规则的模型近似黑盒模型。然而，到目前为止，尚未研究基于规则的模型是否以与其近似的黑盒模型相同的方式做出决策。在这项工作中，以相同方式进行决策被理解为决策的一致性和用于决策的最重要属性的一致性。本研究提出了一种新方法，确保基于规则的代理模型模仿黑盒模型的性能。所提出的解决方案执行解释融合，涉及规则生成并考虑所选 XAI 方法为要解释的黑盒模型确定的特征重要性。该方法的结果可以是全局和局部的基于规则的解释。通过对代表分类问题的 30 个表格基准数据集进行广泛分析，验证了所提解决方案的质量。评估包括与参考方法的比较和说明性案例研究。此外，本文还讨论了基于规则的方法在 XAI 中的可能应用途径，以及基于规则的解释（包括所提出的方法）如何满足用户的观点和对内容和演示的要求。所创建的软件和包含完整实验结果的详细报告可在 GitHub 存储库（https://github.com/ruleminer/FI-rules4XAI）上找到。]]></description>
      <guid>https://arxiv.org/abs/2407.14543</guid>
      <pubDate>Tue, 23 Jul 2024 06:28:20 GMT</pubDate>
    </item>
    <item>
      <title>Thought-Like-Pro：通过基于自驱动 Prolog 的思维链增强大型语言模型的推理能力</title>
      <link>https://arxiv.org/abs/2407.14562</link>
      <description><![CDATA[arXiv:2407.14562v1 公告类型：新 
摘要：大型语言模型 (LLM) 作为通用助手表现出色，在各种推理任务中表现出色。这一成就代表着朝着实现通用人工智能 (AGI) 迈出了重要一步。尽管取得了这些进步，但 LLM 的有效性通常取决于所采用的特定提示策略，并且仍然缺乏一个强大的框架来促进跨各种推理任务的学习和泛化。为了应对这些挑战，我们引入了一个新颖的学习框架 THOUGHT-LIKE-PRO。在这个框架中，我们利用模仿学习来模仿思维链 (CoT) 过程，该过程由符号 Prolog 逻辑引擎生成的推理轨迹进行验证和翻译。该框架以自我驱动的方式进行，使 LLM 能够根据给定的指令制定规则和语句，并利用符号 Prolog 引擎得出结果。随后，LLM 将 Prolog 衍生的连续推理轨迹转换为自然语言 CoT 以进行模仿学习。我们的实证结果表明，我们提出的方法大大增强了 LLM 的推理能力，并在分布外推理任务中表现出强大的泛化能力。]]></description>
      <guid>https://arxiv.org/abs/2407.14562</guid>
      <pubDate>Tue, 23 Jul 2024 06:28:20 GMT</pubDate>
    </item>
    <item>
      <title>神经网络中的关系组合：调查与行动号召</title>
      <link>https://arxiv.org/abs/2407.14662</link>
      <description><![CDATA[arXiv:2407.14662v1 公告类型：新
摘要：许多神经网络似乎将数据表示为“特征向量”的线性组合。用于发现这些向量的算法最近取得了令人瞩目的成功。然而，我们认为，如果不了解关系组合，这种成功是不完整的：神经网络如何（或是否）组合特征向量来表示更复杂的关系。为了促进这一领域的研究，本文提供了已提出的各种关系机制的导览，以及这些机制如何影响可解释特征的搜索的初步分析。我们最后提出了一系列有希望的实证研究领域，这可能有助于确定神经网络如何表示结构化数据。]]></description>
      <guid>https://arxiv.org/abs/2407.14662</guid>
      <pubDate>Tue, 23 Jul 2024 06:28:20 GMT</pubDate>
    </item>
    </channel>
</rss>