<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title></title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.ai在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Wed, 02 Apr 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title></title>
      <link>https://arxiv.org/abs/2504.00063</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2504.00063</guid>
      <pubDate>Wed, 02 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://arxiv.org/abs/2504.00125</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2504.00125</guid>
      <pubDate>Wed, 02 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>数字网中的大型语言模型：快速测试其数值推理能力</title>
      <link>https://arxiv.org/abs/2504.00226</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2504.00226</guid>
      <pubDate>Wed, 02 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://arxiv.org/abs/2504.00277</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2504.00277</guid>
      <pubDate>Wed, 02 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://arxiv.org/abs/2504.00280</link>
      <description><![CDATA[ARXIV：2504.00280V1公告类型：新 
摘要：本文研究了扩散策略在非平稳的，基于视觉的RL设置中的应用，特别是针对任务动态和目标随时间发展的环境。我们的工作基于在动态现实世界中遇到的实际挑战，例如机器人组装线和自主导航，在该场景中，代理必须从高维视觉输入中调整控制策略。我们采用扩散策略 - 利用迭代的随机转化来完善包括Procgen和Dointmaze在内的基准环境的潜在动作表示。我们的实验表明，尽管计算需求增加，但扩散策略始终优于标准RL方法，例如PPO和DQN，从而获得了更高的均值和最大奖励，并且可变性降低。这些发现强调了该方法在不断变化的条件下生成连贯的，上下文相关的动作序列的能力，同时还突出了以进一步改善处理极端非平稳性的领域。]]></description>
      <guid>https://arxiv.org/abs/2504.00280</guid>
      <pubDate>Wed, 02 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>与本地数据保护的协作LLM数值推理</title>
      <link>https://arxiv.org/abs/2504.00299</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2504.00299</guid>
      <pubDate>Wed, 02 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://arxiv.org/abs/2504.00389</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2504.00389</guid>
      <pubDate>Wed, 02 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://arxiv.org/abs/2504.00424</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2504.00424</guid>
      <pubDate>Wed, 02 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://arxiv.org/abs/2504.00509</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2504.00509</guid>
      <pubDate>Wed, 02 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://arxiv.org/abs/2504.00613</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2504.00613</guid>
      <pubDate>Wed, 02 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://arxiv.org/abs/2504.00615</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2504.00615</guid>
      <pubDate>Wed, 02 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于LLM的自主代理商的人格驱动的决策</title>
      <link>https://arxiv.org/abs/2504.00727</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2504.00727</guid>
      <pubDate>Wed, 02 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://arxiv.org/abs/2504.00762</link>
      <description><![CDATA[ARXIV：2504.00762V2公告类型：新 
摘要：本文提出了一种简单，有效且具有成本效益的策略，可以通过扩展测试时间计算来提高LLM性能。我们的策略建立在重复采样到投票框架的基础上，这是一个新颖的转折：结合了多种模型，甚至更弱的模型，以利用其互补优势，这可能是由多样化的培训数据和范式产生的。通过将一致性用作信号，我们的策略在模型之间动态切换。理论分析强调了我们战略的效率和绩效优势。在六个数据集上进行的广泛实验表明，我们的策略不仅优于自符合性和最先进的多代理辩论方法，而且大大降低了推论成本。此外，ModelSwitch仅需要几个可比较的LLM即可实现最佳性能，并且可以通过验证方法扩展，这表明了在生成验证范式中利用多个LLM的潜力。]]></description>
      <guid>https://arxiv.org/abs/2504.00762</guid>
      <pubDate>Wed, 02 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://arxiv.org/abs/2504.00795</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2504.00795</guid>
      <pubDate>Wed, 02 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://arxiv.org/abs/2504.00831</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2504.00831</guid>
      <pubDate>Wed, 02 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>调查大型语言模型，以诊断学生在数学问题解决方面的认知能力</title>
      <link>https://arxiv.org/abs/2504.00843</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2504.00843</guid>
      <pubDate>Wed, 02 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Agent S2：用于计算机使用代理的组成通才特权框架</title>
      <link>https://arxiv.org/abs/2504.00906</link>
      <description><![CDATA[ARXIV：2504.00906V1公告类型：新 
摘要：计算机使用代理通过与计算机和移动设备上的图形用户界面（GUI）直接交互来自动化数字任务，从而通过完成用户查询的开放式空间来提高人类生产力。但是，当前的代理商面临重大挑战：GUI元素的不精确基础，长期任务计划的困难以及依靠单个通才模型来执行各种认知任务的绩效瓶颈。为此，我们介绍了Agent S2，这是一个新颖的构图框架，该框架将各种通才和专业模型的认知责任委托。我们提出了一种新型的地面技术，以实现精确的GUI定位，并在多个时间尺度上引入主动的分层计划，以响应不断发展的观测，在多个时间尺度上动态完善动作计划。评估表明，Agent S2在三个突出的计算机使用基准上建立了新的最先进（SOTA）性能。具体而言，代理S2在OSWORLD 15步骤和50步评估上的Claude计算机使用和UI-TAR等领先的基线代理（例如Claude Computer使用和UI-TARS）的相对改善达到18.9％和32.7％。此外，Agent S2有效地将其概括为其他操作系统和应用程序，在Windowsagentarena上超过了先前的最佳方法52.8％，而AndroidWorld相对较高。代码可在https://github.com/simular-ai/agent-s上找到。]]></description>
      <guid>https://arxiv.org/abs/2504.00906</guid>
      <pubDate>Wed, 02 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://arxiv.org/abs/2504.00907</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2504.00907</guid>
      <pubDate>Wed, 02 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AI设计中的法官：关于通过视觉模型实现人类专家等价的统计观点</title>
      <link>https://arxiv.org/abs/2504.00938</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2504.00938</guid>
      <pubDate>Wed, 02 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://arxiv.org/abs/2504.00002</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2504.00002</guid>
      <pubDate>Wed, 02 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>