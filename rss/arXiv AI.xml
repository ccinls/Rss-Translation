<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.ai更新在arxiv.org上</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.ai在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Wed, 09 Apr 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>Eduplanner：用于自定义和智能教学设计的基于LLM的多代理系统</title>
      <link>https://arxiv.org/abs/2504.05370</link>
      <description><![CDATA[ARXIV：2504.05370V1公告类型：新 
摘要：大型语言模型（LLM）在人工通用情报（AGI）时代具有显着高级的智能教育。一个有希望的应用在于对课程和学习活动的教学设计自动概括，重点关注两个关键方面：（1）定制生成：基于学生的不同学习能力和状态生成针对性的靶向教学内容，以及（2）智能优化：基于从学习有效性或测试评分的反馈中迭代优化内容。目前，一个大型LLM无法有效地管理整个过程，这为设计智能教学计划带来了挑战。为了解决这些问题，我们开发了Eduplanner，这是一种基于LLM的多代理系统，包括评估器代理，优化器代理和一个问题分析师，从事对抗性协作的工作，以生成针对课程和学习活动的自定义和智能的教学设计。以数学课程为例，Eduplanner采用一种新颖的技能树结构来准确地对学生群体的背景数学知识进行建模，根据学生的知识水平和学习能力来个性化课程的教学设计和学习活动。此外，我们介绍了CIDDP，这是一个基于LLM的五维评估模块，其中包括清晰度，完整性，深度，实用性和相关性，以全面评估数学课程计划质量和自举智能优化。在GSM8K和代数数据集上进行的实验表明，Eduplanner在评估和优化课程和学习活动的教学设计方面表现出色。消融研究进一步验证了框架中每个组件的重要性和有效性。我们的代码可在https://github.com/zc0812/edu_planner上公开获取]]></description>
      <guid>https://arxiv.org/abs/2504.05370</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>增强学习剂的互动解释</title>
      <link>https://arxiv.org/abs/2504.05393</link>
      <description><![CDATA[ARXIV：2504.05393V1公告类型：新 
摘要：随着强化学习方法越来越多地积累成就，理解其解决方案的需求变得更加至关重要。大多数可解释的强化学习（XRL）方法产生了一个静态解释，描绘了开发人员对应解释以及如何解释的直觉。相比之下，社会科学的文献提出，有意义的解释是作为解释器和说明器之间的对话结构的，这表明对用户以及她与代理商的交流更为积极。在本文中，我们介绍了ASQ-IT  - 一种交互式解释系统，该系统根据用户的查询来描述所描述感兴趣行为的时间属性的用户的查询。我们的方法基于形式方法：ASQ-中的查询，它的用户界面映射是我们开发的有限痕迹（LTLF）的线性时间逻辑片段（LTLF），并且我们的查询处理算法基于自动机理论。用户研究表明，最终用户可以在ASQ-IT中理解和制定查询，并且使用ASQ-IT可以帮助用户识别错误的代理行为。]]></description>
      <guid>https://arxiv.org/abs/2504.05393</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>推理模型知道何时正确：探测隐藏状态以进行自我验证</title>
      <link>https://arxiv.org/abs/2504.05419</link>
      <description><![CDATA[ARXIV：2504.05419V1公告类型：新 
摘要：推理模型在数学和逻辑推理等任务上取得了出色的性能，这要归功于它们在推理过程中搜索的能力。但是，即使达到正确的答案，他们仍然会遭受过度思考的痛苦，通常会执行不必​​要的推理步骤。这提出了一个问题：模型可以评估推理过程中其中间答案的正确性吗？在这项工作中，我们研究推理模型是否通过探测模型的隐藏状态来编码有关答案正确性的信息。最终的探针可以高精度验证中间答案，并产生高度校准的分数。此外，我们发现模型的隐藏状态编码了未来答案的正确性，从而在中间答案完全提出之前，可以尽早预测正确性。然后，我们将探针用作验证者，以决定是否在推理期间在中间答案处退出推理，将推理令牌的数量减少24 \％，而不会损害性能。这些发现证实，推理模型确实编码了正确性的概念但无法利用它，从而揭示了大量未开发的潜力以提高其效率。]]></description>
      <guid>https://arxiv.org/abs/2504.05419</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>棱镜：LLMS代码生成的动态和灵活的基准测试与蒙特卡洛树搜索</title>
      <link>https://arxiv.org/abs/2504.05500</link>
      <description><![CDATA[ARXIV：2504.05500V1公告类型：新 
摘要：大语言模型（LLM）的快速发展已经超过了传统评估方法。静态基准测试无法捕获LLM功能的深度和广度并最终变得过时，而大多数动态方法要么过于严重依赖于基于LLM的评估，要么仍然受到预定义的测试集的约束。我们介绍了Prism，这是一个灵活的，动态的基准测试框架，旨在全面的LLM评估。 PRISM建立在三个关键组成部分的基础上：（1）基于树的状态表示，将评估作为马尔可夫决策过程进行建模，（2）一种蒙特卡洛树搜索算法，该算法适用于发现具有挑战性的评估场景，以及（3）多代理评估管道，可以同时评估多样性的能力。为了确保可靠的评估，Prism将树木探索模式的结构测量与难度级别的性能指标相结合，提供了误差模式，测试覆盖范围和解决方案方法的详细诊断。通过对五个最先进的LLM的广泛实验，我们分析了模型架构和规模如何影响各种任务困难的代码生成性能。我们的结果表明，棱镜的有效性是一种动态基准，随着模型的进步发展，同时为其局限性提供了更深入的见解。]]></description>
      <guid>https://arxiv.org/abs/2504.05500</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Sciscigpt：推进人类科学科学的合作</title>
      <link>https://arxiv.org/abs/2504.05559</link>
      <description><![CDATA[ARXIV：2504.05559V1公告类型：新 
摘要：大规模数据集的可用性不断提高，促进了许多科学领域的快速进步，从而为研究和发现创造了前所未有的机会，同时构成了重大的分析挑战。大型语言模型（LLM）和AI代理商的最新进展为人类协作开辟了新的可能性，提供了强大的工具来浏览这一复杂的研究景观。在本文中，我们介绍了Sciscigpt，这是一种开源的原型AI合作者，该合作者使用科学科学作为测试床来探索LLM驱动的研究工具的潜力。 Sciscigpt自动化复杂的工作流程，支持各种分析方法，加速研究原型和迭代，并促进可重复性。通过案例研究，我们证明了其简化广泛的经验和分析研究任务的能力，同时突出了其更广泛的提高研究潜力。我们进一步提出了针对人类协作的LLM代理能力成熟度模型，设想了一个路线图，以进一步改善和扩展Sciscigpt等框架。随着人工智能能力的不断发展，像Sciscigpt这样的框架可能在科学研究和发现中扮演着越来越重要的角色，从而释放了进一步的机会。同时，这些新进步也从确保透明度和道德使用到平衡人和人工智能贡献，提出了关键的挑战。解决这些问题可能会影响科学探究的未来，并告知我们如何培训下一代科学家在越来越多的AI综合研究生态系统中蓬勃发展。]]></description>
      <guid>https://arxiv.org/abs/2504.05559</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>持续学习具有脑启发的时间发展机制的多种认知功能</title>
      <link>https://arxiv.org/abs/2504.05621</link>
      <description><![CDATA[ARXIV：2504.05621V1公告类型：新 
摘要：当前人工智能网络中的认知功能与网络量表的指数增长相关，而人脑则可以不断学习数百种具有明显低能消耗的认知功能。该优势部分是由于大脑跨区域的时间发展机制，在这种机制中，渐进的形成，重组和从基本到高级区域的连接进行修剪，促进知识传递并防止网络冗余。受这些的启发，我们提出了具有脑启发的时间发育机制（TD-MCL）的多种认知功能的持续学习，从而使人们能够从简单到复杂的认知增强感知 - 运动 - 运动 - 运动互动（PMI）多个认知任务方案。 TD-MCL模型提出了不同认知模块之间远程连接的顺序演变，以促进积极的知识转移，同时使用反馈引导的局部连接抑制和修剪以有效消除以前的任务中的冗余，从而减少能源消耗，同时保留获得的知识。实验表明，所提出的方法可以在不引入正则化，重播或冻结策略的情况下降低网络量表的同时获得持续的学习能力，并且与直接学习相比，在新任务上实现了卓越的准确性。提出的方法表明，大脑的发育机制为探索一般认知能力的生物学上合理，低能增强提供了宝贵的参考。]]></description>
      <guid>https://arxiv.org/abs/2504.05621</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Stayltc：一个具有成本效益的多模式框架，用于住院时间长度预测</title>
      <link>https://arxiv.org/abs/2504.05691</link>
      <description><![CDATA[ARXIV：2504.05691V1公告类型：新 
摘要：准确预测医院中住院时间（LOS）对于改善医疗服务，资源管理和成本效率至关重要。本文介绍了使用液体时恒定网络（LTC）开发的多模式深度学习框架Stayltc。使用电子健康记录（EHR）和临床注释中的结构化数据对传统模型进行了评估LTC及其连续的复发动力学。我们在模拟III数据集上进行的评估表明，LTC的表现明显优于其他大多数时间序列模型，提供了提高的精度，鲁棒性和资源利用率的效率。此外，LTC与时间序列大型语言模型相比，LOS预测表现出可比的性能，同时需要大大降低计算能力和记忆力，从而强调了他们在医疗保健中提高自然语言处理（NLP）任务的潜力。]]></description>
      <guid>https://arxiv.org/abs/2504.05691</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLMS联合智能的自动档案描述</title>
      <link>https://arxiv.org/abs/2504.05711</link>
      <description><![CDATA[ARXIV：2504.05711V1公告类型：新 
摘要：执行档案标准需要专门的专业知识，并手动为档案材料创建元数据描述是一项繁琐且容易出错的任务。这项工作旨在探讨代理AI和大型语言模型（LLM）在解决实施标准化档案描述过程的挑战方面的潜力。为此，我们引入了一种代理AI驱动的系统，用于自动生成档案材料的高质量元数据描述。我们开发了一种联合优化方法，该方法将多个LLM的智能团结起来构建最佳档案元数据。我们还提出了克服与使用LLM相关的挑战的方法。为了评估我们技术的可行性和有效性，我们使用现实的档案材料数据集进行了广泛的实验，该数据集涵盖了各种文档类型和数据格式。评估结果证明了我们的技术的可行性，并强调了与元数据质量和可靠性中的单模溶液相比，联合优化方法的出色性能。]]></description>
      <guid>https://arxiv.org/abs/2504.05711</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AI驱动的锂离子电池健康状态预测状态的预后学：综合分析有验证</title>
      <link>https://arxiv.org/abs/2504.05728</link>
      <description><![CDATA[ARXIV：2504.05728V1公告类型：新 
摘要：本文对锂离子电池中的AI驱动预后（SOH）预测进行了全面评论。我们比较了多个数据集（Calce，NASA，UDDS）和场景（例如，温度和驱动条件的各种数据集（CALCE，NASA，UDDS），比较了包括FFNN，LSTM和BILSTM在内的各种AI算法的有效性。此外，我们分析了影响SOH波动的因素，例如温度和充电率，并通过模拟验证我们的发现。结果表明，BilstM的精度最高，与LSTM相比，平均RMSE降低了15％，突出了其在现实世界中的稳健性。]]></description>
      <guid>https://arxiv.org/abs/2504.05728</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从表面到深：使用知识图和LLM整合外部知识以进行后续问题生成</title>
      <link>https://arxiv.org/abs/2504.05801</link>
      <description><![CDATA[ARXIV：2504.05801V1公告类型：新 
摘要：在对话系统中，基于上下文的动态生成后续问题可以帮助用户探索信息并提供更好的用户体验。人类通常能够提出涉及一些一般生活知识并表现出更高级别认知能力的问题。但是，现有方法产生的问题通常仅限于浅色上下文问题，这些问题是没有启发性的，并且存在很大的差距。在本文中，我们提出了一个三阶段外部知识增强的后续问题生成方法，该方法通过识别上下文主题，在线构建知识图（kg）来生成问题，并最终将它们与大型语言模型相结合以生成最终问题。该模型通过引入外部常识知识并执行知识融合操作来生成信息丰富和探索性的后续问题。实验表明，与基线模型相比，我们的方法产生的问题在维持上下文相关性的同时更有信息，更接近人类的质疑水平。]]></description>
      <guid>https://arxiv.org/abs/2504.05801</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>神经领域的元学习</title>
      <link>https://arxiv.org/abs/2504.05806</link>
      <description><![CDATA[ARXIV：2504.05806V1公告类型：新 
摘要：神经领域（NF）已成为复杂数据表示的多功能框架。这项工作揭示了一种新的问题设置，称为\ emph {Meta-continual学习神经领域}（MCL-NF），并引入了一种新颖的策略，该策略采用了模块化体系结构与基于优化的元学习结合。我们的策略专注于克服现有方法的持续学习神经领域的局限性，例如灾难性的遗忘和缓慢的收敛性，我们的策略以显着提高的学习速度实现了高质量的重建。我们进一步介绍了Fisher信息的最大化神经辐射场（FIM-NERF），该损失在样本级别上最大化信息增长以增强学习概括，并具有证明的收敛保证和概括性结合。我们在图像，音频，视频重建方面进行了广泛的评估，并在六个不同的数据集上查看综合任务，这证明了我们的方法在重建质量和速度上比现有MCL和CL-NF方法的优势。值得注意的是，我们的方法可以快速适应城市规模的NERF渲染，并减少了参数要求。]]></description>
      <guid>https://arxiv.org/abs/2504.05806</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>生成的AI代理是否有效的个性化财务顾问？</title>
      <link>https://arxiv.org/abs/2504.05862</link>
      <description><![CDATA[ARXIV：2504.05862V1公告类型：新 
摘要：大型基于语言模型的代理人作为一种低成本机制越来越流行，以提供个性化的会话建议，并在相对简单的场景（例如电影建议）中表现出令人印象深刻的功能。但是，这些代理在复杂的高风险领域中如何执行，其中域专业知识是必不可少的，并且错误带来了很大的风险？本文调查了LLM-ADVISOR在金融领域中的有效性，重点介绍了三个截然不同的挑战：（1）当用户自己可能不确定其需求时，引起用户偏好，（2）为多元化的投资偏好提供个性化指导，（3）利用顾问个性来建立关系和促进信任。通过与64名参与者的基于实验室的用户研究，我们表明，LLM-ADVISOR在引起偏好时经常与人类顾问绩效相匹配，尽管他们可以难以解决矛盾的用户需求。在提供个性化建议时，LLM能够积极影响用户行为，但表现出明显的故障模式。我们的结果表明，准确的偏好启发是关键，否则，LLM-Advisor几乎没有影响，甚至可以将投资者引向不合适的资产。更令人担忧的是，用户似乎对提供的建议质量不敏感，或者更糟糕的是，这些可能会有反比的关系。的确，用户报告了对LLM采用外向角色的偏爱和提高的满意度以及情感信任，即使这些代理人提供了更糟糕的建议。]]></description>
      <guid>https://arxiv.org/abs/2504.05862</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>代理指南：一个简单的代理行为水印框架</title>
      <link>https://arxiv.org/abs/2504.05871</link>
      <description><![CDATA[ARXIV：2504.05871V1公告类型：新 
摘要：诸如社交媒体平台之类的数字生态系统中智能代理的部署越来越多，引起了人们对可追溯性和问责制的重大关注，尤其是在网络安全和数字内容保护方面。依靠令牌级操作的传统大型语言模型（LLM）水印技术由于行为令牌化和行为到作用翻译过程中的信息丢失的挑战而不适合代理。为了解决这些问题，我们提出了代理指南，这是一种新型的行为水印框架，它通过通过概率偏见引导代理人的高级决策（行为）来嵌入水印，同时保留特定执行的自然性（行动）。我们的方法将代理行为分为两个层次，行为（例如，选择书签）和动作（例如，用特定标签的书签）将水印引导的偏见应用于行为概率分布。我们采用基于Z统计的统计分析来检测水印，以确保在多轮比赛中可靠提取。在社交媒体场景中具有不同代理概况的实验表明，代理指南以低的假阳性速率实现有效的水印检测。我们的框架为代理水印提供了一种实用，坚固的解决方案，并在识别恶意代理和保护专有代理系统方面提供了应用。]]></description>
      <guid>https://arxiv.org/abs/2504.05871</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>近似模型计数中的系统参数决策</title>
      <link>https://arxiv.org/abs/2504.05874</link>
      <description><![CDATA[ARXIV：2504.05874V1公告类型：新 
摘要：本文提出了一种新的方法，用于确定基于哈希的近似模型的内部参数计数算法$ \ mathsf {actmc} $。在此问题中，所选的参数值必须确保$ \ Mathsf {actmc} $可能是近似正确的（PAC），同时也使其尽可能高效。现有的这种问题方法依赖于启发式方法；在本文中，我们通过将其提出为优化问题来解决此问题，该问题是由$ \ Mathsf {actmc} $的正确性证明到任意参数值而产生的。
  我们的方法将算法的声音和最优性分开，使我们能够解决前者无需重复的案例论证，同时为后者建立明确的框架。此外，减少后，由此产生的优化问题采用了一个异常简单的形式，可以使用基本的搜索算法，并提供有关参数值如何影响算法性能的洞察力。实验结果表明，我们的优化参数改善了最新$ \ Mathsf {actmc} $的运行时性能，取决于误差容量。]]></description>
      <guid>https://arxiv.org/abs/2504.05874</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>宙斯盾：基于人类注意力的智能车辆系统的可解释指南</title>
      <link>https://arxiv.org/abs/2504.05950</link>
      <description><![CDATA[ARXIV：2504.05950V1公告类型：新 
摘要：近年来，自动智能车辆（AIV）中提高决策能力（AIVS）一直是一个激烈的话题。尽管有进步，但培训机器以捕捉感兴趣的地区以综合场景理解（如人类的看法和推理）仍然是一个重大挑战。这项研究介绍了一个新颖的框架，基于人类注意力的智能车辆系统（AEGIS）的可解释指南。宙斯盾利用人类的注意力从眼球轨道转变为指导增强学习（RL）模型来识别对决策的关键区域。宙斯盾使用预训练的人类注意力模型来指导RL模型来确定决策的关键区域。通过在六个场景中收集来自20名参与者的120万帧，宙斯盾预训练了一个模型，以预测人类注意力的模式。]]></description>
      <guid>https://arxiv.org/abs/2504.05950</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>代表猫头鹰DL中的规范法规，用于自动化的合规性检查，由文本注释支持</title>
      <link>https://arxiv.org/abs/2504.05951</link>
      <description><![CDATA[ARXIV：2504.05951V1公告类型：新 
摘要：合规性检查是确定监管实体是否遵守这些法规的过程。当前，合规性检查主要是手动的，需要大量的时间和高技能的专家，同时仍然容易受到人为因素的错误。但是，已经探索了各种方法以自动化合规性检查，但是，尚未采用猫头鹰DL语言中的法规，这些法规尚未通过猫头鹰推理进行合规性检查。在这项工作中，我们提出了一个注释模式和一种算法，该算法将文本注释转换为机器解释的猫头鹰DL代码。提出的方法通过适用于建筑物施工领域的示例的概念验证实现来验证。]]></description>
      <guid>https://arxiv.org/abs/2504.05951</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>信息理论奖励分解可概括的RLHF</title>
      <link>https://arxiv.org/abs/2504.06020</link>
      <description><![CDATA[ARXIV：2504.06020V1公告类型：新 
摘要：可推广的奖励模型对于从人类反馈（RLHF）学习的强化至关重要，因为它可以正确评估看不见的及时响应对。但是，现有的奖励模型缺乏这种能力，因为它们通常是通过增加所选响应和拒绝响应之间的奖励差距来训练的，同时忽略了响应的提示。因此，当训练有素的奖励模型通过位于数据分布之外的迅速响应对评估时，忽略提示的效果可能会导致奖励模型的概括不佳。为了解决这个问题，我们将奖励价值分解为两个独立的组成部分：迅速的无奖励和迅速相关的奖励。迅速的奖励代表仅由响应决定的评估，而及时相关的奖励反映了从提示和响应中获得的奖励。我们从信息理论的角度提取这两个组件，这不需要额外的模型。随后，我们通过根据数据示例迅速奖励值确定数据样本来提出一种新的奖励学习算法。通过玩具示例，我们证明了提取的及时及其及时相关的奖励有效地表征了奖励模型的两个部分。此外，标准评估表明，我们的方法提高了奖励模型的一致性性能和概括能力。]]></description>
      <guid>https://arxiv.org/abs/2504.06020</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Leanabell-Prover：正式推理中的训练后缩放</title>
      <link>https://arxiv.org/abs/2504.06122</link>
      <description><![CDATA[ARXIV：2504.06122V1公告类型：新 
摘要：通过LLM的自动定理证明（ATP）的最新进展突出了使用4个代码正式推理的潜力。但是，正如开放的AI O1/O3和DeepSeek R1所证明的那样，最近的训练后缩放率尚未彻底改变ATP。在这项工作中，我们研究了ATP的整个训练后培训，旨在使其与自然语言的推理模型中的突破保持一致。要开始，我们将与混合数据集进行连续训练当前的ATP模型，该模型包括许多语句对 - 备用介绍对，以及旨在融合人类推理和假设的认知行为的其他数据。接下来，我们通过使用LEAN 4编译器返回的结果奖励来探索强化学习。通过我们设计的持续培训和强化学习过程，我们成功地改善了现有的正式掠夺者，包括DeepSeek-Prover-V1.5和Goedel-Prover，在全能一代领域中实现了最先进的表现。例如，我们在minif2f上实现了59.8％的通过率（通过@32）。这是一个正在进行的项目，我们将逐步更新我们的发现，发布数据和培训详细信息。]]></description>
      <guid>https://arxiv.org/abs/2504.06122</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>分散的AI内存：Shimi，可扩展代理推理的语义层次记忆索引</title>
      <link>https://arxiv.org/abs/2504.06135</link>
      <description><![CDATA[ARXIV：2504.06135V1公告类型：新 
摘要：检索型发电（RAG）和基于向量的搜索已成为AI系统中内存的基础工具，但它们在抽象，可扩展性和语义精度方面挣扎，尤其是在分散的环境中。我们提出了Shimi（语义层次内存索引），这是一种统一的体系结构，将知识建模为概念的动态结构化层次结构，使代理能够基于含义而不是表面相似性来检索信息。 Shimi将记忆组织为分层的语义节点，并支持从抽象意图到特定实体的自上而下的遍历，提供更精确和可解释的检索。至关重要的是，Shimi是针对分散的生态系统的本地设计的，在该生态系统中，代理维护本地记忆树并在整个网络之间异步地同步它们。我们介绍了一个轻巧的同步协议，该协议利用Merkle-DAG摘要，Bloom过滤器和CRDT式冲突解决方案，以使部分同步与最小的开销。通过涉及分散剂协作的基准实验和用例，我们证明了Shimi在检索准确性，语义保真度和可伸缩性方面的优势 - 将其定位为分散认知系统的核心基础设施层。]]></description>
      <guid>https://arxiv.org/abs/2504.06135</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>技能流：通过沟通适应AI代理的有效技能和代码转移</title>
      <link>https://arxiv.org/abs/2504.06188</link>
      <description><![CDATA[ARXIV：2504.06188V1公告类型：新 
摘要：AI代理是自主系统，可以基于预定义的编程执行特定任务。在这里，我们提出了SkillFlow，这是一个模块化的技术不足的框架，可通过从其环境或其他代理商中获取新技能来以临时方式扩展其功能。我们提出了一个理论模型，该模型在此框架下检查了哪个条件将是有益的，然后我们探讨了Skillflow加速任务完成并导致实际应用程序中累积成本降低的能力，即为日历事件安排代理。我们证明，在一些迭代中，SkillFlow会导致相当大的时间（24.8％，p值= $ 6.4 \ times10^{ -  3} $）在时间和成本上增加，尤其是在通信成本很高的情况下。最后，我们从研究良好的生物系统中绘制了类比，并将该框架与侧基基因转移的框架进行了比较，这是新型环境中适应性和进化的重要过程。]]></description>
      <guid>https://arxiv.org/abs/2504.06188</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>