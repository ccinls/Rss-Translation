<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.ai更新在arxiv.org上</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.ai在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Mon, 10 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>Perpo：通过判别奖励优化感知偏好优化</title>
      <link>https://arxiv.org/abs/2502.04371</link>
      <description><![CDATA[ARXIV：2502.04371V1公告类型：新 
摘要：本文介绍了感知偏好优化（PERPO），这是一种旨在应对生成预训练的预训练的多模式大语言模型（MLLM）的视觉歧视挑战的感知对齐方法。为了使MLLM与人类的视觉感知过程保持一致，Perpo采用歧视性奖励来收集各种负面样本，然后进行列表偏好优化以对它们进行排名。通过利用奖励作为排名的定量保证金，我们的方法有效地弥合了生成性偏好优化和歧视性经验风险优化的方法最小化。 Perpo显着增强了MLLM的视觉歧视能力，同时保持其生成优势，减轻图像无关奖励黑客入侵，并确保在视觉任务中保持一致的性能。这项工作标志着朝着更感知和多功能的MLLM迈出的至关重要的一步。我们还希望Perpo鼓励社区重新考虑MLLM对齐策略。]]></description>
      <guid>https://arxiv.org/abs/2502.04371</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>代理是依赖框架的</title>
      <link>https://arxiv.org/abs/2502.04403</link>
      <description><![CDATA[ARXIV：2502.04403V1公告类型：新 
摘要：代理是一种系统的能力，可以指导目标实现目标，并且是跨生物学，哲学，认知科学和人工智能研究的中心主题。确定系统展览机构是否是一个众所周知的困难问题：例如，丹内特（Dennett，1989）突出了确定哪些原理可以决定岩石，恒温器或机器人是否拥有代理机构的难题。我们在这里从强化学习的角度来解决这个难题，即认为代理在根本上取决于框架：对系统代理的任何测量都必须相对于参考框架进行。我们通过提出一个哲学论点来支持这一主张，即Barandiaran等人提出的代理的每个基本特性。 （2009）和Moreno（2018）本身是依赖框架的。我们得出的结论是，任何代理基础科学都需要框架依赖性，并讨论这种主张对强化学习的含义。]]></description>
      <guid>https://arxiv.org/abs/2502.04403</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>安全对于负责任的开放式系统至关重要</title>
      <link>https://arxiv.org/abs/2502.04512</link>
      <description><![CDATA[ARXIV：2502.04512V1公告类型：新 
摘要：AI的进步是由基础模型和好奇心驱动的学习旨在提高能力和适应性的结合而显着驱动的。该领域中不断增长的感兴趣领域是开放性的 -  AI系统能够连续，自主地产生新颖和多样化的文物或解决方案的能力。这已经与加速科学发现并在AI代理中持续适应有关。该立场论文认为，开放式AI的固有动态和自我传播性质引入了重要的，不受欢迎的风险，包括维持一致性，可预测性和控制的挑战。本文系统地研究了这些挑战，提出缓解策略，并呼吁不同利益相关者支持开放式AI的安全，负责任和成功发展。]]></description>
      <guid>https://arxiv.org/abs/2502.04512</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有连续奖励域的强大概率模型检查</title>
      <link>https://arxiv.org/abs/2502.04530</link>
      <description><![CDATA[ARXIV：2502.04530V1公告类型：新 
摘要：概率模型传统上检查了感兴趣量度的预期价值的属性。该限制可能无法捕获系统运行的相当一部分服务的服务质量，尤其是当由于重尾行为或多种模式而导致其预期价值的概率分布不当时。最新的作品灵感来自分布强化学习使用离散直方图以近似整数奖励分布的启发，但它们在持续的奖励空间中挣扎，并在平衡准确性和可扩展性方面面临挑战。我们提出了一种新的方法，用于使用与Erlang混合物匹配的力矩匹配在离散时间马尔可夫链中处理连续和离散奖励分布。通过分析通过时刻生成函数得出高阶矩，我们的方法在保留真实分布的统计属性的同时，用理论上有界误差近似奖励分布。这种详细的分布见解可以根据整个奖励分布功能对质量属性进行配方和鲁棒模型检查，而不是限制其预期价值。我们包括一个理论基础，以确保有限的近似误差，以及实验评估，证明了我们方法在实际模型检查问题中的准确性和可伸缩性。]]></description>
      <guid>https://arxiv.org/abs/2502.04530</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过顺序决策制作统一和优化选择数据值</title>
      <link>https://arxiv.org/abs/2502.04554</link>
      <description><![CDATA[ARXIV：2502.04554V1公告类型：新 
摘要：数据选择已成为数据评估的关键下游应用。尽管现有的数据评估方法在选择任务中已经显示出希望，但使用数据值进行选择的理论基础和全部潜力仍然在很大程度上没有探索。在这项工作中，我们首先证明了用于选择的数据值可以自然地重新重新构建为顺序决定问题，其中最佳数据值可以通过动态编程得出。我们显示该框架通过近似动态编程的镜头统一并重新诠释了现有的方法，例如数据沙普利，特别是作为对这个顺序问题的近视奖励函数近似。此外，我们分析了当基真实效用函数表现出单调的曲率单调性函数时，顺序数据选择最佳性如何受到影响。为了解决获得最佳数据值的计算挑战，我们提出了使用学识渊博的两分图作为替代实用程序模型的有效近似方案，当正确指定和学习替代替代工具时，确保贪婪选择仍然是最佳选择。广泛的实验证明了我们在不同数据集中方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2502.04554</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过对比分歧的优先优化：您的奖励模型是秘密的NLL估计器</title>
      <link>https://arxiv.org/abs/2502.04567</link>
      <description><![CDATA[ARXIV：2502.04567V1公告类型：新 
摘要：现有关于偏好优化的研究（PO）的重点是在简单的启发式方法之后构建成对偏好数据，例如最大程度地基于人（或AI）排名分数，最大程度地提高了优先和分配的完成之间的余量。但是，这些启发式方法都没有完全理论上的理由。在这项工作中，我们开发了一个新颖的PO框架，该框架提供了理论指导，可以有效地采样剥夺的完成。为了实现这一目标，我们将PO制定为最小化概率模型的负模样（NLL），并建议通过采样策略估算其归一化常数。正如我们将证明的那样，这些估计样品可以作为PO中的分配完成。然后，我们选择对比度差异（CD）作为采样策略，并提出了一种新型的MC-PO算法，该算法将Monte Carlo（MC）内核从CD应用于样品硬质量负质量W.R.T.参数化奖励模型。最后，我们提出了ONMC-PO算法，这是MC-PO的扩展到在线设置。在流行的一致性基准上，MC-PO的表现优于现有的SOTA基线，而ONMC-PO可以进一步改进。]]></description>
      <guid>https://arxiv.org/abs/2502.04567</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>代理推理：使用工具进行深入研究的推理LLM</title>
      <link>https://arxiv.org/abs/2502.04644</link>
      <description><![CDATA[ARXIV：2502.04644V1公告类型：新 
摘要：我们介绍了代理推理，该框架通过整合使用外部工具的代理来增强大型语言模型（LLM）推理。与传统的基于LLM的推理方法不同，这些方法仅依赖于内部推理，代理推理动态参与Web搜索，代码执行和结构化的推理 - 上下文内存，以解决需要深入研究和多步逻辑扣除的复杂问题。我们的框架介绍了思维地图代理，该框架构建了一个结构化的知识图来跟踪逻辑关系，从而改善了演绎推理。此外，网络搜索和编码代理的集成可以实时检索和计算分析，从而提高了推理准确性和决策。对博士学位科学推理（GPQA）和特定领域的深入研究任务的评估表明，我们的方法显着胜过现有模型，包括领先的检索功能增强生成（RAG）系统和封闭源LLM。此外，我们的结果表明，代理推理可以改善专家级知识的综合，测试时间可伸缩性和结构化问题解决。该代码为：https：//github.com/theworldofagents/agentic-reasoning。]]></description>
      <guid>https://arxiv.org/abs/2502.04644</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>$ {\ rm p {\小屋顶}</title>
      <link>https://arxiv.org/abs/2502.04671</link>
      <description><![CDATA[ARXIV：2502.04671V1公告类型：新 
摘要：神经网络在交互式证明助理（ITP）等自动定理方面表现出了巨大的希望。但是，大多数神经定理模型仅限于特定的ITP，从而在ITP之间提供了跨语义$ ​​\ textit {Transfer} $的机会。我们通过多语言证明框架来解决这个弱点，$ {\ rm p {\ small fore} w {\ small ala}} $，允许神经定理 - 示威者和两个已建立的ITP（COQ和LEAN）之间的标准化形式的相互作用形式。它可以收集多种语言证明步骤数据（数据记录ITP状态上的证明操作结果），以培训神经掠夺者。 $ {\ rm p {\小屋顶} w {\ small ala}} $允许通过有效的并行证明搜索算法系统地评估模型在不同的ITP和问题域上的模型性能。我们表明，由$ {\ rm p {\ small flow} w {\ small ala}} $启用的多语言培训可以导致成功通过ITP的传输。具体而言，一种在$ {\ rm p {\ small flow} w {\ small ala}} $的混合物中训练的模型 - 生成的coq和精益数据优于仅优于标准的prove-at-at-$的单只能模型K $公制。我们为$ \ href {https://github.com/trishullab/proof-wala} {prociquwala \;开放了所有代码，包括$ \ href {https://github.com/trishullab/proof-wala \;框架} $，以及$ \ href {https://github.com/trishullab/itp-interface} {多语言\; ITP \;相互作用\;框架} $。]]></description>
      <guid>https://arxiv.org/abs/2502.04671</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过递归自我征收对超人AI的可扩展监督</title>
      <link>https://arxiv.org/abs/2502.04675</link>
      <description><![CDATA[ARXIV：2502.04675V1公告类型：新 
摘要：随着AI能力越来越超过人类在复杂任务中的熟练程度，包括SFT和RLHF在内的当前一致性技术在确保可靠的监督方面面临着根本的挑战。这些方法依赖于直接的人类评估，并且当AI输出超过人类认知阈值时变得站不住脚。为了应对这一挑战，我们探讨了两个假设：（1）对批评的批判比批评本身更容易，扩展了广泛认可的观察结果，即验证比生成比批评领域更容易，因为批评本身是一种专业的一代形式; （2）这种难度关系被递归地持有，表明当直接评估是不可行的，进行高级批评（例如，对批评的批评的批评）提供了更可拖延的监督途径。为了审查这些假设，我们在多个任务中执行人类，人类和AI-AI实验。我们的结果表明了支持这些假设的令人鼓舞的证据，并表明递归自我批判是可扩展监督的有前途的方向。]]></description>
      <guid>https://arxiv.org/abs/2502.04675</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过迭代潜在空间政策优化学习狼人游戏中的战略语言代理</title>
      <link>https://arxiv.org/abs/2502.04686</link>
      <description><![CDATA[ARXIV：2502.04686V1公告类型：新 
摘要：基于大型语言模型（LLM）的代理最近在各种领域中显示出令人印象深刻的进展，包括开放式对话和多步骤决策。但是，将这些代理商应用于需要战略决策和自由形式语言互动的狼人等社会扣除游戏，这仍然是无聊的。基于反事实遗憾最小化（CFR）或增强学习（RL）的传统方法通常取决于预定义的动作空间，这使得它们不适合具有不受限制的文本动作空间的语言游戏。同时，纯LLM的代理通常会遭受固有的偏见，并且需要大型数据集进行微调。我们提出了潜在的空间策略优化（LSPO），这是一个迭代框架，通过首先将自由形式文本映射到离散的潜在空间来解决这些挑战，其中CFR和RL等方法可以更有效地学习战略策略。然后，我们将学习的策略重新转换为自然语言对话，这些对话用于通过直接偏好优化（DPO）微调LLM。通过在这些阶段进行迭代交替，我们的LSPO代理逐渐增强了战略推理和语言交流。狼人游戏中的实验结果表明，我们的方法可以提高代理商在每次迭代中的表现，并胜过现有的狼人代理商，从而强调了其对自由形式语言决策的承诺。]]></description>
      <guid>https://arxiv.org/abs/2502.04686</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>弥合xai的差距，为什么可靠的指标对解释性和合规性很重要</title>
      <link>https://arxiv.org/abs/2502.04695</link>
      <description><![CDATA[ARXIV：2502.04695V1公告类型：新 
摘要：该立场论文强调了评估可解释AI（XAI）的关键差距，因为缺乏标准化和可靠的指标，从而降低了其实际价值，可信赖性和满足监管要求的能力。当前的评估方法通常是分散的，主观的和偏见的，使它们容易操纵和复杂复杂模型的评估。一个核心问题是缺乏解释的基础真理，使各种XAI方法的比较变得复杂。为了应对这些挑战，我们倡导广泛的研究来开发强大的，上下文敏感的评估指标。这些指标应抵抗与每个用例有关的操纵，并基于人类判断和现实世界的适用性。我们还建议创建特定领域的评估基准，以与医疗保健和金融等领域的用户和监管需求保持一致。通过鼓励学术界，行业和监管机构之间的合作，我们可以创建平衡灵活性和一致性的标准，确保XAI解释有意义，值得信赖，并且符合不断发展的法规。]]></description>
      <guid>https://arxiv.org/abs/2502.04695</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过大型语言模型的测试时间缩放生成符号世界模型</title>
      <link>https://arxiv.org/abs/2502.04728</link>
      <description><![CDATA[ARXIV：2502.04728V1公告类型：新 
摘要：解决复杂的计划问题需要大型语言模型（LLMS）明确对状态过渡进行建模，以避免规则违规，遵守限制，并确保最优性受到自然语言固有的歧义的阻碍。为了克服这种歧义，规划域定义语言（PDDL）被杠杆化作为一种​​计划抽象，可实现精确和正式的状态描述。使用PDDL，我们可以生成一个符号世界模型，在该模型中，可以无缝地应用经典的搜索算法（例如A*）来查找最佳计划。但是，由于缺乏PDDL培训数据，直接生成具有当前LLM的PDDL域仍然是一个开放的挑战。为了应对这一挑战，我们建议扩大LLMS的测试时间计算，以增强其PDDL推理功能，从而使高质量的PDDL域的产生。具体来说，我们引入了一种简单而有效的算法，该算法首先采用了N最佳采样方法来提高初始解决方案的质量，然后通过口头化的机器学习以细粒度的方式完善解决方案。我们的方法在PDDL域的生成中的大幅度优于O1-Mini，在两个任务上达到了超过50％的成功率（即从自然语言描述或PDDL问题生成PDDL域）。这是在不需要额外培训的情况下完成的。通过利用PDDL作为状态抽象，我们的方法能够在几乎所有竞争级的计划任务上都超过当前最新方法。]]></description>
      <guid>https://arxiv.org/abs/2502.04728</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>小天狼星：通过自举推理自我提出的多代理系统</title>
      <link>https://arxiv.org/abs/2502.04780</link>
      <description><![CDATA[ARXIV：2502.04780V1公告类型：新 
摘要：由大型语言模型（LLM）提供动力的多代理AI系统越来越多地用于解决复杂的任务。但是，这些系统通常依赖于脆弱的，手动设计的提示和启发式方法，从而使优化变得困难。优化多代理系统的关键挑战是为专用代理购买合适的培训数据。我们介绍了Sirius，这是一个为多代理系统的自我改进，推理驱动的优化框架。我们方法的核心是建造体验库：高质量推理轨迹的存储库。图书馆是通过保留导致成功结果的推理步骤来构建的，为优化多代理系统提供了强大的培训集。此外，我们介绍了一个图书馆的扩展程序，该程序可以完善不成功的轨迹，从而进一步丰富了库。 Sirius在推理和生物医学质量检查方面将绩效提高2.86 \％\％\％，并增强竞争环境中的代理商谈判。我们的结果表明，Sirius在未来生成可重复使用的数据以增强多代理性能的同时，将来可以增强自我纠正和自我播放。]]></description>
      <guid>https://arxiv.org/abs/2502.04780</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>关于顺序的故障智能过程计划</title>
      <link>https://arxiv.org/abs/2502.04998</link>
      <description><![CDATA[ARXIV：2502.04998V1公告类型：新 
摘要：我们提出并研究了一个计划问题，我们称之为顺序的故障智能过程计划（SFIPP）。 SFIPP捕获了许多顺序多阶段决策问题中常见的奖励结构，在所有阶段成功的情况下，该计划才被认为是成功的。这种奖励结构与经典的添加奖励结构不同，并且在重要的应用中出现，例如药物/材料发现，安全性和质量关键产品设计。我们为设置设计了非常紧密的在线算法，在每个阶段，我们需要在不同的动作之间进行选择。我们这样做是为了确定性行为是确定性的基础案例，以及概率行动结果的案例，在这些情况下，我们有效地平衡了通过使用多武器的匪徒bandit算法来进行学习和开发的探索。在我们的经验评估中，我们证明了我们开发的专门算法，它利用有关SFIPP实例结构的其他信息，优于我们更一般的算法。]]></description>
      <guid>https://arxiv.org/abs/2502.04998</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>分析先进的AI系统针对生命和意识的定义</title>
      <link>https://arxiv.org/abs/2502.05007</link>
      <description><![CDATA[ARXIV：2502.05007V1公告类型：新 
摘要：在功能意义上，人工智能可以真正意识到吗？本文探讨了通过生活的镜头探索开放性的问题，这个概念统一了具有经验标志的经验标志，例如自适应自我维护，新兴的复杂性和基本的自我参考建模。我们提出了许多指标，以检查先进的AI系统是否已获得意识，同时强调我们不声称所有AI词干都会变得有意识。相反，我们建议表现出足够高级的体系结构，表现出免疫力，例如破坏性防御，镜像自我识别类似物或元认知更新可能会跨越类似于类似于生活的或意识的特征的钥匙阈值。为了证明这些想法，我们首先要评估自适应自我维护能力，并将受控数据腐败破坏到培训过程中。结果证明了AI检测这些不一致之处并恢复或自我校正的能力类似于再生生物学过程。我们还将动物启发的镜像自识别测试适应神经嵌入，发现经过部分训练的CNN可以完全准确地将自我与外国特征区分开。然后，我们通过对五个最先进的聊天机器人（Chatgpt4，Gemini，Gemini，Cllexity，Claude和Copilot）进行基于问题的镜像测试来扩展分析，并证明了与其他人相比，他们能够识别自己的答案聊天机器人。]]></description>
      <guid>https://arxiv.org/abs/2502.05007</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>组合数据的计算和学习</title>
      <link>https://arxiv.org/abs/2502.05063</link>
      <description><![CDATA[ARXIV：2502.05063V1公告类型：新 
摘要：二十一世纪是一个以数据为导向的时代，人类的活动和行为，身体现象，科学发现，技术进步以及世界上发生的几乎所有发生的事情，导致数据的大量产生，收集和利用。
  数据中的连接性是关键属性。一个直接的示例是万维网，每个网页都通过超链接连接到其他网页，提供了一种定向连接的形式。组合数据是指基于某些连接规则的数据项的组合。其他形式的组合数据包括社交网络，网格，社区簇，集合系统和分子。
  这个博士论文侧重于与组合数据学习和计算。我们研究和研究连接数据内部和整个连接数据中的拓扑和连通性特征，以提高学习的性能并实现高算法效率。]]></description>
      <guid>https://arxiv.org/abs/2502.05063</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>思想自适应图：测试时间自适应推理统一链，树和图形结构</title>
      <link>https://arxiv.org/abs/2502.05078</link>
      <description><![CDATA[ARXIV：2502.05078V1公告类型：新 
摘要：大型语言模型（LLM）表现出了令人印象深刻的推理能力，但它们的性能高度取决于提示策略和模型量表。尽管已经部署了加强学习和微调来提高推理，但这些方法会引起大量的计算和数据开销。在这项工作中，我们介绍了思想的自适应图（AGOT），这是一种基于图形的动态推理框架，仅在测试时增强LLM推理。 Agot不依赖固定步骤（COT）或思想树（TOT）之类的固定阶段方法，而是递归地将复杂的查询分解为结构化的子问题，形成了相互依​​存的推理步骤的动态定向的无环图（DAG）。通过仅选择性地将需要进一步分析的子问题扩展，AGOT将链，树和图形范式的优势统一到一个凝聚力的框架中，该框架分配了最需要的计算。我们验证了涵盖多跳检索，科学推理和数学问题解决问题的各种基准测试的方法，可在科学推理任务（GPQA）提高46.2％（GPQA），可与通过计算强化的强化学习方法和超级强化的状态 -   - 可相当迭代的方法。这些结果表明，动态分解和结构化递归为训练后修饰提供了可扩展的，具有成本效益的替代方案，为LLMS中更健壮的通用推理铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2502.05078</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Snap Social Circles数据集的社区检测算法的比较分析</title>
      <link>https://arxiv.org/abs/2502.04341</link>
      <description><![CDATA[Arxiv：2502.04341V1公告类型：交叉 
摘要：在网络研究中，社区检测一直是对网络科学的重大兴趣的话题，其中许多论文和算法提议揭示网络内的基础结构。在本文中，我们对适用于Facebook社交媒体网络的SNAP Social Circles数据集的几种著名社区检测算法进行了比较分析。实施的算法包括Louvain，Girvan-Newman，频谱聚类，K-均值聚类等。我们根据各种指标，例如模块化，归一化的切割比率，silhouette，silhouette评分，紧凑度和分离性评估了这些算法的性能。我们的发现揭示了对每种算法在检测社交网络中各种有意义社区的有效性的见解，从而阐明了它们的力量和局限性。这项研究有助于了解社区检测方法，并为其在分析现实世界社交网络中的应用提供宝贵的指导。]]></description>
      <guid>https://arxiv.org/abs/2502.04341</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>关于使用机器学习和深度学习模型进行精神疾病检测的教程</title>
      <link>https://arxiv.org/abs/2502.04342</link>
      <description><![CDATA[ARXIV：2502.04342V1公告类型：交叉 
摘要：社交媒体已成为理解心理健康的重要来源，为研究人员提供了一种检测诸如用户生成的职位抑郁症之类的疾病的方法。本教程提供了实用的指导，以应对在这些平台上应用机器学习和深度学习方法的共同挑战。它着重于使用不同数据集的策略，改善文本预处理以及解决诸如不平衡数据和模型评估等问题。现实世界中的例子和分步说明演示了如何有效地应用这些技术，重点是透明度，可重复性和道德考虑。通过共享这些方法，该教程旨在帮助研究人员建立更可靠且广泛适用的心理健康研究模型，从而有助于更好的早期检测和干预工具。]]></description>
      <guid>https://arxiv.org/abs/2502.04342</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Jingfang：一种传统的中药大型语言专家级医学诊断和基于综合症的治疗</title>
      <link>https://arxiv.org/abs/2502.04345</link>
      <description><![CDATA[ARXIV：2502.04345V1公告类型：交叉 
摘要：中医（TCM）在健康保护和疾病治疗中起着至关重要的作用，但其实际应用需要广泛的医学知识和临床经验。现有的TCM大语言模型（LLMS）表现出对医学咨询和诊断和基于不准确的综合症治疗的严重局限性。为了解决这些问题，这项研究建立了Jingfang（JF）：一种新型的TCM大语言模型，该模型展示了医学诊断和基于综合征分化的治疗的专家水平能力。我们为医疗咨询而创新了一个多代理动态协作链机制（MDCCTM），以有效且准确的诊断能力使JF能够实现JF。此外，开发了综合征剂和双阶段检索方案（DSR），以显着增强基于综合征分化的JF治疗的JF的能力。 Jingfang不仅促进了LLM的应用，而且还促进了TCM在人类健康保护和疾病治疗中的有效实践。]]></description>
      <guid>https://arxiv.org/abs/2502.04345</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>