<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.AI 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.AI 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Fri, 24 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>通过订单聚合实现并行信念收缩</title>
      <link>https://arxiv.org/abs/2501.13295</link>
      <description><![CDATA[arXiv:2501.13295v1 公告类型：新
摘要：信念收缩的标准“串行”（又名“单例”）模型模拟​​了代理的信念语料库对单个信息项的删除做出反应的方式。该模型的一个显着扩展引入了“并行”（又名“包”或“多个”）更改的概念，其中同时删除了一整套信息项。对后者的现有研究主要集中在单步并行收缩：理解单个并行收缩后的信念行为。它还专注于对特征属性极弱的串行收缩操作的并行情况的概括。在这里，我们考虑如何扩展遵循更强属性的串行收缩操作。可能更重要的是，我们还考虑了迭代情况：一系列并行收缩之后的信念行为。我们提出了一种通用方法，用于扩展串行迭代信念改变算子以处理基于 Booth &amp; Chandler 的 TeamQueue 二进制顺序聚合器的 n 元泛化的并行改变。]]></description>
      <guid>https://arxiv.org/abs/2501.13295</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>人工智能人格理论</title>
      <link>https://arxiv.org/abs/2501.13533</link>
      <description><![CDATA[arXiv:2501.13533v1 公告类型：新
摘要：我是一个人，你也是。从哲学上讲，我们有时会赋予非人类动物人格，主权国家或公司等实体在法律上可以被视为人。但是，我们什么时候应该将人格赋予人工智能系统？在本文中，我们概述了人工智能人格的必要条件，重点关注代理、心智理论和自我意识。我们讨论了机器学习文献中关于当代人工智能系统（如语言模型）满足这些条件的程度的证据，发现证据令人惊讶地不确定。
如果人工智能系统可以被视为人，那么人工智能对齐的典型框架可能不完整。虽然文献中已经详细讨论了代理，但人格的其他方面却相对被忽视了。人工智能代理通常被认为追求固定的目标，但人工智能人可能具有足够的自我意识来反思他们的目标、价值观和在世界上的地位，从而促使他们的目标发生改变。我们重点介绍了开放的研究方向，以促进对人工智能人格及其与协调的相关性的理解。最后，我们反思了围绕人工智能系统治疗的伦理考量。如果人工智能系统是人，那么寻求控制和协调在伦理上可能是站不住脚的。]]></description>
      <guid>https://arxiv.org/abs/2501.13533</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>由粗到细的过程奖励建模增强数学推理能力</title>
      <link>https://arxiv.org/abs/2501.13622</link>
      <description><![CDATA[arXiv:2501.13622v1 公告类型：新
摘要：过程奖励模型（PRM）对于数学推理任务至关重要，因为它可以为每个中间步骤分配奖励。PRM 需要构建过程监督数据进行训练，这依赖于思路链（CoT）或基于树的方法来构建推理步骤，然而，单个推理步骤可能是冗余的或包含难以检测的细微错误。我们将这些归因于在过程数据收集过程中忽视粒度划分的问题。在本文中，我们提出了一个由粗到细的框架来解决这个问题。具体而言，在收集过程监督数据时，我们通过按照预设的合并粒度合并相邻步骤来收集粗推理步骤，然后我们依次降低合并粒度以收集细粒度推理步骤。对于每个合成的新步骤，我们根据上一步的标签重新标记。在训练过程中，我们还以由粗到细的方式遍历收集到的训练语料。我们在流行的数学推理数据集上针对不同的损失标准进行了广泛的实验，所提出的框架可以持续提高推理性能。]]></description>
      <guid>https://arxiv.org/abs/2501.13622</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过基于张量的线性时间逻辑在有限迹上进行形式验证的神经符号轨迹学习</title>
      <link>https://arxiv.org/abs/2501.13712</link>
      <description><![CDATA[arXiv:2501.13712v1 公告类型：新
摘要：我们提出了一种用于有限迹线性时态逻辑 (LTLf) 的张量语义的新形式化，并在定理证明器 Isabelle/HOL 中进行了正确性的形式化证明。我们证明，通过定义和验证 LTLf 约束的可微损失函数，并自动生成与 PyTorch 集成的实现，可以将这种形式化集成到神经符号学习过程中。我们表明，通过使用这种损失，该过程可以学习满足预先指定的逻辑约束。我们的方法为约束训练提供了一个完全严格的框架，消除了在“不安全”的编程语言（如 Python）中直接临时、手动实现逻辑方面的许多固有风险，同时保持了实施效率。]]></description>
      <guid>https://arxiv.org/abs/2501.13712</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 LTL 运算符确定线性单子数据日志查询的数据复杂度（扩展版）</title>
      <link>https://arxiv.org/abs/2501.13762</link>
      <description><![CDATA[arXiv:2501.13762v1 公告类型：新
摘要：我们关注的是回答线性单子数据日志查询的数据复杂性，这些查询的规则主体中的原子可以以线性时间逻辑 LTL 的运算符为前缀。我们首先观察到，对于数据复杂性，回答任何带有运算符 $\bigcirc/\bigcirc^-$ 的连接查询（在下一个/上一个时刻）要么在 AC0 中，要么在 $ACC0\!\setminus\!AC0$ 中，要么在 $NC^1$ 完全中，要么在 LogSpace-hard 中，要么在 NLogSpace 中。然后我们表明，决定回答此类查询的 LogSpace-hard 性的问题是 PSpace-complete，而检查 AC0 和 ACC0 类中的成员资格以及 $NC^1$ 完全性可以在 ExpSpace 中完成。最后，我们证明，如果 $NC^1 \ne NLogSpace$ 和 $LogSpace \ne NLogSpace$，则对于使用运算符 $\Diamond_f/\Diamond_p$（将来/过去某个时间）的查询，AC0 或 ACC0、$NC^1$ 完整性和 LogSpace 难度中的成员资格是不可判定的。]]></description>
      <guid>https://arxiv.org/abs/2501.13762</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>确保医疗 AI 安全：可解释的 AI 驱动的虚假模型行为和相关数据检测与缓解</title>
      <link>https://arxiv.org/abs/2501.13818</link>
      <description><![CDATA[arXiv:2501.13818v1 公告类型：新
摘要：尽管深度神经网络在存在虚假相关性的情况下倾向于捷径学习，但其在高风险医疗应用中的应用日益广泛，这在实践中可能会产生致命的后果。检测和缓解捷径行为是一项具有挑战性的任务，通常需要领域专家进行大量的标记工作。为了缓解这个问题，我们引入了一个半自动化框架，利用可解释人工智能 (XAI) 的见解，从数据和模型的角度识别虚假行为。这允许检索虚假数据点并检测编码相关预测规则的模型电路。此外，我们展示了如何将这些快捷方式编码用于基于 XAI 的样本和像素级数据注释，为偏差缓解方法提供有价值的信息，以消除不良的捷径行为。我们使用两种模式下的四个医疗数据集展示了我们框架的适用性，这些数据集具有由数据伪影引起的受控和现实世界的虚假相关性。我们成功识别并缓解了 VGG16、ResNet50 和当代 Vision Transformer 模型中的这些偏差，最终提高了它们对现实世界医疗任务的稳健性和适用性。]]></description>
      <guid>https://arxiv.org/abs/2501.13818</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>论人工智能模型的推理能力及其量化方法</title>
      <link>https://arxiv.org/abs/2501.13833</link>
      <description><![CDATA[arXiv:2501.13833v1 公告类型：新
摘要：大型语言模型 (LLM) 的最新进展加剧了围绕其推理能力基本性质的争论。虽然这些模型在 GPQA 和 MMLU 等基准测试中取得了高性能，但它们在更复杂的推理任务中表现出局限性，凸显了对更严格的评估方法的需求。我们提出了一种新颖的现象学方法，它超越了传统的准确性指标来探究模型行为的潜在机制，建立了一个可以广泛影响我们分析和理解人工智能系统的框架。以多项选择推理任务中的位置偏差为例，我们展示了系统性扰动如何揭示模型决策的基本方面。为了分析这些行为，我们开发了两个互补的现象学模型：一个概率混合模型 (PMM)，将模型响应分解为推理、记忆和猜测部分，以及一个信息理论一致性 (ITC) 分析，量化模型置信度和策略选择之间的关系。通过对推理基准进行受控实验，我们表明，真正的推理对于当前的模型来说仍然具有挑战性，表面上的成功往往依赖于复杂的记忆和模式匹配组合，而不是真正的逻辑推理。更根本的是，我们证明，单凭准确性往往会夸大模型的推理能力，因为模型行为可以通过认知策略相空间中的潜在机制来表征，揭示模型在响应查询时如何动态平衡不同的方法。该框架为现实世界的部署提供了定量标准，允许应用程序根据策略分布而不是聚合性能指标来指定可靠性阈值。]]></description>
      <guid>https://arxiv.org/abs/2501.13833</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>微笑背后的匕首：用幸福结局的故事愚弄法学硕士</title>
      <link>https://arxiv.org/abs/2501.13115</link>
      <description><![CDATA[arXiv:2501.13115v1 公告类型：交叉 
摘要：大型语言模型 (LLM) 的广泛采用引起了 \textit{越狱} 攻击的极大关注，其中通过优化或手动设计制作的对抗性提示利用 LLM 生成恶意内容。然而，基于优化的攻击效率和可转移性有限，而手动设计要么容易被检测到，要么需要与 LLM 进行复杂的交互。在本文中，我们首先指出了越狱攻击的一个新视角：LLM 对 \textit{positive} 提示更敏感。基于此，我们部署了 Happy Ending Attack (HEA) 以将恶意请求包装在一个场景模板中，该场景模板主要通过 \textit{happy ending} 形成积极提示，从而欺骗 LLM 立即越狱或在后续恶意请求时越狱。这使得 HEA 既高效又有效，因为它只需要最多两个步骤就可以完全越狱 LLM。大量实验表明，我们的 HEA 可以成功越狱最先进的 LLM，包括 GPT-4o、Llama3-70b、Gemini-pro，平均攻击​​成功率达到 88.79%。我们还为 HEA 的成功提供了潜在的定量解释。]]></description>
      <guid>https://arxiv.org/abs/2501.13115</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MyGO Multiplex CoT：一种通过双思维链进行大型语言模型自我反思的方法</title>
      <link>https://arxiv.org/abs/2501.13117</link>
      <description><![CDATA[arXiv:2501.13117v1 公告类型：交叉 
摘要：大型语言模型 (LLM) 的最新进展已在各种推理和决策任务中展示了其令人印象深刻的能力。然而，推理过程的质量和连贯性仍然可以从增强的内省和自我反思中受益。在本文中，我们介绍了多重 CoT（思维链），这种方法通过启动双重思维链 (CoT) 思维，使 LLM 能够在推理时模拟一种自我审查形式。多重 CoT 利用迭代推理的力量，其中模型生成初始思维链，随后通过第二轮思维生成批评和改进这种推理。这种递归方法允许更连贯、更合乎逻辑和更强大的答案，从而改善整个决策过程。我们展示了如何在现有的 LLM 架构中使用简单的即时工程有效地实现此方法，从而实现与学习细化模型 (LRM) 类似的效果，而无需额外训练。此外，我们还提供了在 Google Colab 中实现该方法的实用指南，以便轻松集成到实际应用中。]]></description>
      <guid>https://arxiv.org/abs/2501.13117</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士设计的“不安分的土匪”奖励函数中的多语言性：对任务绩效和公平性的影响</title>
      <link>https://arxiv.org/abs/2501.13120</link>
      <description><![CDATA[arXiv:2501.13120v1 公告类型：交叉 
摘要：多臂老虎机 (RMAB) 已成功应用于各种环境中的资源分配问题，包括公共卫生。随着强大的大型语言模型 (LLM) 的快速发展，它们越来越多地用于设计奖励函数以更好地匹配人类偏好。最近的研究表明，LLM 可用于根据语言提示根据社区需求定制自动分配决策。然而，这主要针对英语提示进行研究，并且仅关注任务绩效。这可能是一个问题，因为基层工人，特别是在印度等发展中国家的工人，更喜欢使用当地语言工作，其中一些语言资源匮乏。此外，考虑到问题的性质，用户无意的人口群体偏见也是不可取的。在这项工作中，我们研究了当 DLM 算法（最近使用 LLM 为 RMAB 设计奖励函数的研究）以非英语语言命令提示时对任务绩效和公平性的影响。具体来说，我们在合成环境中针对翻译成多种语言的各种提示运行该模型。提示本身的复杂程度各不相同。我们的结果表明，与其他语言相比，LLM 提出的奖励函数在用英语提示时明显更好。我们还发现，提示的确切措辞会影响任务性能。此外，随着提示复杂性的增加，所有语言的性能都会变差；然而，英语提示的性能比资源较少的语言更稳定。在公平性方面，我们发现资源较少的语言和更复杂的提示都很可能在意想不到的维度上造成不公平。]]></description>
      <guid>https://arxiv.org/abs/2501.13120</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型的情景记忆生成和评估基准</title>
      <link>https://arxiv.org/abs/2501.13121</link>
      <description><![CDATA[arXiv:2501.13121v1 公告类型：交叉 
摘要：情景记忆——回忆基于时间和空间的特定事件的能力——是人类认知的基石，不仅可以实现连贯的故事讲述，还可以实现规划和决策。尽管大型语言模型 (LLM) 具有非凡的能力，但它们缺乏强大的情景记忆机制：我们认为，将情景记忆功能集成到 LLM 中对于推动 AI 向类似人类的认知发展至关重要，可以提高其一致推理的潜力并将其输出建立在现实世界的情景事件中，从而避免虚构。为了应对这一挑战，我们引入了一个全面的框架来建模和评估 LLM 情景记忆能力。从认知科学中汲取灵感，我们开发了一种结构化的方法来表示情景事件，封装时间和空间背景、涉及的实体和详细描述。我们合成了一个独特的、不受污染的情景记忆基准，并发布了开源代码和数据集，以评估 LLM 在各种回忆和情景推理任务中的表现。我们对最先进的模型（包括 GPT-4 和 Claude 变体、Llama 3.1 和 o1-mini）的评估表明，即使是最先进的 LLM 也会在情景记忆任务中遇到困难，尤其是在处理多个相关事件或复杂的时空关系时——即使在短至 10k-100k 个标记的上下文中也是如此。]]></description>
      <guid>https://arxiv.org/abs/2501.13121</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>零样本验证引导的思路链</title>
      <link>https://arxiv.org/abs/2501.13122</link>
      <description><![CDATA[arXiv:2501.13122v1 公告类型：交叉 
摘要：先前的研究已经证明了思维链 (COT) 提示和验证器在引导大型语言模型 (LLM) 完成推理空间方面的有效性。然而，大多数此类研究要么使用经过微调的验证器，要么依赖于手工制作的少量样本。相比之下，在本文中，我们专注于通过完全零样本机制中的 COT 提示对基于 LLM 的自生成推理步骤进行自我验证。为了探索这种设置，我们设计了一个新的零样本提示，我们称之为 COT STEP，以帮助零样本分解推理步骤，并为基于 LLM 的验证器设计了两个新的零样本提示。我们评估验证器对推理链正确性进行分类的能力，并探索使用验证器分数指导不同 LLM 的各种数学和常识推理任务推理的不同方法。]]></description>
      <guid>https://arxiv.org/abs/2501.13122</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>辩论有助于从弱到强的概括</title>
      <link>https://arxiv.org/abs/2501.13124</link>
      <description><![CDATA[arXiv:2501.13124v1 公告类型：交叉 
摘要：将已经具备能力的模型与所需行为对齐的常用方法依赖于人类提供监督的能力。然而，未来的超人模型将超越人类的能力。因此，人类只能对超人模型进行弱监督。这种预期的人类评估缺陷将削弱未来人工智能系统的安全性。可扩展监督和弱到强泛化是解决这一问题的两种互补方法。在本文中，我们试图结合这两种方法的优势来进一步改善对齐。具体来说，我们研究使用强大的预训练模型改进人工监督的方法，然后使用增强的弱人工监督来监督强模型。为了取得迭代的经验进展，我们考虑一个类比：我们可以使用强模型来改进弱模型监督，然后用它来监督强模型吗？我们通过在一个大型强模型的帮助下对一个小型弱模型在基本事实标签上进行微调，然后对由弱模型生成的标签上的强模型进行微调，从而对其进行了实证测试。我们发现辩论可以帮助弱模型从不可信的强模型中提取可信信息，这在训练弱模型时可以作为样本的背景。我们还表明，弱模型集合有助于利用强模型辩论者生成的长篇论据并获得更稳健的监督估计。在 OpenAI 弱到强 NLP 基准上进行的大量实验表明，组合方法可以实现更好的一致性，这表明辩论有可能帮助实现从弱到强的泛化。]]></description>
      <guid>https://arxiv.org/abs/2501.13124</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过学生选择预测为多项选择题生成合理的干扰项</title>
      <link>https://arxiv.org/abs/2501.13125</link>
      <description><![CDATA[arXiv:2501.13125v1 公告类型：交叉 
摘要：在教育设计多项选择题 (MCQ) 时，创建合理的干扰项对于识别学生的误解和知识差距以及准确评估他们的理解至关重要。然而，先前关于干扰项生成的研究并没有充分重视提高干扰项的难度，导致 MCQ 的有效性降低。本研究提出了一种训练模型以生成学生更有可能选择的干扰项的流程。首先，我们训练一个成对排序器来推理学生的误解并评估两个干扰项的相对合理性。使用该模型，我们创建了一个成对干扰项排名的数据集，然后通过直接偏好优化 (DPO) 训练干扰项生成器以生成更合理的干扰项。在计算机科学科目 (Python、DB、MLDL) 上的实验表明，我们的成对排序器可以有效识别学生的潜在误解，并实现与人类专家相当的排名准确度。此外，我们的干扰项生成器在生成合理的干扰项方面优于几个基线，并且产生具有更高项目辨别指数（DI）的问题。]]></description>
      <guid>https://arxiv.org/abs/2501.13125</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>偏好课程：法学硕士应始终接受其偏好数据的预训练</title>
      <link>https://arxiv.org/abs/2501.13126</link>
      <description><![CDATA[arXiv:2501.13126v1 公告类型：交叉 
摘要：当前的大型语言模型（LLM）通常在整个预训练过程中使用一致的数据分布。然而，随着模型能力的提高，直观地应该使用差异化的数据进行预训练。为了实现它，我们提出了基于困惑度差异的偏好课程学习（PDPC）框架，它始终感知并使用LLM偏好的数据来训练和提升它们。首先，我们引入PD指标来衡量强模型和弱模型对样本的拟合程度的差异。具有高PD的样本对于弱模型来说更难学习，更适合在预训练的后期进行安排。其次，我们提出PD偏好函数来近似模型并随时预测LLM的数据偏好，从而离线完成整个数据的安排并确保不间断的持续训练。在1.3B和3B模型上的实验结果表明我们的PDPC显著超越了基线。值得注意的是，3B 模型取得了更显著的收益，在各个基准测试中平均准确率提高了 4.1% 以上。]]></description>
      <guid>https://arxiv.org/abs/2501.13126</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>采用领导者-追随者策略的多无人机作战分层强化学习框架</title>
      <link>https://arxiv.org/abs/2501.13132</link>
      <description><![CDATA[arXiv:2501.13132v1 公告类型：交叉 
摘要：多无人机空战是一项涉及多架自主无人机的复杂任务，是航空航天和人工智能领域的一个不断发展的领域。本文旨在通过协作策略提高对抗性能。以前的方法主要将动作空间离散为预定义的动作，限制了无人机的机动性和复杂的策略实施。其他方法将问题简化为 1v1 战斗，忽略了多架无人机之间的合作动态。为了解决六自由度空间固有的高维挑战并改善合作，我们提出了一个利用 Leader-Follower 多智能体近端策略优化 (LFMAPPO) 策略的分层框架。具体而言，该框架分为三个层次。顶层对环境进行宏观评估并指导执行策略。中间层确定所需动作的角度。底层为高维动作空间生成精确的动作命令。此外，我们通过分配不同的角色和领导者-追随者策略来优化状态值函数，以训练顶层策略，追随者估计领导者的效用，促进代理之间的有效合作。此外，结合与无人机姿态相一致的目标选择器来评估目标的威胁等级。最后，模拟实验验证了我们提出的方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2501.13132</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用扩散生成模型进行图形表示学习</title>
      <link>https://arxiv.org/abs/2501.13133</link>
      <description><![CDATA[arXiv:2501.13133v1 公告类型：交叉 
摘要：扩散模型已经成为各种数据模式（包括图像和视频）中最先进的生成模型，因为它们能够准确地近似复杂的数据分布。与 VAE 和 GAN 等传统生成方法不同，扩散模型采用渐进式去噪过程，通过多个迭代步骤将噪声转换为有意义的数据。这种渐进式方法增强了它们的表现力和生成质量。不仅如此，扩散模型还被证明可以在学习生成样本的同时从数据中提取有意义的表示。尽管扩散模型取得了成功，但它对图结构数据的应用仍然相对未被探索，这主要是由于图的离散性质，这需要离散扩散过程，这与在其他领域使用的连续方法不同。在这项工作中，我们利用扩散模型的表示能力来学习图数据的有意义的嵌入。通过在自动编码器框架内训练离散扩散模型，我们能够实现有效的自动编码和针对图结构数据独特特征的表征学习。我们只需要最后的编码器来提取表征。我们的方法展示了离散扩散模型用于图表征学习的潜力。]]></description>
      <guid>https://arxiv.org/abs/2501.13133</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>人工智能和显微镜在生命科学研究中的应用与挑战：综述</title>
      <link>https://arxiv.org/abs/2501.13135</link>
      <description><![CDATA[arXiv:2501.13135v1 公告类型：交叉 
摘要：人类生物学及其错综复杂的系统具有巨大的潜力，可以促进人类健康、疾病治疗和科学发现。然而，研究生物相互作用的传统手动方法往往受到生物数据数量和复杂性的限制。人工智能 (AI) 具有分析大量数据集的成熟能力，为应对这些挑战提供了一种变革性方法。本文探讨了人工智能与显微镜在生命科学中的交集，强调了它们的潜在应用和相关挑战。我们详细回顾了各种生物系统如何从人工智能中受益，重点介绍了该领域独有的数据类型和标记要求。特别关注显微镜数据，探索处理和解释这些信息所需的特定人工智能技术。通过解决数据异质性和注释稀缺性等挑战，我们概述了该领域的潜在解决方案和新兴趋势。本文主要从人工智能的角度撰写，旨在为从事人工智能、显微镜和生物学交叉领域的研究人员提供宝贵的资源。它总结了当前的进展、关键见解和未解决的问题，促进了跨学科合作的理解。通过提供全面而简明的领域综合，本文旨在促进创新、促进跨学科参与并加速人工智能在生命科学研究中的应用。]]></description>
      <guid>https://arxiv.org/abs/2501.13135</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用哈希率特征预测比特币价格：小波和深度堆叠方法</title>
      <link>https://arxiv.org/abs/2501.13136</link>
      <description><![CDATA[arXiv:2501.13136v1 公告类型：交叉 
摘要：数字货币由于其非依赖性和分散性，在过去十年中变得流行起来。这些货币的价格有时会出现很大的波动，这增加了对预测的需求。作为最受欢迎的货币，比特币（BTC）已成为研究热点。数字货币（尤其是 BTC）的主要挑战和趋势是价格波动，这需要研究基本的价格预测模型。本研究提出了一种基于堆栈深度学习的分类和回归模型，该模型使用小波消除噪声来预测不同时间间隔内 BTC 的走势和价格。基于堆叠技术的所提模型使用基于深度学习的模型，尤其是神经网络和变压器，进行一、七、三十和九十天的预测。在预处理阶段，还将三种特征选择模型 Chi2、RFE 和嵌入式应用于数据。分类模型对第二天的预测准确率为 63%，对第七天、第三十天和第九十天的预测准确率分别为 64%、67% 和 82%。对于每日价格预测，百分比误差降低至 0.58，而七至九十天的误差范围为 2.72% 至 2.85%。这些结果表明，所提出的模型比文献中的其他模型表现更好。]]></description>
      <guid>https://arxiv.org/abs/2501.13136</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AirRadar：利用深度神经网络推断中国全国空气质量</title>
      <link>https://arxiv.org/abs/2501.13141</link>
      <description><![CDATA[arXiv:2501.13141v1 公告类型：交叉 
摘要：实时监测空气质量对于保障公众健康和促进社会进步至关重要。然而，空气质量监测站的广泛部署受到其高昂成本的限制。为了解决这一限制，我们引入了 \emph{AirRadar}，这是一个深度神经网络，旨在利用现有监测站的数据准确推断缺乏监测站的地区的实时空气质量。通过利用可学习的掩码标记，AirRadar 重建了未监测区域的空气质量特征。具体来说，它分两个阶段运行：首先捕获空间相关性，然后调整分布变化。我们使用来自中国 1,085 个监测站的一年期数据集验证了 AirRadar 的有效性，证明了它优于多个基线，即使有不同程度的未观察数据。源代码可以在 https://github.com/CityMind-Lab/AirRadar 上访问。]]></description>
      <guid>https://arxiv.org/abs/2501.13141</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>