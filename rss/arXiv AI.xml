<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.ai更新在arxiv.org上</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.ai在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Fri, 14 Mar 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>通过验证器在循环中进行自动定理的验证指导</title>
      <link>https://arxiv.org/abs/2503.09730</link>
      <description><![CDATA[ARXIV：2503.09730V1公告类型：新 
摘要：AI推理的最新方法最有前途的方法需要在模型中推出轨迹上应用强化学习的变体（RL），即使是逐步奖励或大量人类注释的轨迹数据。对推出轨迹的依赖使计算成本和时间过高。特别是，推理轨迹的正确性通常只能在完成时进行判断，从而导致RL的稀疏奖励或需要在类似专家迭代的方法中生成昂贵的合成数据。在这项工作中，我们专注于自动定理证明（ATP）任务，并提出了一种新颖的验证器在循环设计中，该设计与现有方法不同，该方法利用了对整个推理轨迹的反馈，它采用自动化验证程序来在推理过程的每个步骤中提供中等反馈。使用精益作为验证者，我们从经验上表明，逐步的本地验证可在模型的推理准确性和效率方面产生全球提高。]]></description>
      <guid>https://arxiv.org/abs/2503.09730</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Agentdam：自主网络代理的隐私泄漏评估</title>
      <link>https://arxiv.org/abs/2503.09780</link>
      <description><![CDATA[ARXIV：2503.09780V1公告类型：新 
摘要：LLM驱动的AI代理是一种新兴的边界，具有提高人类生产力的巨大潜力。但是，授权AI代理在日常任务中代表用户采取行动涉及使他们访问潜在敏感和私人信息，从而导致当代理发生故障时，可能会造成无意间隐私泄漏的风险。在这项工作中，我们提出了一种解决潜在风险的方法，通过训练AI代理更好地满足数据最小化的隐私原则。出于此基准测试的目的，通过“数据最小化”，我们的意思是，仅在必要时才能实现特定任务相关的目的时共享私人信息。我们开发了一个名为Agentdam的基准测试，以评估现有和未来的AI代理可以限制我们指定“必要”来完成任务的潜在私人信息的处理。我们的基准测试了现实的Web交互情况，并适用于所有现有的Web导航代理。我们使用AgentDam来评估AI代理在GPT-4，Llama-3和Claude顶部的构建状况如何，在不必要的情况下可以限制潜在的私人信息的处理，并表明这些代理通常容易无意地使用不必要的敏感信息。我们最终提出了一种基于促进的方法，以减少这种方法。]]></description>
      <guid>https://arxiv.org/abs/2503.09780</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>媒体和负责AI治理：游戏理论和LLM分析</title>
      <link>https://arxiv.org/abs/2503.09858</link>
      <description><![CDATA[ARXIV：2503.09858V1公告类型：新 
摘要：本文研究了AI开发人员，监管机构，用户和媒体之间的复杂相互作用，以促进值得信赖的AI系统。使用进化游戏理论和大型语言模型（LLM），我们在不同的监管制度下对这些参与者之间的战略互动进行了建模。该研究探讨了实现负责任治理的两个关键机制，安全的AI开发和安全AI的采用：通过媒体报告激励有效的法规，并根据评论的建议来调节用户信任。这些发现突出了媒体在向用户提供信息中的关键作用，这可能是通过调查开发人员或监管机构的一种“软”调节形式，作为替代机构AI法规的替代（在许多地区仍然不存在）。游戏理论分析和基于LLM的模拟都揭示了有效的监管和值得信赖的AI开发的条件，从而强调了从进化游戏理论的角度考虑不同监管制度的影响的重要性。该研究得出的结论是，有效的治理需要管理高质量评论的激励措施和成本。]]></description>
      <guid>https://arxiv.org/abs/2503.09858</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于几次班级学习的新基准：重新定义上限</title>
      <link>https://arxiv.org/abs/2503.10003</link>
      <description><![CDATA[ARXIV：2503.10003V1公告类型：新 
摘要：班级学习（CIL）的目的是不断适应新兴课程，同时保留对先前学到的课程的知识。几乎没有射门的课堂学习（FSCIL）提出了更大的挑战，这需要模型仅使用有限数量的样本学习增量类。在常规的CIL中，联合训练被广泛认为是上限，既是基准又是方法论指南。但是，我们发现，由于严重的阶级失衡引起的任务间分离（ICS）的固有难度，联合训练在FSCIL中没有有意义的上限。在这项工作中，我们通过整合不平衡感知的技术，有效地弥合了基础和增量类之间的性能差距，从而引入了针对FSCIL量身定制的新的联合培训基准。此外，我们指出了现有FSCIL方法的实验设置和评估中的不一致。为了确保不同的FSCIL方法与联合培训之间的公平比较，我们将培训条件标准化，并提出了同时考虑验证集和计算复杂性的统一评估方案。通过为FSCIL建立可靠的上限和标准化的评估框架，我们的工作为未来的研究提供了明确的基准和实用的基础。]]></description>
      <guid>https://arxiv.org/abs/2503.10003</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>OR-LLM-ENSENT：通过推理大语言模型的操作研究优化问题的自动建模和解决</title>
      <link>https://arxiv.org/abs/2503.10009</link>
      <description><![CDATA[ARXIV：2503.10009V1公告类型：新 
摘要：运营研究（OR）已广泛应用于各个领域，例如资源分配，生产计划和供应链管理。但是，解决现实世界或问题需要或专家执行数学建模和程序员来开发解决方案算法。这种传统的方法在很大程度上依赖专家，是昂贵的，并且具有长期的开发周期，严重限制了广泛采用或技术的采用。很少有人考虑使用人工智能（AI）代替专业人士来实现或问题的完全自动化解决方案。我们提出了OR-LLM-AGENT，这是第一个实现端到端自动化解决现实世界或问题的AI代理。 OR-LLM代理利用大语言模型（LLMS）的思维链（COT）推理能力将自然语言问题描述转化为正式的数学模型，并自动生成Gurobi求解器代码。在OR-LLM-AGENT中，或编码旨在在沙盒环境中自动执行和维修，从而促进了最终解决方案的推导。由于缺乏用于评估自动解决或问题的专用基准数据集，因此我们构建了一个包括83个现实世界或自然语言所描述的问题的基准数据集。我们使用最新的（SOTA）推理LLM进行比较实验，包括GPT-O3-Mini，DeepSeek-R1和Gemini 2.0 Flash Thinky。 OR-LLM代理达到了100％的最高通行率和85％的最高解决方案精度，这表明自动化或解决问题的可行性。数据和代码已在https://github.com/bwz96sco/or_llm_agent上公开获得。]]></description>
      <guid>https://arxiv.org/abs/2503.10009</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>高级工具学习和选择系统（Atlass）：使用LLM的闭环框架</title>
      <link>https://arxiv.org/abs/2503.10071</link>
      <description><![CDATA[ARXIV：2503.10071V1公告类型：新 
摘要：LLM代理与外部工具的结合使模型可以解决其知识库之外的复杂任务。人工设计的工具不灵活，仅限于专家创建的现有工具范围内的解决方案。为了解决这个问题，我们提出了Atlass，这是一种设计为闭环框架的高级工具学习和选择系统。它使LLM能够通过需求动态生成外部工具来解决问题。在此框架中，代理商在编排工具选择，执行和改进中发挥着至关重要的作用，从而确保了自适应解决问题的能力。 Atlass的操作遵循三个阶段：第一阶段，了解工具要求，涉及确定是否需要工具并指定其功能的代理；第二阶段是工具检索/生成，涉及代理根据其可用性检索或生成工具的代理；第三阶段（任务解决）涉及组合完成初始任务所需的所有组件工具。该工具数据集存储生成的工具，确保可重复使用和最大程度地降低推理成本。当前基于LLM的工具生成系统难以创建需要API或外部软件包的复杂工具。在Atlass中，我们通过自动设置环境，在线获取相关的API文档，并使用Python解释器来创建一个可靠的多功能工具，以在更广泛的情况下起作用，从而解决了问题。 OpenAI GPT-4.0被用作LLM代理，在执行生成的代码之前，通过人为反馈来处理安全和道德问题。通过解决预定义工具集的局限性并增强适应性，Atlass可以用作现实世界的解决方案，该解决方案使用户具有动态生成的工具以进行复杂的问题解决。]]></description>
      <guid>https://arxiv.org/abs/2503.10071</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>并行化多目标A*搜索</title>
      <link>https://arxiv.org/abs/2503.10075</link>
      <description><![CDATA[ARXIV：2503.10075V1公告类型：新 
摘要：多目标最短路径（MOSP）问题是一个经典的网络优化问题，旨在在图表中有多个边缘成本的两个点之间找到所有帕累托最佳路径。关于使用A*（MOA*）的多目标搜索的最新研究表明，在解决困难的MOSP实例方面表现出色。本文提出了一个新颖的搜索框架，该框架允许MOA*具有不同的客观顺序的有效并行化。该框架结合了独特的上边界策略，可帮助搜索在某些情况下将问题的维度降低到一个。实验结果表明，所提出的框架可以增强基于A*的解决方案的性能，并且与问题维度成正比。]]></description>
      <guid>https://arxiv.org/abs/2503.10075</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>语义协同作用：通过高级技能映射解锁政策见解和学习途径</title>
      <link>https://arxiv.org/abs/2503.10094</link>
      <description><![CDATA[ARXIV：2503.10094V1公告类型：新 
摘要：这项研究介绍了一个基于最先进的自然语言处理，语义嵌入和有效搜索技术的综合系统，以检索相似之处，从而从原始文本信息中产生可行的见解。该系统自动从多个文档（例如策略文件和课程Vitae）中提取并汇总了标准能力，并在公认的能力，职业概况和相关学习课程之间建立牢固的关系。为了验证其性能，我们进行了多层评估，其中包括合成和现实世界中文档中的明确和隐性技能参考。结果显示出近乎人类的准确性，显式技能检测的F1得分超过0.95，而隐式提及则高于0.93。因此，该系统为支持整个AE4RIA网络的深入协作建立了合理的基础。该方法涉及一条基于广泛的预处理和数据清洁，语义嵌入和分割的多阶段管道，并使用基于FAISS的搜索方法进行技能提取。提取的技能与职业框架（在ESCO本体论中提出）以及通过可持续发展目标学院提供的学习途径有关。此外，通过破折号和情节实施的交互式可视化软件，呈现图形和表格，用于实时探索，并由参与决策，培训和学习供应，职业过渡和招聘的人明智的决策。总体而言，通过严格验证的支持，该系统通过从原始，复杂的文本信息中提供结构化和可行的见解，为改善决策，人力资源发展和终身学习提供了有希望的前景。]]></description>
      <guid>https://arxiv.org/abs/2503.10094</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Stepmathagent：通过误差树评估数学过程的逐步代理</title>
      <link>https://arxiv.org/abs/2503.10105</link>
      <description><![CDATA[ARXIV：2503.10105V1公告类型：新 
摘要：评估数学能力对于评估大语言模型（LLMS）的整体表现至关重要。但是，现有的评估方法通常只关注最终答案，从而导致高度不准确和无法解释的评估结果，以及他们未能评估证据或开放式问题。为了解决这些问题，我们提出了一种基于Error树的新型数学过程评估代理，称为Stepmathagent。该代理结合了四个内部核心操作：逻辑步骤细分，步骤评分，得分聚集和错误树的产生，以及四个外部扩展模块：难度校准，简单性评估，完整性验证和格式评估。此外，我们引入了Stepmathbench，这是一个包括1,000个踩踏过程评估实例的基准测试，该基准源自200个由问题类型，主题类别和难度级别分组的高质量数学问题。在Stepmathbench上进行的实验表明，我们提出的Stepmathagent胜过所有最新方法，证明了人类一致的评估偏好以及对各种情况的广泛适用性。我们的数据和代码可在https://github.com/shu-xun/stepmathagent上找到。]]></description>
      <guid>https://arxiv.org/abs/2503.10105</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自适应偏好聚集</title>
      <link>https://arxiv.org/abs/2503.10215</link>
      <description><![CDATA[ARXIV：2503.10215V1公告类型：新 
摘要：AI对齐是确保AI系统按照人类价值行为的挑战，它已成为基础模型和推荐系统等系统开发的关键问题。尽管如此，目前的主要方法，人类反馈（RLHF）的强化学习还是在汇总了多样化人类偏好的理论局限性。社会选择理论提供了一个框架来汇总偏好，但不是针对AI典型的多维应用程序开发的。利用最近发布的URN过程中的见解，这项工作引入了一种偏好聚合策略，该策略适应用户的上下文，并继承了最大彩票的良好属性，这是一种condorcet concotencet consensiscetent的解决方案概念。]]></description>
      <guid>https://arxiv.org/abs/2503.10215</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM代理显示人类偏见，但表现出独特的学习模式</title>
      <link>https://arxiv.org/abs/2503.10248</link>
      <description><![CDATA[ARXIV：2503.10248V1公告类型：新 
摘要：我们从经验任务的决策中调查了大语言模型（LLM）的选择模式，涉及重复选择和从反馈中学习，并将其行为与人类参与者进行比较。我们发现，在总体上，LLM似乎显示出类似于人类的行为偏见：两者都表现出不足的罕见事件和相关效应。但是，对选择模式的更细微的分析表明，这是出于不同的原因而发生的。与人类不同，LLM表现出强烈的新近度偏见，他们似乎以更复杂的方式做出反应。尽管这些不同的过程平均可能导致相似的行为，但两组之间的选择模式各有近期事件的差异很大。具体而言，诸如``惊喜触发器的变化&#39;&#39;之类的现象和``罕见事件的波浪重新效果&#39;&#39;&#39;在人类中是有力的，但在LLM中完全不存在。我们的发现提供了对使用LLM在学习环境中模拟和预测人类的局限性的见解，并在研究是否复制人类决策趋势时强调了对其行为进行精致分析的必要性。]]></description>
      <guid>https://arxiv.org/abs/2503.10248</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Surgraw：具有手术智能的经过思考推理的多代理工作流程</title>
      <link>https://arxiv.org/abs/2503.10265</link>
      <description><![CDATA[ARXIV：2503.10265V1公告类型：新 
摘要：幻觉，领域知识差距以及对手术场景中任务相互依存的有限理解的阻碍，妨碍了手术智能中视觉模型（VLM）的整合，从而破坏了临床可靠性。尽管最近的VLMS表现出强大的一般推理和思维能力，但它们仍然缺乏精确的手术场景解释所需的领域专业知识和任务意识。尽管经过思考链（COT）可以更有效地构建推理，但当前的方法依赖于自我生成的COT步骤，这些步骤通常会加剧固有的域间隙和幻觉。为了克服这一点，我们提出了Surgraw，这是一个由COT驱动的多代理框架，可为机器人辅助手术中的大多数任务提供透明，可解释的见解。通过在五个任务中采用专门的COT提示：仪器识别，行动识别，行动预测，患者数据提取和结果评估，Surgraw通过结构化的，域内感知的推理来减轻幻觉。检索增强的生成（RAG）也已集成到外部医学知识中，以弥合域间隙并提高响应可靠性。最重要的是，分层的代理系统可确保COT所包裹的VLM代理有效地协作，同时了解任务相互依存关系，并通过小组讨论机制促进了逻辑一致性。为了评估我们的方法，我们介绍了Surgcotbench，这是第一个带有结构化框架级注释的基于推理的数据集。通过全面的实验，我们证明了拟议的Surgraw的有效性，对12个机器人程序的基线VLM的准确性提高了29.32％，实现了最先进的性能，并提高了可解释，可信赖的，可信赖的和自主的手术援助。]]></description>
      <guid>https://arxiv.org/abs/2503.10265</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>声明签名人：朝着声明过程模型符合检查的有效最佳一致性朝着迈进</title>
      <link>https://arxiv.org/abs/2503.10479</link>
      <description><![CDATA[ARXIV：2503.10479V1公告类型：新 
摘要：在许多工程应用中，必须精确遵循过程，从而使事件日志和声明过程模型之间的一致性检查对于确保遵守所需行为至关重要。这是一个关键领域，人工智能（AI）在推动有效过程改进方面起着关键作用。但是，由于这些模型固有的巨大搜索空间，计算最佳对齐构成了重大的计算挑战。因此，现有的方法通常会在可扩展性和效率上挣扎，从而限制了它们在现实世界中的适用性。本文介绍了一种新型算法，该算法使用A*搜索算法（已建立的AI探路技术）从新的角度利用声明模型的灵活性来解决问题。声明签名人的关键特征包括仅执行积极促进限制违规行为的动作，利用量身定制的启发式措施来导航到最佳解决方案，并采用早期修剪来消除非生产力的分支，同时还通过预处理和整合多个固定的动作来简化该过程。使用8,054个合成和现实生活对准问题评估了所提出的方法，这表明了其通过显着优于当前技术状态来有效计算最佳比对的能力。通过使过程分析师更有效地识别和理解一致性问题，Declarealigner有可能推动有意义的过程改进和管理。]]></description>
      <guid>https://arxiv.org/abs/2503.10479</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>攻城：通过树搜索自主多扭转大型语言模型的越狱</title>
      <link>https://arxiv.org/abs/2503.10619</link>
      <description><![CDATA[ARXIV：2503.10619V1公告类型：新 
摘要：我们介绍了Siege，这是一个多转化的对抗框架，该框架通过树搜索的角度对大语言模型（LLM）安全的逐渐侵蚀进行建模。与依靠一个精心设计的提示的单转弯越狱不同，攻城以广度优先的方式扩展了对话，分支了多个对抗性提示，从而利用部分依从性从先前的回应中剥夺。通过跟踪这些增量策略泄漏并将其重新注入随后的查询，围攻揭示了如何将小特许权积累到完全不允许的输出中。对越狱板数据集的评估表明，攻城在单个多转弯运行中的GPT-3.5涡轮增压率为100％，而GPT-4的成功率比Crescendo或goat等基线的较少。该树搜索方法论提供了一个深入的视图，即模型保护如何在连续的对话转弯中降低了降级，从而强调了语言模型的强大多转弯测试程序的紧迫性。]]></description>
      <guid>https://arxiv.org/abs/2503.10619</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>动作不确定性：体现药物的信心启发</title>
      <link>https://arxiv.org/abs/2503.10628</link>
      <description><![CDATA[ARXIV：2503.10628V1公告类型：新 
摘要：表达置信度对于导航动态多模式环境的具体体现的代理人来说是一项挑战，在这种环境中，不确定性均来自感知和决策过程。我们介绍了在开放式多模式环境中调查体现置信度启发的第一批工作。我们介绍了启发策略，该政策构建了跨感应，演绎和绑架推理的置信度评估以及执行策略，从而通过场景重新解释，动作抽样和假设推理来增强信心校准。评估Minecraft环境中校准和故障预测任务的代理，我们表明结构化推理方法（例如经过思考链）可以改善置信度校准。但是，我们的发现还揭示了区分不确定性的持续挑战，尤其是在绑架环境下，强调了对更复杂的体现置信度启发方法的需求。]]></description>
      <guid>https://arxiv.org/abs/2503.10628</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有新型指标的统一框架，用于评估XAI技术在LLMS中的有效性</title>
      <link>https://arxiv.org/abs/2503.05050</link>
      <description><![CDATA[ARXIV：2503.05050V1公告类型：交叉 
摘要：LLM的日益复杂性对其透明度和解释性提出了重大挑战，因此需要使用可解释的AI（XAI）技术来增强可信度和可用性。这项研究介绍了一个全面的评估框架，该框架具有四个新型指标，用于评估五个LLM和两个下游任务中五种XAI技术的有效性。我们使用此框架来评估几种XAI技术石灰，摇摆，集成梯度，层面相关性传播（LRP）以及注意机制可视化（AMV），并使用IMDB电影评论和推文情感提取数据集进行了视觉机制可视化（AMV）。评估的重点是四个关键指标：人类策划协议（HA），鲁棒性，一致性和对比度。我们的结果表明，石灰在多个LLM和评估指标之间始终达到高分，而AMV表现出了较高的鲁棒性和近乎完美的一致性。 LRP在对比度上擅长，尤其是在更复杂的模型中。我们的发现为不同XAI方法的优势和局限性提供了宝贵的见解，为开发和选择适用于LLM的XAI技术提供了指导。]]></description>
      <guid>https://arxiv.org/abs/2503.05050</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>授权未来的劳动力：优先考虑AI-Accelerated就业市场的教育</title>
      <link>https://arxiv.org/abs/2503.09613</link>
      <description><![CDATA[ARXIV：2503.09613V1公告类型：交叉 
摘要：AI在工作场所的快速整合需要新的方法来跨学科的劳动力教育和培训以及更广泛的AI素养。政府，工业和教育机构的协调行动是必要的，以确保工人可以适应加速技术变革。]]></description>
      <guid>https://arxiv.org/abs/2503.09613</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用编辑的大型语言模型作为一般科学优化器</title>
      <link>https://arxiv.org/abs/2503.09620</link>
      <description><![CDATA[ARXIV：2503.09620V1公告类型：交叉 
摘要：大型语言模型（LLM）在科学方案的数学优化中已被广泛采用，以获得广泛的知识和高级推理能力。现有方法主要集中于利用LLM以迅速的方式解决优化问题，该方法将观察反馈作为其他文本描述。但是，由于LLM的\ textbf {对提示的高度敏感性}和\ textbf {倾向在冗长的提示中迷失了}，因此这些方法难以有效利用每个优化阶段的{观察性}反馈，从而严重阻碍了现实情况下的应用程序。为了应对这些挑战，我们提出了一种概念上简单而通用的{BI级}优化方法，即\ textbf {g} eneral \ textbf {s} cientific \ cientific \ textbf {o} ptimizers（gso）。具体而言，GSO首先利用内部级别的模拟器作为实验平台来评估当前解决方案并提供观察性反馈。然后，LLM是知识渊博和多才多艺的科学家，通过从反馈中提出潜在的错误作为外部级别优化来生成新的解决方案。最后，通过模型编辑共同更新了模拟LLM中的专家知识。广泛的实验表明，GSO始终使用\ textIt {六}不同的LLM骨架在\ textit {七个}不同的任务上胜过现有的最新方法，证明了有效性和广泛的应用程序。]]></description>
      <guid>https://arxiv.org/abs/2503.09620</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>当然是机器人？可信赖的社交机器人通过强大的多模式神经过程检测</title>
      <link>https://arxiv.org/abs/2503.09626</link>
      <description><![CDATA[arxiv：2503.09626v1公告类型：交叉 
摘要：社会机器人检测对于缓解错误信息，在线操纵和协调的不真实行为至关重要。尽管现有的基于神经网络的检测器在基准测试方面表现良好，但由于跨数据集的分配变化，它们在概括方面遇到了困难，并且经常为超出培训数据以外的分布帐户提供过度自信的预测。为了解决这个问题，我们介绍了一个新颖的社会机器人检测（UESBD）框架的新型不确定性估计，该框架量化了超出分类以外的检测器的预测不确定性。对于此任务，我们提出了强大的多模式神经过程（RMNP），该过程旨在增强多模式神经过程的鲁棒性，从而使社交机器人迷彩引起的模态不一致。 RMNP首先通过特定于模式的编码来学习单峰表示。然后，使用单峰的专注神经过程来编码单峰潜在变量的高斯分布。此外，为了避免社交机器人窃取人类特征以伪装自己，从而导致某些方式提供冲突性信息，我们引入了一个证据的门控网络，以明确模拟模式的可靠性。联合潜在分布是通过专家的广义产品来学习的，专家的广义产品将每种模式的可靠性都考虑到融合过程中。最终的预测是通过蒙特卡洛对关节潜伏分布的采样，然后是解码器。在三个现实世界基准上进行的实验表明，RMNP在分类和不确定性估计中的有效性及其对模态冲突的鲁棒性。]]></description>
      <guid>https://arxiv.org/abs/2503.09626</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FPG：大规模高斯裂口的馈送语义意识到的影子风格的转移</title>
      <link>https://arxiv.org/abs/2503.09635</link>
      <description><![CDATA[ARXIV：2503.09635V1公告类型：交叉 
摘要：我们提出了FPGS，这是一种由高斯分裂代表的大规模辐射场的馈送光真逼真的风格转移方法。 FPGS，用任意，多个样式参考图像的大规模3D场景进行风格化，而无需其他优化，同时保留了3D高斯人的多视图一致性和实时渲染速度。先前的艺术需要每种风格的优化或耗时的每场训练阶段，并且仅限于小规模的3D场景。 FPG通过引入样式构成的3D功能字段来有效地对大规模3D场景进行样式化，该场景继承了Adain的Feed-Forward-Ford-Forward-Ford-Ford-Ford-Fornward Stylization Machinery，并支持了任意样式参考图像。此外，FPGs通过语义通信匹配和本地ADAIN支持多引用样式化，从而为3D场景样式增加了不同的用户控制。 FPG还通过将语义匹配和样式传输过程直接应用于3D空间中的查询功能，从而保持多视图的一致性。在实验中，我们证明了FPGS可实现具有不同参考图像的大规模静态和动态3D场景的有利的影像质量场景风格。项目页面：https：//kim-geonu.github.io/fpgs/]]></description>
      <guid>https://arxiv.org/abs/2503.09635</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>