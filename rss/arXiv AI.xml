<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.ai更新在arxiv.org上</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.ai在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Mon, 03 Mar 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>超越透明度：计算可靠性作为算法的外部主义认识论</title>
      <link>https://arxiv.org/abs/2502.20402</link>
      <description><![CDATA[ARXIV：2502.20402V1公告类型：新 
摘要：本章对算法的认识论感兴趣。当我打算处理这个主题时，这是关于认知理由的问题。当前的理由方法强调了算法的透明度，这需要阐明其内部机制（例如函数和变量），并证明（或）如何产生输出。因此，通过透明的理由方式取决于有关该算法的内容，从这个意义上讲，算法是内部的。相比之下，我主张我称计算可靠性（CR）的外部主义认识论。虽然我以前曾在计算机模拟领域介绍并检查了CR（[42，53，4]），但本章扩展了这种可靠的认识论，以涵盖各种科学纪律中使用的更广泛的算法，并特别强调了机器学习应用程序。 CR认为，如果算法的输出是由可靠的算法产生的，则可以证明其输出是合理的。可靠的算法是使用可靠性指标指定，编码，使用和维护的算法。这些可靠性指标源于形式方法，算法指标，专家能力，研究文化和其他科学努力。本章的主要目的是描述CR的基础，阐明其操作机制，并概述其作为算法外部主义认识论的潜力。]]></description>
      <guid>https://arxiv.org/abs/2502.20402</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型战略推理通过行为游戏理论评估</title>
      <link>https://arxiv.org/abs/2502.20432</link>
      <description><![CDATA[ARXIV：2502.20432V1公告类型：新 
摘要：战略决策涉及互动推理，在该推理中，代理会适应其选择对他人的选择，但现有对大语言模型（LLMS）的评估通常强调NASH平衡（NE）近似，从而忽略了推动其战略选择的机制。为了弥合这一差距，我们引入了一个以行为游戏理论为基础的评估框架，将推理能力与上下文效果相关。在测试22个最先进的LLMS时，我们发现GPT-O3-Mini，GPT-O1和DeepSeek-R1主导了大多数游戏，但也表明仅模型量表并不能确定性能。在提示增强的方面，经过思考链（COT）提示并不普遍有效，因为它仅针对某些级别的模型增加了战略性推理，同时在其他地方提供有限的收益。此外，我们研究了编码的人口特征对模型的影响，观察到某些任务会影响决策模式。例如，与男性相比，与男性相比，GPT-4O与男性的战略推理相比，与男性相比，与男性相比，与其他性取向相比，Gemma将推理水平更高，表明固有的偏见。这些发现强调了对道德标准和上下文一致性的必要性，以平衡改善推理与公平性。]]></description>
      <guid>https://arxiv.org/abs/2502.20432</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>R-Parvi：通过奖励镜头的基于粒子的变异推断</title>
      <link>https://arxiv.org/abs/2502.20482</link>
      <description><![CDATA[ARXIV：2502.20482V1公告类型：新 
摘要：提出了一种奖励引导，无梯度的PARVI方法，\ textit {r-parvi}，用于对部分已知的密度进行采样（例如，达到常数）。 r-parvi将采样问题提出了作为奖励驱动的粒子流动：粒子是从先前的分布中绘制的，通过参数空间导航，并通过奖励机制融合了目标密度的奖励机制来确定的运动，而稳态粒子构型近似于目标几何形状。粒子环境相互作用是通过随机扰动和奖励机制模拟的，奖励机制将颗粒驱动到高密度区域，同时保持多样性（例如，防止崩溃成簇）。 R-Parvi为一类概率模型（例如在贝叶斯推断和生成性建模中遇到的概率模型）提供快速，灵活，可扩展和随机抽样以及推断。]]></description>
      <guid>https://arxiv.org/abs/2502.20482</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>关于基准在机器中的人类智能</title>
      <link>https://arxiv.org/abs/2502.20502</link>
      <description><![CDATA[ARXIV：2502.20502V1公告类型：新 
摘要：最近的基准研究声称，AI在各种认知任务上已经接近甚至超过了人类水平的表现。但是，该立场论文认为当前的AI评估范例不足以评估类似人类的认知能力。我们确定了一系列关键的缺点：缺乏人类验证的标签，人类反应变异性和不确定性的表示不足以及依赖简化和生态感染的任务。我们通过对十个现有的AI基准进行人体评估研究来支持我们的主张，这表明任务和标签设计中存在很大的偏见和缺陷。为了解决这些局限性，我们提出了五项具体建议，以开发未来的基准，这些建议将在AI中对人类样的认知能力进行更严格和有意义的评估，并对此类AI应用产生各种影响。]]></description>
      <guid>https://arxiv.org/abs/2502.20502</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Nutrigen：个性化的进餐计划生成器，利用大型语言模型来增强饮食和营养依从性</title>
      <link>https://arxiv.org/abs/2502.20601</link>
      <description><![CDATA[ARXIV：2502.20601V1公告类型：新 
摘要：保持均衡饮食对于整体健康至关重要，但是由于营养复杂性，时间限制和缺乏饮食知识，许多人在饮食计划中挣扎。个性化食品建议可以通过根据个人喜好，习惯和饮食限制来量身定制进餐计划来帮助应对这些挑战。但是，现有的饮食推荐系统通常缺乏适应性，无法考虑诸如食品成分可用性之类的现实限制，并且需要广泛的用户输入，这使得它们对于可持续和可扩展的日常使用不切实际。为了解决这些限制，我们介绍了Nutrigen，这是一个基于大语言模型（LLM）的框架，旨在生成个性化的膳食计划，以与用户定义的饮食偏好和约束相符。通过构建个性化的营养数据库和利用及时的工程，我们的方法使LLMS能够合并可靠的营养参考，例如USDA营养数据库，同时保持灵活性和易用性。我们证明，LLM在产生准确且用户友好的食物建议方面具有强大的潜力，通过提供结构化，实用和可扩展的餐食计划来解决现有饮食推荐系统中的关键局限性。我们的评估表明，Llama 3.1 8b和GPT-3.5 Turbo的百分比误差分别为1.55 \％和3.68％，制定了与用户定义的热量偏差，同时最小化偏差并提高精度。此外，我们将DeepSeek V3的性能与几种既定模型进行了比较，以评估其在个性化营养计划中的潜力。]]></description>
      <guid>https://arxiv.org/abs/2502.20601</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Personabench：通过访问（合成）私人用户数据评估有关了解个人信息的AI模型</title>
      <link>https://arxiv.org/abs/2502.20616</link>
      <description><![CDATA[ARXIV：2502.20616V1公告类型：新 
摘要：个性化对AI助手至关重要，尤其是在与单个用户合作的私人AI模型的背景下。该域中的关键方案涉及使AI模型访问和解释用户的私人数据（例如对话历史记录，用户交互，应用程序使用），以了解个人详细信息，例如传记信息，偏好和社交联系。但是，由于此类数据的敏感性，没有公开可用的数据集可以使我们能够通过直接访问个人信息来评估AI模型了解用户的能力。
  为了解决这一差距，我们引入了一条合成数据生成管道，该管道创建了模拟人类活动的多样化，现实的用户资料和私人文档。利用此综合数据，我们提出了Personabench，这是一种基准测试，旨在评估AI模型在理解模拟私人用户数据中得出的个人信息时的性能。
  我们使用与用户的个人信息直接相关的问题评估了检索功能的生成（RAG）管道，并由提供给模型的相关私人文档支持。我们的结果表明，当前的检索授权AI模型难以通过从用户文档中提取个人信息来回答私人问题，从而强调了需要改进方法来增强AI中个性化功能的需求。]]></description>
      <guid>https://arxiv.org/abs/2502.20616</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自动数据库描述文本到SQL的生成</title>
      <link>https://arxiv.org/abs/2502.20657</link>
      <description><![CDATA[ARXIV：2502.20657V1公告类型：新 
摘要：在文本到SQL任务的上下文中，表和列描述对于弥合自然语言和数据库架构之间的差距至关重要。本报告提出了一种方法，用于自动生成有效数据库描述时，当显式描述不可用时。所提出的方法采用双处理方法：一种粗到五个过程，然后进行精细到核的过程。粗到精细的方法利用LLM的固有知识来指导从数据库到表，最后到列的理解过程。这种方法提供了对数据库结构的整体理解，并确保上下文对齐。相反，精细的方法开始于列级，在回到表级别时提供了更准确和细微的理解。鸟类基准的实验结果表明，使用拟议的描述与不使用描述相比，SQL的生成准确性提高了0.93 \％，并且达到了37％的人类水平的性能。源代码可在https://github.com/xgenerationlab/xiyan-dbdescgen上公开获得。]]></description>
      <guid>https://arxiv.org/abs/2502.20657</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Proai：主动的多机构对话AI，具有精神诊断的结构化知识库</title>
      <link>https://arxiv.org/abs/2502.20689</link>
      <description><![CDATA[ARXIV：2502.20689V1公告类型：新 
摘要：大多数LLM驱动的对话AI系统都可以发挥作用，响应用户提示而无需指导交互。大多数LLM驱动的对话AI系统都可以反应地运行，响应用户提示而无需指导交互。但是，许多现实世界中的应用程序，例如精神病诊断，咨询和访谈 - 重新攻击AI，以主动发挥作用，提出正确的问题，并将对话转向特定目标。使用心理健康差异诊断作为应用程序环境，我们介绍了一个面向目标的，主动的对话AI框架Proai。 Proai集成了结构化知识引导的记忆，多代理主动推理和多方面的评估策略，从而使LLMS能够参与临床医生式的诊断推理，而不是简单的响应产生。通过模拟的患者互动，用户经验评估和专业的临床验证，我们证明了ProAI在精神障碍鉴别诊断中的精度最高为83.3％，同时保持专业和移情互动标准。这些结果突出了更可靠，自适应和目标驱动的AI诊断助手的潜力，使LLM超越了反应性对话系统。]]></description>
      <guid>https://arxiv.org/abs/2502.20689</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>为什么对AI的信任可能是不可避免的</title>
      <link>https://arxiv.org/abs/2502.20701</link>
      <description><![CDATA[ARXIV：2502.20701V1公告类型：新 
摘要：在人类的互动中，解释被广泛认为是对AI系统的信任所必需的。我们认为，信任可能是先决条件，因为有时解释是不可能的。我们从将解释作为通过知识网络作为搜索过程的形式化而得出的结果，在有限的时间内，解释者必须在共享概念和要解释的概念之间找到路径。我们的模型表明，即使在理论上的理想条件下，解释也可能失败 - 当演员是理性的，诚实的，有动力的，可以完美交流并拥有重叠的知识时。这是因为成功的解释不仅需要共享知识的存在，还需要在时间限制内找到连接路径，因此，在发现共享知识之前停止尝试解释是合理的。该结果对人类的互动具有重要意义：随着AI系统，尤其是大型语言模型，变得更加复杂，能够产生表面上引人注目但虚假的解释，人类可能会默认信任而不是需要真正的解释。这会构成错位信任和不完美知识集成的风险。]]></description>
      <guid>https://arxiv.org/abs/2502.20701</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>模糊的投机解码，以进行可调的准确度折衷方案</title>
      <link>https://arxiv.org/abs/2502.20704</link>
      <description><![CDATA[ARXIV：2502.20704V1公告类型：新 
摘要：投机解码（SD）在许多情况下限制了严格的分布等效性与目标模型的分布等效性，将潜在的速度UPS限制为在许多情况下达到可比的结果。此外，执行分配等效性意味着用户无法交易与目标模型分布的偏差以获得进一步的推理速度提高。为了解决这些限制，我们引入了模糊的投机解码（FSD） - 一种解码算法，该算法通过纯粹基于目标模型分布之间的差异来纯粹接受候选令牌来概括SD。通过允许与目标模型受控的差异，FSD使用户可以灵活地以推理速度进行贸易发电质量。在几个基准测试中，我们的方法能够以比SD的速度达到超过5个令牌的显着改善，仅在基准准确性的绝对降低范围内仅大约2％的绝对降低。在许多情况下，FSD甚至能够以超过2个令牌的速度匹配SD基准的准确性，这表明分布等效性对于维持目标模型性能是不需要的。]]></description>
      <guid>https://arxiv.org/abs/2502.20704</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深度解决：通过基于树的探索和双点思维来增强复杂的工程解决方案设计</title>
      <link>https://arxiv.org/abs/2502.20730</link>
      <description><![CDATA[ARXIV：2502.20730V1公告类型：新 
摘要：设计针对复杂工程挑战的解决方案对于人类生产活动至关重要。但是，先前在检索增强生成（RAG）领域的研究还没有足够解决与复杂工程解决方案设计有关的任务。为了填补这一空白，我们引入了一个新的基准测试台，以评估系统为具有多个复杂约束的工程问题生成完整且可行的解决方案的能力。为了进一步推进复杂工程解决方案的设计，我们提出了一个新型系统，即解决基于树的探索和双点思维机制来生成可靠的解决方案。广泛的实验结果表明，Solutionrag在解决方案板上实现了最新的（SOTA）性能，突出了其在现实世界应用中增强复杂工程解决方案设计的自动化和可靠性的潜力。]]></description>
      <guid>https://arxiv.org/abs/2502.20730</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用位于互动说明的单词获得基础表示</title>
      <link>https://arxiv.org/abs/2502.20754</link>
      <description><![CDATA[ARXIV：2502.20754V1公告类型：新 
摘要：我们提出了一种从混合定位，与人类指导员进行的相互作用中获取基础表示单词的方法的方法。这项工作着重于获取各种类型的知识，包括知觉，语义和程序知识以及学习含义。交互式学习使代理商可以通过要求有关未知概念的说明来控制其学习，从而使学习有效。我们的方法已在SOAR中实例化，并已在能够操纵小物体的台式机器人臂上进行了评估。]]></description>
      <guid>https://arxiv.org/abs/2502.20754</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>阻尼B-Pinn：基于阻尼特征的贝叶斯物理信息，用于车辆状态估计</title>
      <link>https://arxiv.org/abs/2502.20772</link>
      <description><![CDATA[ARXIV：2502.20772V1公告类型：新 
摘要：具有噪声的多输入多输出（MIMO）系统的状态估计，例如车辆底盘系统，由于输入和输出之间的不完善和复杂的关系，提出了重大挑战。为了解决这个问题，我们设计了一个基于阻尼器特征的贝叶斯物理知识神经网络（Damper-B-Pinn）。首先，我们引入了受阻尼器的机械性能启发的神经元前进过程，该过程限制了时期之间的神经元值的突然跳跃，同时保持搜索能力。此外，我们将优化的贝叶斯辍学层应用于MIMO系统，以增强抵抗噪声的鲁棒性并防止非连接问题。物理信息被纳入损失函数，以作为神经网络的物理先验。然后，与其他最先进的基准相比，在十个数据集和14种车辆类型中，在十个数据集和14种车辆类型中验证了我们的阻尼B-PINN结构的有效性。]]></description>
      <guid>https://arxiv.org/abs/2502.20772</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Medhalltune：用于减轻视觉模型中医学幻觉的指导调节基准</title>
      <link>https://arxiv.org/abs/2502.20780</link>
      <description><![CDATA[ARXIV：2502.20780V1公告类型：新 
摘要：在医疗保健应用中，视觉模型（VLM）的使用越来越多地提出了与幻觉相关的巨大挑战，在这些挑战中，模型可能会产生看似合理的结果，实际上是不正确的。这种幻觉会危害临床决策，可能损害诊断和治疗。在这项工作中，我们提出了Medhalltune，这是一种专门用于评估和减轻医疗VLMS幻觉的大规模基准。 Medhalltune包括超过100,000张图像和1,000,000个指令对，包括幻觉和非隔离样本，每个样本都带有地面注释。我们使用MedHalltune对当前的医学和一般VLM进行了全面评估，评估了它们在关键指标中的绩效，包括临床准确性，相关性，详细信息水平和风险水平。实验结果表明，通过MedHalltune进行微调成功提高了几种现有模型管理幻觉的能力，并提高了在下游视觉问题避开（VQA）任务上的零发性能，从而使它们更可靠地用于实际医疗应用。我们的工作有助于发展更值得信赖的VLM。代码和数据集将在\ href {https://github.com/russellyq/medhalltune} {medhalltune}上获得。]]></description>
      <guid>https://arxiv.org/abs/2502.20780</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MV-MATH：在多视觉上下文中评估多模式数学推理</title>
      <link>https://arxiv.org/abs/2502.20808</link>
      <description><![CDATA[ARXIV：2502.20808V1公告类型：新 
摘要：多模式的大语言模型（MLLM）在各个数据集的视觉上下文中显示了数学推理的有希望的功能。但是，大多数现有的多模式数学基准限于单视性上下文，这些上下文与现实世界中数学应用程序中常见的多视觉场景有所不同。为了解决这一差距，我们介绍了MV-MATH：一个精心策划的数据集的2,009个高质量数学问题。每个问题都集成了多个图像与文本交织在一起，这些图像来自真实的K-12场景，并具有详细的注释。 MV-MATH包括多项选择，自由形式和多步骤问题，涵盖了3个难度级别的11个主题领域，并作为评估MLLMS在多维式环境中的数学推理的全面和严格的基准。通过广泛的实验，我们观察到，MLLM在多视觉数学任务中遇到了重大挑战，并且相对于MV-MATH上的人类能力的性能差距很大。此外，我们分析了各种模型的性能和误差模式，从而提供了对MLLMS在多视觉设置中的数学推理能力的见解。]]></description>
      <guid>https://arxiv.org/abs/2502.20808</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一项试点实证研究，涉及何时以及如何使用知识图作为检索增强产生</title>
      <link>https://arxiv.org/abs/2502.20854</link>
      <description><![CDATA[ARXIV：2502.20854V1公告类型：新 
摘要：将知识图（KGS）的整合到检索增强生成（RAG）框架中引起了重大兴趣，早期的研究表明，有望减轻幻觉和提高模型的准确性。然而，仍然缺乏对快速新兴kg rag方法的系统理解和比较分析。本文试图通过在与不同技术配置相关的各种应用程序方案中分析其性能来系统地回答何时以及如何使用KG-rag的问题。在使用KG-rag框架概述了思维图并总结了其流行的管道之后，我们对KG-rag进行了试点实证研究，以在不同的情况下在7个数据集中重新实现和评估6 kg-rag方法，分析了9 kg-rag配置与17 llms结合使用的9 kg-rag配置的影响。我们的结果强调了适当的应用条件和kg-rag组件的最佳配置的关键作用。]]></description>
      <guid>https://arxiv.org/abs/2502.20854</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>将临床知识合并为医学研究和应用的大型语言模型：一项调查</title>
      <link>https://arxiv.org/abs/2502.20988</link>
      <description><![CDATA[ARXIV：2502.20988V1公告类型：新 
摘要：临床知识是从有关原因，预后，诊断和疾病治疗的研究中汲取的信息的收集。这种知识可以改善治愈性能并促进身体健康。随着大型语言模型（LLM）的出现，旨在将学术医学AI系统应用于现实世界中医学场景的医学人工智能（医学AI）已进入了新的发展时代，从而从学术和工业研究中提供了出色的作品，例如Mothergpt和Pangud-Drug。但是，该领域缺乏对学术界和行业建立医疗AI系统的全面汇编和比较。因此，这项调查重点是医疗AI系统的建筑范例，包括使用临床数据库，数据集，培训管道，整合医学知识图，系统应用程序和评估系统。我们希望这项调查可以帮助相关实践研究人员了解医疗保健各个领域的学术模型的当前表现，以及实施这些科学成就的潜在问题和未来的方向。]]></description>
      <guid>https://arxiv.org/abs/2502.20988</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基础模型是否有用用于脑电图分析的功能提取器？</title>
      <link>https://arxiv.org/abs/2502.21086</link>
      <description><![CDATA[ARXIV：2502.21086V1公告类型：新 
摘要：基础模型在自然语言处理和计算机视觉中的成功促进了一般时间序列分析的类似方法。尽管这些模型对各种任务有效，但它们在数据有限的医疗领域中的适用性仍然在很大程度上没有探索。为了解决这个问题，我们研究了基础模型在涉及脑电图（EEG）的医学时间序列分析中的有效性。通过对年龄预测，癫痫发作检测和临床相关脑电图事件的分类等任务的广泛实验，我们将其诊断准确性与专门的脑电图模型进行了比较。我们的分析表明，基础模型提取有意义的脑电图功能，即使没有域名适应的特殊模型，也可以提取特定于任务的生物标志物。此外，我们证明诊断准确性在很大程度上受到诸如上下文长度之类的建筑选择的影响。总体而言，我们的研究表明，具有一般时间序列理解的基础模型消除了对大型领域特定数据集的依赖性，使其成为临床实践的宝贵工具。]]></description>
      <guid>https://arxiv.org/abs/2502.21086</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一项基于LLM的Delphi研究，以预测Genai Evolution</title>
      <link>https://arxiv.org/abs/2502.21092</link>
      <description><![CDATA[ARXIV：2502.21092V1公告类型：新 
摘要：预测复杂和快速发展的系统的未来轨迹仍然是一个重大挑战，尤其是在数据稀缺或不可靠的领域。这项研究通过利用大型语言模型进行Delphi研究，介绍了一种新颖的定性预测方法。该方法用于探索生成人工智能的未来演变，揭示了对诸如地缘政治紧张局势，经济差异，监管框架和道德考虑因素等关键因素的见解。结果突出了基于LLM的Delphi研究如何促进结构化的场景分析，在减轻受访者疲劳等问题的同时捕获不同的观点。但是，局限性在知识截止，固有的偏见和对初始条件的敏感性方面出现。尽管该方法为结构化的远见提供了一种创新的手段，但该方法也可以视为一种新颖的推理形式。需要进一步的研究来完善其管理异质性，提高可靠性和整合外部数据源的能力。]]></description>
      <guid>https://arxiv.org/abs/2502.21092</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>重新评估大语模型中心理评估理论</title>
      <link>https://arxiv.org/abs/2502.21098</link>
      <description><![CDATA[ARXIV：2502.21098V1公告类型：新 
摘要：大型语言模型（LLMS）是否具有思想理论（TOM）的问题（通常定义为推理他人心理状态的能力）引发了巨大的科学和公共利益。但是，关于LLMS是否拥有TOM的证据，评估最近的增长并未导致收敛。在这里，我们汲取了认知科学的灵感来重新评估LLMS中TOM评估的状态。我们认为，关于LLMS是否具有TOM的分歧的主要原因是缺乏应与模型相匹配的人类行为或这些行为的计算的明确性。我们还强调了当前评估可能偏离TOM能力的“纯”测量的方法，这也导致了混乱。最后，我们讨论了未来研究的几个方向，包括汤姆和务实的交流之间的关系，这可以提高我们对人造系统以及人类认知的理解。]]></description>
      <guid>https://arxiv.org/abs/2502.21098</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>