<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.AI 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.AI 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Mon, 03 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>通过符号压缩提高大型语言模型效率：一种实现可解释性的形式化方法</title>
      <link>https://arxiv.org/abs/2501.18657</link>
      <description><![CDATA[arXiv:2501.18657v1 公告类型：新
摘要：大型语言模型（LLM）在代码生成和逻辑推理任务中面临严重的标记效率瓶颈，这一挑战直接影响推理成本和模型可解释性。本文提出了一种基于符号压缩的形式化框架，集成组合逻辑、信息论最优编码和上下文感知推理技术，在保持语义完整性的同时实现标记效率的阶跃提升。我们在函数式编程范式中建立了一个数学框架，推导出符号密度和模型可解释性之间的定量关系，并提出了一个可微分压缩因子度量来评估编码效率。此外，我们利用参数高效微调（PEFT）技术实现了 GAEL 语言的低成本应用。实验结果表明，该方法在代码生成任务中实现了 78.3% 的标记压缩率，同时通过结构显式性将逻辑可追溯性提高了 62%。该研究为法学硕士（LLM）中的高效推理提供了新的理论工具，为模型可解释性研究开辟了一条符号路径。]]></description>
      <guid>https://arxiv.org/abs/2501.18657</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>模拟流：一种控制大型语言模型和使用生成式人工智能构建复杂系统的编程范式</title>
      <link>https://arxiv.org/abs/2501.18668</link>
      <description><![CDATA[arXiv:2501.18668v1 公告类型：新
摘要：我们引入了模拟流，这是一种编程范例，旨在有效控制和利用大型语言模型 (LLM) 进行复杂、动态的模拟和代理工作流。我们的主要目标是创建一个干扰最小的框架，利用 LLM 的代理能力，同时解决其在保持一致性、选择性忽略/包含信息和执行严格的世界规则方面的局限性。模拟流通过基于状态的方法实现这一点，其中变量由“操作员”按顺序步骤修改，以重复格式产生输出并遵守状态变量的一致规则。这种方法将 LLM 集中在定义的任务上，同时旨在使上下文流保持“分布”。该方法结合了实体组件系统 (ECS) 架构，以更直观的方式编写程序，促进跨不同组件和实体重用工作流。这种 ECS 方法增强了输出流的模块化，允许进行复杂的多实体模拟，同时保持格式一致性、信息控制和规则执行。它由自定义编辑器支持，可帮助创建、运行和分析模拟。我们通过正在进行的市场经济模拟、三个角色在公园玩接球游戏的社交模拟和一套经典强化学习基准任务的说明性示例展示了模拟流的多功能性。这些示例展示了模拟流在数百到数千次迭代中处理复杂、不断变化的场景的能力，促进了不同代理工作流程和模型之间的比较，并保持了 LLM 驱动模拟的一致性和持续有趣的发展。]]></description>
      <guid>https://arxiv.org/abs/2501.18668</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM 生成的 AI 规划启发式方法：我们还需要领域独立性吗？</title>
      <link>https://arxiv.org/abs/2501.18784</link>
      <description><![CDATA[arXiv:2501.18784v1 公告类型：新
摘要：领域独立启发式方法长期以来一直是 AI 规划的基石，它提供了适用于广泛任务的通用解决方案，而无需特定领域的工程。然而，大型语言模型 (LLM) 的出现为生成针对特定规划问题的启发式方法提供了机会，这可能会挑战领域独立性作为严格设计原则的必要性。在本文中，我们探讨了使用 LLM 从以后继生成器和用通用编程语言编写的目标测试表示的任务描述中自动得出规划启发式方法。我们研究了领域特定 LLM 生成的启发式方法与传统领域独立方法在计算效率和可解释性方面的权衡。我们的实验表明，LLM 可以创建在某些标准 IPC 域上实现最先进性能的启发式方法，以及它们解决缺乏足够规划域定义语言 ({\sc pddl}) 表示的问题的能力。我们讨论这些结果是否意味着范式转变以及它们如何补充现有方法。]]></description>
      <guid>https://arxiv.org/abs/2501.18784</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>弥合推理差距：小型法学硕士课程可以使用通用策略进行规划</title>
      <link>https://arxiv.org/abs/2501.18817</link>
      <description><![CDATA[arXiv:2501.18817v1 公告类型：新
摘要：大型语言模型 (LLM) 推理能力的最新进展表明 LLM 解决简单规划任务的能力有所提高。但是，只要改进推理能力的驱动力是模型的大小和复杂性，运行它们相关的财务和计算成本也会增加。这种趋势引发了关于持续可访问性的问题，以及这些改进是否会随着模型的规模和成本不断增长而以相同的速度增长。我们提出了两种方法来增强资源密集程度较低的 LLM 的推理能力。（1）为他们提供由资源密集程度更高的 LLM 生成的解决给定领域内任务的通用策略。（2）通过迭代提示这些模型纠正其提出的解决方案中的错误来利用它们的成本效益。我们从规划和数学推理任务中获得的实证结果表明，这些方法将资源密集程度较低的 LLM 的性能提升到与资源密集程度较高的 LLM 相当的水平，而成本却只是后者的一小部分。此外，我们还表明，在我们的实验中使用通用策略平均将资源密集程度较低的模型的成本降低了近 30%。]]></description>
      <guid>https://arxiv.org/abs/2501.18817</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>语言游戏是通向超人工智能的途径</title>
      <link>https://arxiv.org/abs/2501.18924</link>
      <description><![CDATA[arXiv:2501.18924v1 公告类型：新 
摘要：大型语言模型 (LLM) 向人工智能 (ASI) 的演变取决于数据复制，这是一个循环过程，在此过程中，模型生成、整理和重新训练新数据以改进能力。然而，当前的方法可能会陷入数据复制陷阱：在闭环中优化固定的人为分布中的输出会导致停滞，因为模型仅仅重新组合现有知识而不是探索新领域。在本文中，我们提出语言游戏作为扩大数据复制的途径，通过三种机制打破这一循环：(1) \textit{角色流动性}，通过使多智能体系统能够跨任务动态转换角色来增强数据多样性和覆盖率；(2) \textit{奖励多样性}，嵌入可以驱动复杂智能行为的多种反馈标准； （3）\textit{规则可塑性}，迭代地发展交互约束以促进可学习性，从而注入持续的新颖性。通过将语言游戏扩展到全球社会技术生态系统，人机共同进化产生了无限的数据流，推动了开放式探索。该框架将数据复制重新定义为超人智能的引擎，而不是闭环。]]></description>
      <guid>https://arxiv.org/abs/2501.18924</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PathE：利用实体无关路径实现参数高效的知识图谱嵌入</title>
      <link>https://arxiv.org/abs/2501.19095</link>
      <description><![CDATA[arXiv:2501.19095v1 公告类型：新
摘要：知识图谱 (KG) 以实体（节点）和关系的形式存储人类知识，并广泛应用于各种应用中。KG 嵌入是解决知识发现、链接预测和推理等任务的有效方法。这通常是通过为所有或部分实体分配和学习嵌入表来完成的。由于这会随着实体数量线性扩展，因此在具有数百万个节点的现实世界 KG 中学习嵌入模型在计算上是难以解决的。为了解决这个可扩展性问题，我们的模型 PathE 仅为关系分配嵌入表（通常比实体少几个数量级），并且所需的参数不到以前参数高效方法的 25%。我们不是存储实体嵌入，而是学习通过利用多个实体关系路径来计算它们，以在三元组中对单个实体进行情境化。经过四项基准测试，PathE 在关系预测方面取得了最佳表现，并且在消费级硬件上训练时，在路径丰富的知识图谱的链接预测方面仍具有竞争力。我们进行了消融实验来测试我们的设计选择，并分析了模型对关键超参数的敏感性。PathE 对于实际应用中常见的关系多样且连接良好的知识图谱而言，既高效又经济。]]></description>
      <guid>https://arxiv.org/abs/2501.19095</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>欧洲人工智能法案中的逻辑模式：分析</title>
      <link>https://arxiv.org/abs/2501.19112</link>
      <description><![CDATA[arXiv:2501.19112v1 公告类型：新
摘要：本文从逻辑模式的角度全面分析了《欧洲人工智能法案》，旨在准备其形式化表示，例如在逻辑多元知识工程框架和方法 (LogiKEy) 中。LogiKEy 基于形式化方法开发规范推理的计算工具，采用高阶逻辑 (HOL) 作为统一的元逻辑，通过浅层语义嵌入整合不同的逻辑。Isabelle/HOL 是一种配备多个自动定理证明器的证明辅助工具，它促进了这种整合。讨论了《人工智能法案》中的模式及其适合表示的逻辑。对于这些逻辑的选择，创建了 HOL 中的嵌入，然后用于编码示例段落。初步实验评估了这些嵌入对自动推理的适用性，并强调了实现更强大的推理能力的关键挑战。]]></description>
      <guid>https://arxiv.org/abs/2501.19112</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>对抗性幻灭的模仿游戏与多模态生成思维链角色扮演</title>
      <link>https://arxiv.org/abs/2501.19143</link>
      <description><![CDATA[arXiv:2501.19143v1 公告类型：新
摘要：作为人工智能的基石，机器感知面临着对抗性幻觉带来的根本威胁。这些对抗性攻击表现为两种主要形式：演绎幻觉，其中特定刺激是基于受害者模型的一般决策逻辑制作的；归纳幻觉，其中受害者模型的一般决策逻辑由特定刺激塑造。前者利用模型的决策边界来创建刺激，当应用时，会干扰其决策过程。后者强化了模型中的条件反射，在其学习阶段嵌入后门，当受到刺激触发时，会导致异常行为。对抗性幻觉的多面性要求统一的防御框架，以解决各种攻击形式的漏洞。在这项研究中，我们提出了一种基于模仿游戏概念的幻灭范式。模仿游戏的核心是一个多模态生成代理，由思维链推理控制，它可以观察、内化和重建样本的语义本质，摆脱将样本恢复到原始状态的传统追求。作为概念验证，我们使用多模态生成对话代理进行实验模拟，并在各种攻击场景下评估该方法。]]></description>
      <guid>https://arxiv.org/abs/2501.19143</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自主网络防御代理的实证博弈论分析</title>
      <link>https://arxiv.org/abs/2501.19206</link>
      <description><![CDATA[arXiv:2501.19206v1 公告类型：新
摘要：最近日益复杂的网络攻击不断增加，对强大且有弹性的自主网络防御 (ACD) 代理的需求也随之增加。鉴于所采用的网络攻击策略、技术和程序 (TTP) 种类繁多，需要能够返回可通用策略的学习方法。同时，ACD 代理的保证仍然是一个悬而未决的挑战。我们通过使用原则性双预言机 (DO) 算法对 ACD 的深度强化学习 (DRL) 方法进行实证博弈论分析来解决这两个挑战。该算法依赖于对手迭代学习（近似）针对彼此策略的最佳响应；对于自主网络操作代理来说，这是一项计算成本高昂的工作。在这项工作中，我们介绍并评估了一种理论上合理的、基于潜力的奖励塑造方法来加快这一过程。此外，鉴于开源 ACD-DRL 方法的数量不断增加，我们扩展了 DO 公式以允许多个响应预言 (MRO)，从而为 ACD 方法的整体评估提供了一个框架。]]></description>
      <guid>https://arxiv.org/abs/2501.19206</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SHARPIE：强化学习和人机交互实验的模块化框架</title>
      <link>https://arxiv.org/abs/2501.19245</link>
      <description><![CDATA[arXiv:2501.19245v1 公告类型：新
摘要：强化学习 (RL) 为建模和训练 AI 代理提供了一种通用方法，包括人机交互场景。在本文中，我们提出了 SHARPIE（用于交互式实验的共享人机强化学习平台），以满足对支持 RL 代理和人类实验的通用框架的需求。它的模块化设计包括一个用于 RL 环境和算法库的多功能包装器、一个面向参与者的 Web 界面、日志实用程序、在流行云和参与者招募平台上的部署。它使研究人员能够研究与人类和 RL 代理之间的交互相关的各种研究问题，包括与交互式奖励规范和学习、从人类反馈中学习、行动委托、偏好引出、用户建模和人机合作相关的问题。该平台基于人机交互的通用界面，旨在标准化人类环境中的 RL 研究领域。]]></description>
      <guid>https://arxiv.org/abs/2501.19245</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>可解释强化学习中人类主体评价的客观指标</title>
      <link>https://arxiv.org/abs/2501.19256</link>
      <description><![CDATA[arXiv:2501.19256v1 公告类型：新
摘要：解释从根本上来说是一个人类过程。了解解释的目标和受众至关重要，但现有的可解释强化学习 (XRL) 研究通常不会在评估中咨询人类。即使他们这样做了，他们也会经常诉诸主观指标，例如信心或理解，这些指标只能让研究人员了解用户的意见，而不能了解他们对特定问题的实际效果。本文呼吁研究人员使用客观的人类指标进行基于可观察和可操作行为的解释评估，以建立更具可重复性、可比较性和认识论基础的研究。为此，我们策划、描述和比较了几种客观评估方法，用于将解释应用于调试代理行为并支持人机协作，并使用新颖的基于网格的环境说明我们提出的方法。我们讨论了主观和客观指标如何相互补充以提供整体验证，以及未来的工作如何需要利用标准化基准进行测试，以便在研究之间进行更大的比较。]]></description>
      <guid>https://arxiv.org/abs/2501.19256</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大奖！对齐作为最大彩票</title>
      <link>https://arxiv.org/abs/2501.19266</link>
      <description><![CDATA[arXiv:2501.19266v1 公告类型：新
摘要：强化学习从人类反馈 (RLHF) 是将大型语言模型 (LLM) 与人类价值观对齐的标准，众所周知，它无法满足直观可取的属性，例如尊重大多数人的偏好 \cite{ge2024axioms}。为了克服这些问题，我们建议使用称为 \emph{最大彩票} 的概率社会选择规则来替代 RLHF。我们表明，一组对齐技术，即纳什学习从人类反馈 (NLHF) \cite{munos2023nash} 及其变体，可以近似最大彩票结果，从而继承其有益属性。
我们通过实验证实，我们提出的方法比标准 RLHF 更能稳健地处理处理偏好时出现的情况，包括支持大多数人的偏好、提供处理偏好数据中非传递性的原则性方法以及对不相关替代方案的稳健性。这使得系统能够更好地融入人类价值观并尊重人类意图。]]></description>
      <guid>https://arxiv.org/abs/2501.19266</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于概念的可解释人工智能：指标和基准</title>
      <link>https://arxiv.org/abs/2501.19271</link>
      <description><![CDATA[arXiv:2501.19271v1 公告类型：新 
摘要：基于概念的解释方法，例如概念瓶颈模型（CBM），旨在通过将机器学习模型的决策与人类可理解的概念联系起来，提高机器学习模型的可解释性，关键假设是这些概念可以准确地归因于网络的特征空间。然而，这一基本假设尚未得到严格验证，主要是因为该领域缺乏标准化的指标和基准来评估此类概念的存在和空间对齐。为了解决这个问题，我们提出了三个指标：概念全局重要性指标、概念存在性指标和概念位置指标，包括一种可视化概念激活的技术，即概念激活映射。我们对事后 CBM 进行基准测试，以说明它们的能力和挑战。通过定性和定量实验，我们证明，在许多情况下，即使是事后 CBM 确定的最重要的概念也不会出现在输入图像中；此外，当它们存在时，它们的显着性图无法与预期区域对齐，因为要么激活整个对象，要么错误识别相关的概念特定区域。我们分析了这些限制的根本原因，例如概念的自然相关性。我们的研究结果强调需要更谨慎地应用基于概念的解释技术，特别是在空间可解释性至关重要的环境中。]]></description>
      <guid>https://arxiv.org/abs/2501.19271</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用大型语言模型为智能家居生成合成用户行为序列</title>
      <link>https://arxiv.org/abs/2501.19298</link>
      <description><![CDATA[arXiv:2501.19298v1 公告类型：新
摘要：近年来，随着智能家居系统的普及，这些环境中的安全问题已成为日益严重的威胁。目前，大多数智能家居安全解决方案（例如异常检测和行为预测模型）都是使用预先收集的固定数据集进行训练的。然而，数据集收集过程耗时长，缺乏适应不断发展的智能家居环境所需的灵活性。此外，个人数据的收集引发了用户的重大隐私问题。最近，大型语言模型 (LLM) 凭借其在自然语言处理、推理和解决问题方面的强大能力，已成为跨不同应用领域广泛任务的强大工具。在本文中，我们提出了一种基于 LLM 的合成数据集生成 IoTGen 框架，以增强下游智能家居智能模型的泛化。通过生成反映环境变化的新合成数据集，智能家居智能模型可以重新训练以克服固定和过时数据的限制，使其更好地与现实家居环境的动态特性保持一致。具体来说，我们首先提出了一种针对物联网行为数据量身定制的结构模式感知压缩 (SPPC) 方法，该方法在显著减少 token 消耗的同时保留了数据中最有用的内容。然后，我们提出了一种系统的方法创建提示并实现数据生成，以自动生成具有规范和合理属性的物联网合成数据，辅助任务模型进行自适应训练以提高泛化能力和真实世界性能。]]></description>
      <guid>https://arxiv.org/abs/2501.19298</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SETS：利用自我验证和自我纠正来缩短测试时间</title>
      <link>https://arxiv.org/abs/2501.19306</link>
      <description><![CDATA[arXiv:2501.19306v1 公告类型：新
摘要：大型语言模型 (LLM) 的最新进展为通过利用测试时间计算来提高复杂推理任务的性能创造了新的机会。然而，传统方法（例如重复抽样和多数投票或奖励模型评分）除了需要昂贵的特定任务奖励模型训练外，还经常随着测试时间计算的扩展而面临收益递减的问题。在本文中，我们提出了自我增强测试时间扩展 (SETS)，这是一种利用最近先进的 LLM 的自我验证和自我校正功能来克服这些限制的新方法。SETS 将采样、自我验证和自我校正集成到一个统一的框架中，从而实现高效且可扩展的测试时间计算，从而提高复杂任务的能力。通过对具有挑战性的规划和推理基准进行大量实验，与其他方法相比，我们证明 SETS 实现了显着的性能改进和更有利的测试时间扩展规律。]]></description>
      <guid>https://arxiv.org/abs/2501.19306</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>主动生活事件服务的本体分析</title>
      <link>https://arxiv.org/abs/2501.19308</link>
      <description><![CDATA[arXiv:2501.19308v1 公告类型：新
摘要：生活事件服务是由多个政府机构联合提供的直接数字公共服务，以便个人能够履行因个人生活中的特定事件或情况而产生的所有义务并使用所有权利。生活事件服务将与同一生活事件相关的多项公共服务合并为一项服务，供服务消费者使用。本文基于 Guarino、Guizzardi、Nardi、Wagner 等人的作品，对生活事件服务进行了本体论分析。本体论分析的目的是理解生活事件、基于生活事件的主动公共服务以及其他相关概念的含义。这种本体论分析至关重要，因为对于实施电子政务和数字公共服务的硬件和软件架构，必须就底层术语的确切含义达成一致。]]></description>
      <guid>https://arxiv.org/abs/2501.19308</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MINDSTORES：基于记忆的神经决策合成，用于具身系统中面向任务的强化</title>
      <link>https://arxiv.org/abs/2501.19318</link>
      <description><![CDATA[arXiv:2501.19318v1 公告类型：新
摘要：虽然大型语言模型 (LLM) 已显示出作为具身代理的零样本规划器的良好能力，但它们无法从经验中学习并构建持久的心理模型，这限制了它们在像 Minecraft 这样的复杂开放世界环境中的稳健性。我们引入了 MINDSTORES，这是一个体验增强的规划框架，使具身代理能够通过与环境的自然交互来构建和利用心理模型。从人类构建和改进认知心理模型的方式中汲取灵感，我们的方法通过维护过去经验的数据库来扩展现有的零样本 LLM 规划，该数据​​库可为未来的规划迭代提供信息。关键的创新是将累积的经验表示为（状态、任务、计划、结果）三元组的自然语言嵌入，然后 LLM 规划器可以有效地检索和推理这些三元组，以产生见解并指导新状态和任务的计划细化。通过在 MineDojo 环境（为 Minecraft 中的代理提供低级控制的模拟环境）中进行大量实验，我们发现 MINDSTORES 学习和应用知识的能力明显优于现有的基于内存的 LLM 规划器，同时保持了零样本方法的灵活性和泛化优势，这代表着朝着能够通过自然经验不断学习的更强大的具身 AI 系统迈出了重要一步。]]></description>
      <guid>https://arxiv.org/abs/2501.19318</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士是否会策略性地披露、隐藏和推断信息？《变色龙游戏》中的理论与实证分析</title>
      <link>https://arxiv.org/abs/2501.19398</link>
      <description><![CDATA[arXiv:2501.19398v1 公告类型：新
摘要：基于大型语言模型 (LLM) 的代理在包含非合作方的环境中变得很常见。在这样的环境中，代理的决策需要向对手隐瞒信息，向合作者透露信息，并推断信息以识别其他代理的特征。为了调查 LLM 是否具有这些信息控制和决策能力，我们让 LLM 代理玩基于语言的隐藏身份游戏变色龙。在游戏中，一群彼此不认识的非变色龙代理旨在在不透露秘密的情况下识别变色龙代理。游戏需要变色龙和非变色龙都具备上述信息控制能力。实证结果表明，虽然非变色龙 LLM 代理可以识别变色龙，但它们无法向变色龙隐瞒秘密，而且它们的获胜概率远远低于甚至平凡策略的水平。为了正式解释这种行为，我们对从隐藏到揭示的一系列策略进行了理论分析，并为非变色龙的获胜概率提供了界限。基于不同策略的实证结果和理论分析，我们推断基于 LLM 的非变色龙代理会向身份不明的代理透露过多信息。我们的研究结果指出了当代 LLM（包括 GPT-4、GPT-4o、Gemini 1.5 和 Claude 3.5 Sonnet）在战略互动方面的弱点。]]></description>
      <guid>https://arxiv.org/abs/2501.19398</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用神经双级优先级进行更快的配置性能错误测试</title>
      <link>https://arxiv.org/abs/2501.15392</link>
      <description><![CDATA[arXiv:2501.15392v1 公告类型：交叉 
摘要：随着软件系统变得越来越复杂和可配置，配置设计往往会出现更多的性能问题。这导致一些配置选项意外地降低了性能，偏离了开发人员最初设计的预期。这种差异，即配置性能错误（CPBug），是毁灭性的，可能深深隐藏在源代码中。然而，有效地测试 CPBug 是困难的，不仅是因为测试预言很难设置，还因为配置测量成本高昂，而且有太多可能的配置需要测试。因此，现有的测试工具运行时间过长，或者在预算有限的情况下无法有效检测 CPBug，再加上不准确的测试预言。在本文中，我们试图通过在配置选项和值范围级别上对测试进行神经优先级排序，并使用自动预言估计来实现更快的 CPBug 测试。我们提出的工具称为 NDP，是一个可与不同的启发式生成器配合使用的通用框架。其理念是利用两个神经语言模型：一个用于估计作为预言的 CPBug 类型，而更重要的是，另一个用于推断选项与 CPBug 相关的概率，并根据此概率对要搜索的选项和值范围进行优先排序。在几个不同版本的广泛使用的系统上进行的实验表明，NDP 通常可以在 87% 的情况下更好地预测 CPBug 类型，并找到更多的 CPBug，与最先进的工具相比，测试效率提高了 88.88 倍。]]></description>
      <guid>https://arxiv.org/abs/2501.15392</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深度优化 IC3 算法的 SAT 求解器</title>
      <link>https://arxiv.org/abs/2501.18612</link>
      <description><![CDATA[arXiv:2501.18612v1 公告类型：交叉 
摘要：IC3 算法，也称为 PDR，是一种基于 SAT 的模型检查算法，由于其效率、可扩展性和完整性，近年来对该领域产生了重大影响。它利用 SAT 求解器来解决一系列与相对归纳相关的 SAT 查询。基于我们对 IC3 中 SAT 查询独特特征的观察，本文介绍了 GipSAT，这是一种专门针对 IC3 优化的轻量级 SAT 求解器。通过观察到 SAT 查询不一定需要对所有变量进行决策，GipSAT 会在每次求解之前计算需要决策的变量子集，同时确保结果不受影响。通过观察到 VSIDS 中二叉堆操作的开销不可忽略，GipSAT 利用存储桶而不是二叉堆来实现恒定时间操作。 GipSAT 支持临时子句，无需在每次求解前分配新的激活变量，因此无需重置求解器。综合评估表明 GipSAT 的性能显著提升。与 Minisat 相比，GipSAT 求解时间平均加快了 3.61 倍。]]></description>
      <guid>https://arxiv.org/abs/2501.18612</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>