<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.ai更新在arxiv.org上</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.ai在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Wed, 19 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>评估回形针最大化器：基于RL的语言模型是否更有可能追求工具目标？</title>
      <link>https://arxiv.org/abs/2502.12206</link>
      <description><![CDATA[ARXIV：2502.12206V1公告类型：新 
摘要：随着大型语言模型（LLM）的不断发展，确保他们与人类目标和价值观保持一致仍然是一个紧迫的挑战。一个关键问题是\ textIt {工具融合}，其中AI系统在优化给定的目标时，开发出意想不到的中间目标，覆盖了最终目标并偏离人类意义的目标。这个问题在强化学习（RL）培训的模型中尤其重要，该模型可以产生创造性但意外的策略以最大程度地提高奖励。在本文中，我们通过将用直接RL优化（例如O1模型）训练的模型与从人类反馈（RLHF）进行强化学习的模型进行比较，探讨了LLMS中的仪器融合。我们假设RL驱动的模型由于对目标指导行为的优化方式表现出更强的工具收敛趋势，其方式可能与人类意图失调。为了评估这一点，我们引入了仪器雷瓦尔，这是评估RL训练LLM中仪器收敛的基准。最初的实验揭示了一个案例，即要赚钱的模型出乎意料地追求工具目标，例如自我复制，暗示了乐器融合的迹象。我们的发现有助于更深入地了解AI系统中的一致性挑战以及意外模型行为带来的风险。]]></description>
      <guid>https://arxiv.org/abs/2502.12206</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过跨层门中的MOE推断的精确专家预测</title>
      <link>https://arxiv.org/abs/2502.12224</link>
      <description><![CDATA[ARXIV：2502.12224V1公告类型：新 
摘要：大型语言模型（LLM）在各种任务中都表现出了令人印象深刻的表现，并且它们在边缘方案中的应用引起了极大的关注。然而，非常适合边缘方案的稀疏激活混合物（MOE）模型由于其高内存需求而受到相对较少的关注。已经提出了基于卸载的方法来应对这一挑战，但他们面临着专家预测的困难。不准确的专家预测可能会导致推理延迟。为了促进MOE模型在边缘方案中的应用，我们提出了命运，这是一种旨在MOE模型的卸载系统，以便在资源约束环境中有效推断。命运背后的关键见解是，来自相邻层的门输入可以有效地用于专家预取，可实现高预测准确性，而无需其他GPU开销。此外，命运采用了一种肤色浅的专家缓存策略，将专家的命中率提高到99％。此外，命运还集成了量身定量的量化策略，以进行缓存优化和IO效率。实验结果表明，与按需负载和基于专家激活路径的方法相比，命运在预填充速度中最多可达到4.5倍和1.9倍的速度，并分别以解码速度达到4.1倍和2.2倍加速，同时保持推理质量。此外，在不同的内存预算中，命运的绩效改进是可扩展的。]]></description>
      <guid>https://arxiv.org/abs/2502.12224</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过LLMS将专家知识纳入逻辑计划</title>
      <link>https://arxiv.org/abs/2502.12275</link>
      <description><![CDATA[ARXIV：2502.12275V1公告类型：新 
摘要：本文介绍了Exklop，这是一个新颖的框架，旨在评估大型语言模型（LLMS）如何将专家知识整合到逻辑推理系统中。这种能力在工程中尤其有价值，在工程学中，专家知识（例如制造商推荐的操作范围）可以直接嵌入自动监测系统中。通过镜像专家验证步骤，范围检查和约束验证等任务有助于确保系统的安全性和可靠性。我们的方法系统地评估了LLM生成的逻辑规则，评估了这些关键验证任务中的句法流利性和逻辑正确性。我们还根据代码执行成果通过迭代反馈循环探索自我纠正的模型能力。 Exklop介绍了一个可扩展的数据集，其中包括130个工程场所，950个提示和相应的验证点。它可以实现全面的基准测试，同时可以控制实验的任务复杂性和可扩展性。我们利用合成数据创建方法来对包括Llama3，Gemma，Mixtral，Mistral和Qwen在内的各种LLM进行广泛的经验评估进行广泛的经验评估。结果表明，尽管模型产生了几乎完美的句法正确的代码，但它们经常在翻译专家知识时表现出逻辑错误。此外，迭代自我纠正仅产生边际改善（最高3％）。总体而言，Exklop是一个强大的评估平台，可以简化自我校正系统的有效模型的选择，同时清楚地描述遇到的错误类型。完整的实现以及所有相关数据可在GitHub上获得。]]></description>
      <guid>https://arxiv.org/abs/2502.12275</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一项关于自动计划大型语言模型的调查</title>
      <link>https://arxiv.org/abs/2502.12435</link>
      <description><![CDATA[ARXIV：2502.12435V1公告类型：新 
摘要：大型语言模型（LLMS）的计划能力近年来引起了人们的关注，因为它们具有出色的多步推理能力及其在广泛领域中概括的能力。尽管一些研究人员强调了LLMS执行复杂的计划任务的潜力，但其他研究人员则强调了其性能的重大限制，尤其是当这些模型的任务是处理长途推理的复杂性时。在这项调查中，我们对使用LLM在自动化计划中使用的现有研究进行了严格的研究，并详细研究了其成功和缺点。我们说明，尽管LLM并不适合作为独立计划者，但由于这些局限性，但它们还是有很大的机会，可以与其他方法结合使用。因此，我们提倡一种平衡的方法，该方法利用了LLM的固有灵活性和广义知识，以及传统计划方法的严格和成本效益。]]></description>
      <guid>https://arxiv.org/abs/2502.12435</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>生成AI的计算安全：A信号处理视角</title>
      <link>https://arxiv.org/abs/2502.12445</link>
      <description><![CDATA[Arxiv：2502.12445V1公告类型：新 
摘要：AI安全是一个快速增长的研究领域，旨在防止Frontier AI技术的伤害和滥用，尤其是在生成AI（Genai）工具方面，能够通过文本提示来创建现实和高质量的内容。此类工具的示例包括大型语言模型（LLM）和文本对图像（T2I）扩散模型。由于各种领先的Genai模型的性能由于类似的培训数据源和神经网络架构设计而接近饱和，可靠的安全护栏的开发已成为责任和可持续性的关键区别。本文介绍了计算安全概念的形式化，该概念是一个数学框架，可以通过信号处理理论和方法的镜头对Genai的安全挑战进行定量评估，制定和研究。特别是，我们探讨了Genai中计算安全挑战的两个示例类别，这些类别可以作为假设测试问题提出。为了确保模型输入的安全性，我们显示了如何使用敏感性分析和损失景观分析来检测恶意提示通过越狱尝试。为了确保模型输出的安全性，我们阐明了如何使用统计信号处理和对抗性学习来检测AI生成的内容。最后，我们讨论了关键的开放研究挑战，机会以及信号处理在计算AI安全中的基本作用。]]></description>
      <guid>https://arxiv.org/abs/2502.12445</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过大型语言模型的代理商调查和扩展霍曼斯的社会交流理论</title>
      <link>https://arxiv.org/abs/2502.12450</link>
      <description><![CDATA[ARXIV：2502.12450V1公告类型：新 
摘要：霍曼斯的社会交流理论（集合）被广泛认为是理解人类文明和社会结构的形成和出现的基本框架。在社会科学中，通常根据简单的仿真实验或现实世界的人类研究对该理论进行研究，这些实验都缺乏现实主义或太昂贵而无法控制。在人工智能中，大语言模型（LLM）的最新进展表现出有希望的模拟人类行为的能力。受这些见解的启发，我们采用了跨学科研究的观点，并建议使用基于LLM的代理来研究Homans的集合。具体来说，我们构建了一个由三个LLM代理组成的虚拟社会，并让他们从事社会交流游戏以观察他们的行为。通过广泛的实验，我们发现霍曼斯的集合在我们的代理社会中得到了很好的验证，证明了代理人和人类行为之间的一致性。在这个基础的基础上，我们故意改变了代理商协会的设置，以扩展传统的霍曼斯的场景，从而更加全面和详细。据我们所知，本文标志着研究Homans与基于LLM的代理商进行的第一步。更重要的是，它引入了一种新颖且可行的研究范式，该范式通过基于LLM的代理来桥接社会科学和计算机科学领域。代码可在https://github.com/paitesanshi/set上找到。]]></description>
      <guid>https://arxiv.org/abs/2502.12450</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>提升，删除和自定义：强大的System2-system1代码生成管道</title>
      <link>https://arxiv.org/abs/2502.12492</link>
      <description><![CDATA[ARXIV：2502.12492V1公告类型：新 
摘要：大型语言模型（LLMS）在各个领域，尤其是在系统1任务中表现出了显着的功能，但是在系统2任务中解决问题的机制的复杂性尚未得到充分探索。对System2到System1方法方法的最新研究激增，通过推理时间计算探索系统2推理知识，并将探索知识压缩到System 1过程中。在本文中，我们专注于代码生成，这是代表性系统2任务，并确定了两个主要挑战：（1）复杂的隐藏推理过程和（2）异质数据分布，使强大的LLM Solvers探索和培训变得复杂。为了解决这些问题，我们提出了一个新颖的BDC框架，该框架探讨了有见地的系统2使用MC-Tree Agent-agent算法，具有相互的\ textbf {b} osting，\ textbf {d} isentangles iSentAngle lora-experts，并为每个数据实例获得\ textbf {c} ustomized问题求解器具有输入感知的超级net工作，可以在Lora-Experts上进行重量，从而提供有效性，灵活性和鲁棒性。该框架通过相互验证和增强来利用多个LLM，并集成到蒙特卡洛树搜索过程中，通过基于反射的修剪和改进来增强。此外，我们介绍了disenlora算法，该算法将异质数据簇定为可综合的LORA专家，从而通过输入感知的Hypernetwork实现了适应性的定制问题解决者。这项工作为在复杂的推理任务中推进LLM功能奠定了基础，并提供了一种新颖的System2-System1解决方案。]]></description>
      <guid>https://arxiv.org/abs/2502.12492</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM推理和计划的推理时间计算：基准和见解</title>
      <link>https://arxiv.org/abs/2502.12521</link>
      <description><![CDATA[ARXIV：2502.12521V1公告类型：新 
摘要：我们研究了大语言模型（LLM）在解决复杂任务中的推理和计划功能。推理时间技术的最新进展表明，通过探索推理过程中的中间步骤，在没有额外培训的情况下增强LLM推理的潜力。值得注意的是，OpenAI的O1模型通过新颖的多步推理和验证来显示出令人鼓舞的性能。在这里，我们探讨了扩展推理时间技术如何改善推理和计划，重点是理解计算成本和性能之间的权衡。为此，我们构建了一个全面的基准，称为SYS2Bench，并进行了广泛的实验，对跨五个类别的11个不同任务进行评估，包括算术推理，逻辑推理，常识推理，算法推理，算法推理和计划。我们的发现表明，简单地扩展推理时间计算具有局限性，因为在所有推理和计划任务中，没有一个推理时间技术始终如一地表现良好。]]></description>
      <guid>https://arxiv.org/abs/2502.12521</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Cityeqa：一个层次的LLM代理商在体现的问题上回答城市空间中的基准测试</title>
      <link>https://arxiv.org/abs/2502.12532</link>
      <description><![CDATA[Arxiv：2502.12532V1公告类型：新 
摘要：体现的问答（EQA）主要集中在室内环境上，留下了城市环境的复杂性 - 跨越环境，行动和感知 - 在很大程度上没有探索。为了弥合这一差距，我们介绍了Cityeqa，这是一项新任务，具体的代理商通过在动态城市空间中的积极探索来回答开放式摄影问题。为了支持这项任务，我们介绍了Cityeqa-EC，这是第一个基准的基准数据集，该数据集跨越了六个类别的1,412个人类注销的任务，这是基于现实的3D Urban Simulator。此外，我们提出了为Cityeqa量身定制的新型特工计划者 - 经理演员（PMA）。 PMA启用了长马计划和层次任务执行：计划者将回答的问题分解为子任务，经理在过程控制过程中维护以对象为中心的认知映射，用于空间推理，专业演员处理导航，勘探和探索，探索和集合子任务。实验表明，PMA达到了60.7％的人级答案准确性，明显优于基于边界的基层。在有前途的同时，与人类相比，性能差距强调了Cityeqa的视觉推理的需求。这项工作为城市空间情报的未来进步铺平了道路。数据集和代码可在https://github.com/biluyong/cityeqa.git上找到。]]></description>
      <guid>https://arxiv.org/abs/2502.12532</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>探索人格特质对LLM偏见和毒性的影响</title>
      <link>https://arxiv.org/abs/2502.12566</link>
      <description><![CDATA[ARXIV：2502.12566V1公告类型：新 
摘要：随着AI在人类生活中的不同角色，将大型语言模型（LLMS）具有不同的个性吸引了研究兴趣。尽管“拟人化”增强了人类的交互性和LLM的适应性经验，但它引起了人们对内容安全的关键关注，尤其是在LLM生成的偏见，情感和毒性方面。这项研究探讨了将不同的人格特征分配给LLM的如何影响其产出的毒性和偏见。利用在社会心理学中开发的广泛接受的己科人格框架时，我们设计了实验性的声音促使在三个有毒和偏见基准上测试三个LLM的表现。这些发现证明了所有三种模型对己科人格特征的敏感性，更重要的是，其产出的偏见，负面情绪和毒性的一致变化。特别是，调整几种人格特征的水平可以有效地降低模型表现的偏见和毒性，类似于人格特征与有毒行为之间的人类相关性。这些发现突出了除了培训或通过LLM拟人化的培训方法或微调方法外，还需要检查内容安全性。他们还建议将个性调整为一种简单且低成本的方法来进行受控文本生成。]]></description>
      <guid>https://arxiv.org/abs/2502.12566</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>RM-POT：重新解决数学问题并通过思想计划解决</title>
      <link>https://arxiv.org/abs/2502.12589</link>
      <description><![CDATA[ARXIV：2502.12589V1公告类型：新 
摘要：最近，在培训语言模型中取得了重大进步，以进行分步推理，以解决复杂的数值推理任务。除了解决这些问题的方法之外，问题本身的结构和表述在确定大语言模型的性能中也起着至关重要的作用。我们观察到，即使是数学问题的表面形式的微小变化也会对答案分布和解决率产生深远的影响。这突出了LLMS对表面级别变化的脆弱性，在通过复杂问题推理时揭示了其有限的鲁棒性。在本文中，我们提出了RM-POT，RM-POT是一个整合问题重新重新制定（RM），代码辅助推理（POT）和域名的三阶段框架，几乎没有学会来解决这些局限性。我们的方法首先将输入问题重新制定为各种表面形式，以减少结构性偏见，然后从特定于特定于域的特定领域问题库中检索五个语义上的示例，以提供上下文指导，并最终生成可执行的Python代码以进行精确计算。]]></description>
      <guid>https://arxiv.org/abs/2502.12589</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Perovskite-llm：知识增强的钙钛矿太阳能细胞研究的大型语言模型</title>
      <link>https://arxiv.org/abs/2502.12669</link>
      <description><![CDATA[Arxiv：2502.12669V1公告类型：新 
摘要：钙钛矿太阳能电池的快速发展（PSC）导致了研究出版物的指数增长，从而迫切需要该领域中有效的知识管理和推理系统。我们为PSC提供了一个全面的知识增强系统，该系统集成了三个关键组成部分。首先，我们开发了Perovskite-KG，这是一个特定于领域的知识图，该图表由1,517篇研究论文构建，其中包含23,789个实体和22,272个关系。其次，我们创建了两个互补数据集：Perovskite-Chat，包括55,101个高质量的问题 - 答案对，通过新型的多代理框架生成，以及钙钛矿 - 季节性，包含2,217个精心策划的材料科学问题。第三，我们介绍了两种专业的大型语言模型：用于领域特定知识援助的Perovskite-Chat-llm和用于科学推理任务的Perovskite-Rounowing-llm。实验结果表明，我们的系统在特定于领域的知识检索和科学推理任务中大大优于现有模型，从而为研究人员提供了有效的文献综述工具，实验设计和PSC研究中的复杂问题解决。]]></description>
      <guid>https://arxiv.org/abs/2502.12669</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>vidcapbench：可控文本的视频字幕的综合基准标题</title>
      <link>https://arxiv.org/abs/2502.12782</link>
      <description><![CDATA[ARXIV：2502.12782V1公告类型：新 
摘要：可控文本对视频（T2V）模型的培训在很大程度上取决于视频和字幕之间的一致性，但是现有的研究很少将视频字幕评估与T2V生成评估联系起来。本文介绍了Vidcapbench，这是一种视频标题评估方案，专为T2V生成而设计，不可知论为任何特定的字幕格式。 Vidcapbench采用数据注释管道，结合了专家模型标签和人类精致，将每个收集的视频与跨越视频美学，内容，运动和物理定律的关键信息相关联。然后，vidcapbench将这些关键信息属性划分为自动评估和手动评估子集，以满足敏捷开发的快速评估需求以及彻底验证的准确性要求。通过评估众多最先进的字幕模型，我们证明了与现有的视频字幕评估方法相比，Vidcapbench的稳定性和全面性。使用现成的T2V模型进行验证表明，Vidcapbench上的分数与T2V质量评估指标之间存在显着的正相关，这表明VidCapbench可以为培训T2V模型提供宝贵的指导。该项目可在https://github.com/vidcapbench/vidcapbench上找到。]]></description>
      <guid>https://arxiv.org/abs/2502.12782</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过AI进行自适应反馈：比较LLM和教师在实验方案中的反馈质量</title>
      <link>https://arxiv.org/abs/2502.12842</link>
      <description><![CDATA[arxiv：2502.12842v1公告类型：新 
摘要：有效的反馈对于促进学生在科学探究方面的成功至关重要。随着人工智能的进步，大语言模型（LLMS）为提供即时和自适应反馈提供了新的可能性。但是，这种反馈通常缺乏现实世界实践者提供的教学验证。为了解决这一局限性，我们的研究评估并比较了LLM代理的反馈质量与学生写的实验协议方面的人类教师和科学教育专家的反馈质量。四名盲人评估者，所有科学探究和科学教育专业人士，评估了1）LLM代理人产生的反馈文本，2）教师和3）科学教育专家使用五点李克特量表，基于有效反馈的六个标准：馈送，反馈，向前喂养，建设性语调，语言清晰度和技术术语。我们的结果表明，LLM生成的反馈与教师和专家在整体质量方面没有显着差异。但是，LLM代理商的性能滞后在馈后维度，其中涉及在学生的工作环境中识别和解释错误。定性分析强调了LLM代理在上下文理解和明确交流特定错误中的局限性。我们的发现表明，将LLM生成的反馈与人类专业知识相结合可以通过利用LLM的效率和对教育工作者的细微理解来增强教育实践。]]></description>
      <guid>https://arxiv.org/abs/2502.12842</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>持续学习对话AI：通过A2C增强学习的个性化代理框架</title>
      <link>https://arxiv.org/abs/2502.12876</link>
      <description><![CDATA[ARXIV：2502.12876V1公告类型：新 
摘要：创建个性化和适应性的对话AI仍然是一个关键挑战。本文介绍了使用A2C增强学习实施的连续学习对话AI（CLCA）方法，以超越静态大语言模型（LLMS）。我们使用LLMS生成的模拟销售对话来培训A2C代理。该代理商学会了优化个性化对话策略，专注于参与和交付价值。我们的系统体系结构将增强学习与LLMS集成，以进行数据创建和响应选择。这种方法提供了一种实用的方式来建立个性化的AI同伴，通过不断学习，超越传统的静态LLM技术来发展。]]></description>
      <guid>https://arxiv.org/abs/2502.12876</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>迈向更多上下文的代理：提取器生成器优化框架</title>
      <link>https://arxiv.org/abs/2502.12926</link>
      <description><![CDATA[ARXIV：2502.12926V1公告类型：新 
摘要：基于大语言模型（LLM）的代理在解决各种通用应用程序中的复杂任务方面取得了巨大的成功。但是，它们的表现通常在特定于上下文的情况下降低，例如专业行业或研究领域，因为缺乏与领域相关的知识导致不精确或次优的结果。为了应对这一挑战，我们的工作介绍了一种系统的方法来通过优化其基于LLM的代理的上下文适应性，以优化其基本提示至关重要的组件，以控制代理行为，角色和互动。手动制定特定于上下文任务的优化提示是劳动密集型，容易出错的，并且缺乏可扩展性。在这项工作中，我们介绍了一个旨在自动化基于上下文LLM的代理的提取器生成器框架。我们的方法通过两个关键阶段运行：（i）从金标准输入输出示例的数据集中提取提取，以及（ii）通过高级优化策略及时生成，该策略迭代地识别表现不佳的情况并应用自我提高技术。该框架通过实现在各种输入之间进行更精确的概括，尤其是在特定于上下文特定的任务中，可以实质上提高及时的适应性，在这种任务中，维持语义一致性和最小化误差传播对于可靠的性能至关重要。尽管考虑到单阶段的工作流程开发，但该方法自然而然地扩展到多阶段的工作流程，可在各种基于代理的系统中提供广泛的适用性。经验评估表明，我们的框架显着提高了迅速优化的代理的性能，从而为基于LLM的代理提供了一种结构化和有效的方法。]]></description>
      <guid>https://arxiv.org/abs/2502.12926</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有元认知触发器的大语言模型中的自适应工具</title>
      <link>https://arxiv.org/abs/2502.12961</link>
      <description><![CDATA[Arxiv：2502.12961V1公告类型：新 
摘要：大型语言模型（LLMS）显示出显着的紧急功能，通过利用外部工具来解决需要专业处理或实时数据的复杂问题来改变功能任务的执行。尽管现有研究扩展了LLMS对不同工具的访问（例如，程序口译员，搜索引擎，天气/地图应用程序），但使用这些工具的必要性通常被忽略，从而导致工具不加区分的工具调用。这种天真的方法提出了两个关键问题：（1）由于不必要的工具调用而增加延迟，以及（2）与外部工具的互动错误导致的潜在错误。在本文中，我们将元认知介绍为LLMS对其能力的自我评估的代理，代表了该模型对自身局限性的认识。基于此，我们提出了MECO，这是一种用于外部工具使用的自适应决策策略。 MECO通过在表示空间中捕获高级认知信号来量化元认知得分，从而指导何时调用工具。值得注意的是，Meco无需微调，而成本最低。我们的实验表明，MECO准确地检测了LLMS的内部认知信号，并显着改善了多种基本模型和基准的工具使用决策。]]></description>
      <guid>https://arxiv.org/abs/2502.12961</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>免费的论证交流用于解释图像分类器</title>
      <link>https://arxiv.org/abs/2502.12995</link>
      <description><![CDATA[ARXIV：2502.12995V1公告类型：新 
摘要：深度学习模型是强大的图像分类器，但它们的不透明性阻碍了他们的可信度。由于其纯粹的复杂性和规模，凭借忠实和清晰的方式捕获这些分类器中推理过程的解释方法很少。我们通过定义一种新方法来解释图像分类器的输出，并通过两种代理之间的辩论来解释图像分类器的输出，每种方法都为特定类别提供了解决方案。我们将这些辩论视为自由论证交流（传真）的具体实例，这是一种基于论证的新型多代理框架，允许其他代理人与最初所说的不同。我们定义了两个指标（共识和说服率），以评估传真作为图像分类器的论证解释的有用性。然后，我们进行了许多经验实验，表明传真沿这些指标表现良好，并且比传统的，非夸张的解释方法更忠于图像分类器。我们所有的实现都可以在https://github.com/koriavinash1/fax上找到。]]></description>
      <guid>https://arxiv.org/abs/2502.12995</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>您需要模仿才能获得名声：通过多代理对话解决成绩单稀缺</title>
      <link>https://arxiv.org/abs/2502.13001</link>
      <description><![CDATA[ARXIV：2502.13001V1公告类型：新 
摘要：汇总汇总的高质量数据有限，这主要是由于隐私限制和昂贵的收集过程。我们以名望来解决这一差距，这是一个由500个英语会议组成的数据集和Mimic制作的300次德语，这是我们的新的多代理会议综合框架，该框架通过定义心理扎根的参与者资料，概述对话，概述对话，概述对话，从而在给定的知识源上生成会议记录。并策划大型语言模型（LLM）辩论。模块化的后处理步骤可以完善这些输出，从而减轻潜在的重复性和过正式的音调，从而确保按大规模进行连贯，可信的对话。我们还提出了一个心理扎根的评估框架，以评估自然性，社会行为真实性和成绩单上的困难。人类评估表明，名声近似于实地自发性（自然性4.5/5），以说话者为中心的挑战（口语为3/5），并引入了更丰富的面向信息的困难（难度为4/5）。这些发现强调了名声是现实世界中的良好且可扩展的代理。它可以在需要对话数据或模拟行为限制下模拟社交场景的任务中满足摘要研究和其他以对话为中心的应用程序的新测试场景。]]></description>
      <guid>https://arxiv.org/abs/2502.13001</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>整合强化学习，行动模型学习和数字计划，以应对复杂任务</title>
      <link>https://arxiv.org/abs/2502.13006</link>
      <description><![CDATA[ARXIV：2502.13006V1公告类型：新 
摘要：自动化计划算法需要一个域的模型，以指定每个动作的前提和效果。众所周知，获得这样的域模型非常困难。存在学习域模型的算法，但尚不清楚学习域模型和计划是否是数字计划环境的有效方法，即状态包括离散和数字状态变量。在这项工作中，我们探讨了学习数字域模型的好处，并将其与替代模型解决方案进行比较。作为案例研究，我们在Minecraft中使用了两项任务，Minecraft是一种流行的沙盒游戏，已被用作AI挑战。首先，我们考虑一个离线学习设置，其中一组专家轨迹可以从中学习。这是学习域模型的标准设置。我们使用数字安全动作模型学习（NSAM）算法来学习数字域模型，并通过学习域模型和数字计划者解决新问题。我们称此基于模型的解决方案NSAM _（+P），并将其与几个无模型的模型学习（IL）和离线增强学习（RL）算法进行比较。经验结果表明，某些IL算法可以更快地学习以解决简单的任务，而NSAM _（+p）允许求解需要长期计划的任务，并使其能够在较大环境中解决问题。然后，我们考虑一个在线学习设置，在该设置中，通过在环境中移动代理商来完成学习。对于此设置，我们介绍坡道。在坡道中，代理执行期间收集的观察结果用于同时训练RL策略并学习计划域行动模型。这形成了RL策略和学习域模型之间的积极反馈循环。我们通过实验证明了使用坡道的好处，表明它发现了比几个RL基准的计划更有效的计划并解决了更多的问题。]]></description>
      <guid>https://arxiv.org/abs/2502.13006</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>