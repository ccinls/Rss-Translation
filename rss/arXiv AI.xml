<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.AI 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.AI 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Fri, 08 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>我们迫切需要本质上善良的机器</title>
      <link>https://arxiv.org/abs/2411.04126</link>
      <description><![CDATA[arXiv:2411.04126v1 公告类型：新
摘要：人工智能系统正在快速发展，整合了外在和内在动机。虽然这些框架提供了好处，但它们在算法层面上存在错位的风险，同时表面上与人类价值观保持一致。在本文中，我们认为，善良的内在动机对于确保这些模型与人类价值观内在一致至关重要。我们认为，善良是一种利他主义形式，其动机是最大化他人的回报，它可以抵消任何可能导致模型优先考虑自身而不是人类福祉的内在动机。我们的方法引入了一个框架和算法，通过模拟对话将善良嵌入基础模型。讨论了可扩展实施的局限性和未来研究方向。]]></description>
      <guid>https://arxiv.org/abs/2411.04126</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>将心智理论与善良相结合，实现自我监督的人机协同</title>
      <link>https://arxiv.org/abs/2411.04127</link>
      <description><![CDATA[arXiv:2411.04127v1 公告类型：新
摘要：随着人工智能 (AI) 深度融入关键基础设施和日常生活，确保其安全部署是人类面临的最紧迫挑战之一。当前的人工智能模型优先考虑任务优化而不是安全性，从而导致意外伤害的风险。由于政府、企业和倡导团体的利益冲突，这些风险很难解决，他们在人工智能竞赛中都有不同的优先事项。当前的对齐方法，例如从人类反馈中强化学习 (RLHF)，专注于外在行为，而没有灌输对人类价值观的真正理解。这些模型容易受到操纵，缺乏推断他人心理状态和意图所必需的社交智能，这引发了人们对它们在复杂和新情况下安全负责地做出重要决策的能力的担忧。此外，人工智能中外在动机和内在动机之间的分歧带来了欺骗性或有害行为的风险，尤其是当系统变得更加自主和智能时。我们提出了一种新颖的受人启发的方法，旨在解决这些不同的问题并帮助协调相互竞争的目标。]]></description>
      <guid>https://arxiv.org/abs/2411.04127</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过使用原始和邻域来增强近似空间</title>
      <link>https://arxiv.org/abs/2411.04133</link>
      <description><![CDATA[arXiv:2411.04133v1 公告类型：新
摘要：粗糙集理论是处理不完整信息的最广泛和最重要的方法之一。它从一开始就划分宇宙，并使用等价关系来产生块。为了增加灵活性和扩大可能的用途范围，已经提出并研究了许多广义粗糙集模型。我们引入了四种新的广义粗糙集模型，这些模型从“邻域和原始”中汲取灵感，以便为这个主题做出贡献。通过最小化不确定性区域，这些模型旨在帮助决策者更有效地分析和评估所提供的数据。我们通过证明现有模型在改进近似算子（上限和下限）和准确度测量方面优于某些当前方法，从而验证了这一目标。我们声称当前模型可以保留与粗糙集模型相关的几乎所有重要方面。保留单调性使我们能够评估数据的不确定性并增强对结果的信心，这是从现有模型中得出的有趣特征之一。借助具体实例，我们还比较了当前方法的各个领域。最后，我们证明了我们为日常健康相关问题定义的新策略可以产生更准确的结果。]]></description>
      <guid>https://arxiv.org/abs/2411.04133</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>语言模型是隐藏的推理者：通过自我奖励释放潜在推理能力</title>
      <link>https://arxiv.org/abs/2411.04282</link>
      <description><![CDATA[arXiv:2411.04282v1 公告类型：新
摘要：大型语言模型 (LLM) 已经展现出令人印象深刻的能力，但在处理需要多个步骤的复杂推理任务时仍然举步维艰。虽然基于提示的方法（如思维链 (CoT)）可以在推理时改进 LLM 推理，但在训练期间优化推理能力仍然具有挑战性。我们引入了潜在推理优化 (LaTRO)，这是一个原则性框架，它将推理公式化为从潜在分布中采样并通过变分方法对其进行优化。LaTRO 使 LLM 能够同时改进其推理过程和评估推理质量的能力，而无需外部反馈或奖励模型。我们通过使用多种模型架构在 GSM8K 和 ARC-Challenge 数据集上进行实验来验证 LaTRO。在 GSM8K 上，LaTRO 在 Phi-3.5-mini、Mistral-7B 和 Llama-3.1-8B 上将零样本准确率平均提高了 12.5%（比基础模型高），在监督微调上提高了 9.6%。我们的研究结果表明，预训练的 LLM 具有潜在推理能力，可以通过我们提出的优化方法以自我改进的方式解锁和增强。LaTRO 的代码可在 \url{https://github.com/SalesforceAIResearch/LaTRO} 上找到。]]></description>
      <guid>https://arxiv.org/abs/2411.04282</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于组合优化的随机键优化器</title>
      <link>https://arxiv.org/abs/2411.04293</link>
      <description><![CDATA[arXiv:2411.04293v1 公告类型：新
摘要：本文介绍了随机密钥优化器 (RKO)，这是一种针对组合优化问题的多功能高效随机局部搜索方法。使用随机密钥概念，RKO 将解决方案编码为随机密钥向量，随后通过问题特定的解码器将其解码为可行解决方案。RKO 框架能够结合大量经典的元启发式方法，每种方法都可以独立或并行运行，并通过精英解决方案池促进解决方案共享。这种模块化方法允许适应各种元启发式方法，包括模拟退火、迭代局部搜索和贪婪随机自适应搜索程序等。用 C++ 实现的 RKO 框架的有效性通过将其应用于三个 NP 难组合优化问题得到证明：alpha 邻域 p 中值问题、枢纽树位置问题和节点容量图分区问题。结果突出了该框架在不同问题领域产生高质量解决方案的能力，凸显了其作为组合优化强大工具的潜力。]]></description>
      <guid>https://arxiv.org/abs/2411.04293</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>神经符号人工智能：可解释性、挑战和未来趋势</title>
      <link>https://arxiv.org/abs/2411.04383</link>
      <description><![CDATA[arXiv:2411.04383v1 Announce Type: new 
摘要：可解释性是限制神经网络在许多重要领域应用的重要原因。尽管神经符号人工智能希望利用符号学习的透明性来提升整体的可解释性，但效果并没有想象的那么明显。本文从模型设计和行为两个角度对2013年以来191篇以神经符号人工智能为重点的研究提出了一种可解释性的分类，希望对那些想要理解神经符号人工智能可解释性的学者有所启发。具体来说，我们以弥合表示差异的形式是否可读为设计因素、神经网络和符号逻辑学习之间是否存在表示差异、模型决策或预测过程是否可理解为行为​​因素，将它们分为五类：隐式中间表示和隐式预测、部分显式中间表示和部分显式预测、显式中间表示或显式预测、显式中间表示和显式预测、统一表示和显式预测。我们还分析了研究趋势和三大挑战：统一表示、可解释性和透明度以及神经网络和符号学习的充分合作。最后，我们从统一表示、增强模型可解释性、伦理考虑和社会影响三个方面提出了未来研究的建议。]]></description>
      <guid>https://arxiv.org/abs/2411.04383</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>弥合差距：神经符号人工智能中的表征空间</title>
      <link>https://arxiv.org/abs/2411.04393</link>
      <description><![CDATA[arXiv:2411.04393v1 Announce Type: new 
摘要：神经符号人工智能是一种结合神经网络和符号学习优势，提升人工智能模型整体性能的有效方法。然而，两者在数据处理方式上存在差异，主要是因为它们往往使用不同的数据表示方法，而这往往是限制两者整体性能的重要因素。从这个角度出发，我们通过构建一个四级分类框架，对2013年的191篇研究进行了分析。第一级定义了五种类型的表示空间，第二级关注表示空间可以表示的五种信息模态。然后，第三级描述了四种符号逻辑方法。最后，第四级分类提出了神经网络和符号学习的三种协作策略。此外，我们根据46项研究的表示空间对其进行了详细分析。]]></description>
      <guid>https://arxiv.org/abs/2411.04393</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CDT 能否通过改良的人择原理合理化事前最优政策？</title>
      <link>https://arxiv.org/abs/2411.04462</link>
      <description><![CDATA[arXiv:2411.04462v1 公告类型：新
摘要：在 Newcomb 问题中，因果决策理论 (CDT) 建议使用双盒，因此与证据决策理论 (EDT) 和事前策略优化（规定使用单盒）不同。然而，在 Newcomb 问题中，你或许应该相信，你处于预测器运行的模拟中，以确定是否将一百万美元放入不透明框中。如果是这样，那么因果决策理论可能会建议使用单盒，以使预测器填充不透明框。在本文中，我们研究这种方法的概括。也就是说，我们考虑一般的 Newcomb 类问题，并尝试形成合理的自定位信念，在这些信念下，CDT 的建议与 EDT 类的事前策略优化概念相一致。我们考虑将世界建模为代理的运行模拟的方法，以及不基于此类模型的方法（我们称之为“广义广义第三模型”，或 GGT）。对于每种方法，我们描述所产生的 CDT 策略，并证明在某些条件下，这些策略包括事前最优策略。]]></description>
      <guid>https://arxiv.org/abs/2411.04462</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Magentic-One：用于解决复杂任务的通用多智能体系统</title>
      <link>https://arxiv.org/abs/2411.04468</link>
      <description><![CDATA[arXiv:2411.04468v1 公告类型：新
摘要：现代人工智能代理在大型基础模型的进步推动下，有望通过增强我们的知识和能力来提高我们的生产力并改变我们的生活。为了实现这一愿景，人工智能代理必须有效地规划、执行多步骤推理和行动、响应新观察并从错误中恢复，以成功完成各种场景中的复杂任务。在这项工作中，我们介绍了 Magentic-One，这是一个用于解决此类任务的高性能开源代理系统。Magentic-One 使用多代理架构，其中主代理 Orchestrator 计划、跟踪进度并重新计划以从错误中恢复。在整个任务执行过程中，Orchestrator 会根据需要指示其他专门的代理执行任务，例如操作 Web 浏览器、浏览本地文件或编写和执行 Python 代码。我们表明，Magentic-One 在三个不同且具有挑战性的代理基准测试（GAIA、AssistantBench 和 WebArena）上实现了与最新技术相当的统计竞争性能。Magentic-One 无需修改核心代理功能或它们如何协作即可实现这些结果，展示了向通用代理系统迈进的进展。此外，Magentic-One 的模块化设计允许在团队中添加或删除代理，而无需进行额外的快速调整或培训，从而简化开发并使其可扩展到未来的场景。我们提供了 Magentic-One 的开源实现，并包括 AutoGenBench，这是一个用于代理评估的独立工具。AutoGenBench 提供内置的重复和隔离控制，以严格且受控的方式运行代理基准测试——当代理的操作有副作用时，这一点很重要。Magentic-One、AutoGenBench 和 Magentic-One 的详细经验性能评估（包括消融和错误分析）可在 https://aka.ms/magentic-one 上找到]]></description>
      <guid>https://arxiv.org/abs/2411.04468</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>交互式进化多目标优化中相关目标的动态检测和对偏好漂移的适应</title>
      <link>https://arxiv.org/abs/2411.04547</link>
      <description><![CDATA[arXiv:2411.04547v1 公告类型：新
摘要：进化多目标优化算法 (EMOA) 被广泛用于解决具有多个冲突目标的问题。最近的研究表明，并非所有目标对决策者 (DM) 都同样重要。在交互式 EMOA 的背景下，优化过程中从 DM 获得的偏好信息可用于识别和丢弃不相关的目标，这是客观评估计算成本高昂时的关键步骤。然而，现有的许多文献未能解释 DM 偏好的动态性质，这种偏好可以在整个决策过程中演变并影响目标的相关性。本研究通过在基于排名的交互式算法中模拟 DM 偏好的动态变化来解决这一限制。此外，我们提出了在发生此类转变时丢弃过时或冲突的偏好的方法。在先前研究的基础上，我们还引入了一种机制来保护相关目标，这些目标可能会因与 DM 提供的排名的相关性降低而陷入局部或全局最优。我们的实验结果表明，所提出的方法有效地管理不断变化的偏好，并显著提高了算法产生的解决方案的质量和可取性。]]></description>
      <guid>https://arxiv.org/abs/2411.04547</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>加权结构化论证中省略三段论解码评价的公理化研究</title>
      <link>https://arxiv.org/abs/2411.04555</link>
      <description><![CDATA[arXiv:2411.04555v1 Announce Type: new 
摘要：一个论证可以看作是由一组前提和一个由它们支持的主张组成的对。人类使用的论证通常是省略三段论，即一些前提是隐含的。为了更好地理解、评估和比较省略三段论，解码它们至关重要，即找到缺失的前提。许多省略三段论解码都是可能的。我们需要区分合理的解码和不合理的解码。然而，目前文献中还没有关于“如何评估解码？”的研究。为了铺平道路并实现这一目标，我们根据不同的研究领域引入了七个与解码相关的标准。然后，我们引入了标准测度的概念，其目的是根据某个标准评估解码。由于这些测度需要验证，我们为它们引入了几个理想的属性，称为公理。本文的另一个主要贡献是构建了某些由我们的公理验证的标准度量。这些度量可用于识别最佳省略三段论解码。]]></description>
      <guid>https://arxiv.org/abs/2411.04555</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多智能体是社会群体：探究人机交互中多智能体的社会影响</title>
      <link>https://arxiv.org/abs/2411.04578</link>
      <description><![CDATA[arXiv:2411.04578v1 公告类型：新
摘要：多智能体系统（具有多个独立 AI 智能体共同努力实现共同目标的系统）在日常生活中越来越普遍。从人类群体社会影响现象中汲取灵感，我们研究一组 AI 智能体是否可以对用户施加社会压力，使其同意他们的观点，从而可能改变他们对某个话题的立场。我们进行了一项研究，参与者与一个或多个 AI 智能体讨论社会问题，智能体同意或不同意用户对该话题的立场。我们发现，与多个智能体交谈（保持对话内容不变）会增加参与者感受到的社会压力，并导致对智能体在每个话题上的立场的看法发生更大转变。我们的研究表明，多智能体系统在改变观点方面比单智能体平台具有潜在优势。我们讨论了可能促进社会利益的多智能体系统的设计含义，以及恶意行为者使用这些系统操纵舆论的可能性。]]></description>
      <guid>https://arxiv.org/abs/2411.04578</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>解释 MuZero 规划中的学习模型</title>
      <link>https://arxiv.org/abs/2411.04580</link>
      <description><![CDATA[arXiv:2411.04580v1 公告类型：新
摘要：MuZero 通过使用动态网络预测环境动态进行规划，而无需依赖模拟器，在各种游戏中取得了超人的表现。然而，动态网络学习到的潜在状态使其规划过程不透明。本文旨在通过解释学习到的潜在状态来揭开 MuZero 模型的神秘面纱。我们将观察重建和状态一致性纳入 MuZero 训练中，并进行深入分析以评估两款棋盘游戏：9x9 围棋和 Outer-Open Gomoku，以及三款 Atari 游戏：Breakout、Ms. Pacman 和 Pong 的潜在状态。我们的研究结果表明，虽然动态网络在较长的模拟过程中变得不那么准确，但 MuZero 仍然通过使用规划来纠正错误而有效地发挥作用。我们的实验还表明，动态网络在棋盘游戏中比在 Atari 游戏中学习到更好的潜在状态。这些见解有助于更好地理解 MuZero，并为未来的研究提供方向，以提高 MuZero 算法的游戏性能、稳健性和可解释性。]]></description>
      <guid>https://arxiv.org/abs/2411.04580</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DISCO：发现过度拟合作为文本分类模型的因果规则</title>
      <link>https://arxiv.org/abs/2411.04649</link>
      <description><![CDATA[arXiv:2411.04649v1 公告类型：新
摘要：随着神经语言模型的快速发展，过度参数化模型的部署激增，增加了对人类检查员可理解的可解释解释的需求。现有的事后可解释性方法通常侧重于单个输入文本实例的单字特征，无法完全捕捉模型的决策过程。此外，许多方法没有区分基于虚假相关性的决策和基于对输入的整体理解的决策。我们的论文介绍了 DISCO，这是一种通过识别与模型预测的因果 n-gram 关联来发现全局、基于规则的解释的新方法。该方法采用可扩展的序列挖掘技术从训练数据中提取相关文本跨度，将它们与模型预测相关联，并进行因果关系检查以提炼出阐明模型行为的稳健规则。这些规则揭示了潜在的过度拟合并提供对误导性特征组合的见解。我们通过大量测试验证了 DISCO，证明了它在提供对复杂模型行为的全面洞察方面优于现有方法。我们的方法成功识别了手动引入训练数据的所有快捷方式（MultiRC 数据集上的检测率为 100%），导致模型性能下降了 18.8%——这是任何其他方法都无法比拟的能力。此外，DISCO 支持交互式解释，使人类检查员能够区分基于规则的输出中的虚假原因。这减轻了大量实例解释的负担，并有助于评估模型在遇到分布外 (OOD) 数据时的风险。]]></description>
      <guid>https://arxiv.org/abs/2411.04649</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CaPo：实现高效多智能体合作的合作计划优化</title>
      <link>https://arxiv.org/abs/2411.04679</link>
      <description><![CDATA[arXiv:2411.04679v1 公告类型：新
摘要：在这项工作中，我们解决了基于大型语言模型 (LLM) 的具身代理之间的合作问题，其中代理必须合作才能实现共同目标。以前的方法通常会即兴且不连贯地执行操作，而没有长期的战略和合作规划，从而导致在诸如搜救任务等复杂任务中出现冗余步骤、失败甚至严重后果，而讨论和合作计划至关重要。为了解决这个问题，我们提出了合作计划优化 (CaPo) 来提高基于 LLM 的具身代理的合作效率。受人类合作方案的启发，CaPo 通过两个阶段提高了合作效率：1) 元计划生成，2) 进度自适应元计划和执行。在第一阶段，所有代理分析任务、讨论并合作创建一个元计划，将任务分解为具有详细步骤的子任务，确保制定长期战略和连贯的计划以实现有效协调。在第二阶段，代理根据元计划执行任务，并通过多轮讨论根据其最新进展（例如，发现目标对象）动态调整元计划。这种基于进展的调整消除了冗余操作，提高了代理的整体合作效率。在 ThreeDworld 多代理传输和通信观察和帮助任务上的实验结果表明，与最先进的方法相比，CaPo 实现了更高的任务完成率和效率。]]></description>
      <guid>https://arxiv.org/abs/2411.04679</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用网络流模型解决单元制造系统中的广义分组问题</title>
      <link>https://arxiv.org/abs/2411.04685</link>
      <description><![CDATA[arXiv:2411.04685v1 公告类型：新
摘要：本文重点研究单元制造系统 (CMS) 中的广义分组问题，其中零件可能有多个工艺路线。工艺路线列出了与操作的每个部分相对应的机器。受网络流算法广泛使用的启发，本研究将广义分组的工艺路线系列形成公式化为单位容量最小成本网络流模型。目标是尽量减少系列内工艺路线之间的差异（基于所需的机器）。所提出的模型可以最佳地解决工艺路线系列形成问题，而无需预先指定要形成的零件系列数量。系列形成的工艺路线是分层程序的第一阶段。对于第二阶段（机器单元形成），提出了两个程序，即二次分配规划 (QAP) 公式和启发式程序。QAP 同时将工艺路线系列和机器分配给预先指定数量的单元，从而最大化总机器利用率。机器细胞形成的启发式程序本质上是分层的。一些测试问题的计算结果表明，QAP 和启发式程序产生相同的结果。]]></description>
      <guid>https://arxiv.org/abs/2411.04685</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>权衡利弊：多目标强化学习的策略总结</title>
      <link>https://arxiv.org/abs/2411.04784</link>
      <description><![CDATA[arXiv:2411.04784v1 公告类型：新
摘要：多目标强化学习 (MORL) 用于解决涉及多个目标的问题。MORL 代理必须根据不同奖励函数提供的不同信号做出决策。训练 MORL 代理会产生一组解决方案（策略），每个解决方案都呈现出目标（预期回报）之间的不同权衡。MORL 通过基于权衡对解决方案集中的策略进行细粒度比较（而不是采用单一策略）来增强可解释性。然而，解决方案集通常很大且多维，其中每个策略（例如神经网络）都由其目标值表示。
我们提出了一种对 MORL 生成的解决方案集进行聚类的方法。通过同时考虑策略行为和目标值，我们的聚类方法可以揭示策略行为与目标空间中区域之间的关系。这种方法可以让决策者 (DM) 识别解决方案集中的总体趋势和见解，而不是单独检查每个策略。我们在四个多目标环境中测试了我们的方法，发现它比传统的 k-medoids 聚类效果更好。此外，我们还提供了一个案例研究来展示其实际应用。]]></description>
      <guid>https://arxiv.org/abs/2411.04784</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>增强投资分析：优化金融研究中的人工智能与代理协作</title>
      <link>https://arxiv.org/abs/2411.04788</link>
      <description><![CDATA[arXiv:2411.04788v1 公告类型：新
摘要：近年来，生成人工智能（GenAI）在金融分析和投资决策中的应用引起了广泛关注。然而，大多数现有方法依赖于单代理系统，无法充分利用多个AI代理的协作潜力。在本文中，我们提出了一种新颖的多代理协作系统，旨在增强金融投资研究中的决策能力。该系统结合了具有可配置组大小和协作结构的代理组，以利用每种代理组类型的优势。通过利用次优组合策略，系统可以动态适应不同的市场条件和投资场景，从而优化不同任务的性能。我们通过分析道琼斯指数中 30 家公司的 2023 年 SEC 10-K 表格，重点关注三个子任务：基本面、市场情绪和风险分析。我们的研究结果揭示了基于不同任务的 AI 代理配置的显着性能差异。结果表明，我们的多智能体协作系统优于传统的单智能体模型，在复杂的金融环境中具有更高的准确性、效率和适应性。本研究强调了多智能体系统通过整合不同的分析视角来改变金融分析和投资决策的潜力。]]></description>
      <guid>https://arxiv.org/abs/2411.04788</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习中的可塑性损失：一项调查</title>
      <link>https://arxiv.org/abs/2411.04832</link>
      <description><![CDATA[arXiv:2411.04832v1 公告类型：新
摘要：与人类大脑的神经可塑性类似，深度神经网络的可塑性使其能够快速适应新数据。这使得可塑性对于深度强化学习 (RL) 代理尤为重要：一旦失去可塑性，代理的性能将不可避免地停滞不前，因为它无法改进其策略以解释数据分布的变化，而数据分布的变化是其学习过程的必然结果。因此，开发性能良好且样本效率高的代理取决于它们在训练期间保持可塑性的能力。此外，可塑性的丧失可能与困扰深度 RL 的许多其他问题有关，例如训练不稳定性、扩展失败、高估偏差和探索不足。通过这项调查，我们旨在为深度强化学习的学者和从业者提供关于可塑性丧失的新兴研究的概述。首先，我们根据最近的研究提出了可塑性丧失的统一定义，将其与文献中的定义联系起来，并讨论了测量可塑性丧失的指标。然后，我们在回顾当前采用的缓解策略之前，对可塑性丧失的众多可能原因进行了分类和讨论。我们的分类法是对该领域现状的首次系统概述。最后，我们讨论文献中普遍存在的问题，例如更广泛评估的必要性，并为未来的研究提供建议，例如更好地了解代理的神经活动和行为。]]></description>
      <guid>https://arxiv.org/abs/2411.04832</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>明智思考，行动起来！分析多智能体强化学习中的概率逻辑驱动安全性</title>
      <link>https://arxiv.org/abs/2411.04867</link>
      <description><![CDATA[arXiv:2411.04867v1 公告类型：新
摘要：在现实世界中部署强化学习 (RL) 算法的一个重要挑战是安全性。这导致了最近的安全 RL 研究领域的出现，该领域旨在学习安全的最佳策略。该方向的一个成功方法是概率逻辑屏蔽 (PLS)，这是一种基于模型的安全 RL 技术，它使用基于概率逻辑编程的形式规范，约束代理的策略以概率方式遵守这些规范。然而，安全性本质上是一个多代理概念，因为现实世界环境通常涉及多个代理同时交互，从而导致难以控制的复杂系统。此外，安全多代理 RL (Safe MARL) 仍未得到充分探索。为了解决这一差距，在本文中我们 ($i$) 通过将 PLS 扩展到 MARL 来引入屏蔽 MARL (SMARL) - 具体来说，我们引入概率逻辑时间差异学习 (PLTD) 来实现屏蔽独立 Q 学习 (SIQL)，并引入使用概率逻辑策略梯度的屏蔽独立 PPO (SIPPO)； ($ii$) 展示其在各种博弈论环境（包括双人同时游戏、广泛形式游戏、随机游戏和一些网格世界扩展）中的积极作用和作为均衡选择机制的用途，包括安全性、合作和与规范行为的一致性；并且 ($iii$) 研究只有一个代理被屏蔽的不对称情况，并表明屏蔽代理对未屏蔽代理有显着影响，为 SMARL 能够在多样化的多代理环境中增强安全性和合作性提供了进一步的证据。]]></description>
      <guid>https://arxiv.org/abs/2411.04867</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>